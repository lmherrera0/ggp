# GGP Channel Templates — Consolidated Knowledge Base
## For Claude Projects / Knowledge Base Upload

> This document consolidates all 22 channel templates plus the index from GGP into a single reference.

---

## Document Contents

1. **Channel Index**
2. **Academic Journal**
3. **AI Agents Docs**
4. **Analytical Docs**
5. **Application Docs**
6. **Book Proposal**
7. **Coding Docs**
8. **Data Lineage**
9. **Data Products**
10. **Email**
11. **HBR**
12. **Instagram**
13. **Internal Docs**
14. **Legal Disclaimers**
15. **LinkedIn**
16. **Messaging**
17. **MIT SMR**
18. **Op-Ed**
19. **Presentations**
20. **Press Release**
21. **Skills Docs**
22. **Twitter**
23. **Version Control**

---

# 1. Channel Index
> Source: `references/channels/_index.md`

# GGP Channel Navigation Index

Maps all professional communication channels to their reference templates. Each channel has dedicated guidance aligned with Grounded Gate Protocol principles.

## Everyday Communications

| Channel | File | Use When |
|---------|------|----------|
| Email | `email.md` | Formal requests, confirmations, stakeholder updates, escalations. Default for documented communication. |
| LinkedIn | `linkedin.md` | Thought leadership, industry insights, career updates, professional community engagement. |
| Twitter/X | `twitter.md` | Real-time industry commentary, research announcements, concise takes. Limited detail. |
| Instagram | `instagram.md` | Visual brand storytelling, behind-the-scenes content, team culture, speaking engagements. |
| Messaging | `messaging.md` | Quick questions, urgent clarifications, informal check-ins across WhatsApp, Teams, Slack. |
| Presentations | `presentations.md` | Conference talks, internal briefings, client pitches, training sessions. Structured arguments. |

## High-Profile Publications

| Channel | File | Use When |
|---------|------|----------|
| Press Release | `press-release.md` | Major announcements, product launches, funding news, executive changes. External distribution. |
| Internal Docs | `internal-docs.md` | Memos, reports, status updates, strategy documents. Confidential distribution. |
| Harvard Business Review | `hbr.md` | Narrative-driven business insights. Academic rigour. 2000-4000 words. Thought leadership at scale. |
| MIT Sloan Management Review | `mit-smr.md` | Research-backed practitioner insights. Technology-management focus. Data-heavy arguments. |
| Academic Journal | `academic-journal.md` | Peer-reviewed research. Methodology-transparent papers. 8000-15000 words. Rigorous evidence. |
| Book Proposal | `book-proposal.md` | Book publishing. Pitch to agents/publishers. Long-form narrative arc. Market validation. |
| Op-Ed | `op-ed.md` | Opinion pieces for newspapers/magazines. Strong thesis. Counter-argument acknowledgement. 600-800 words. |

## Technical Documentation

| Channel | File | Use When |
|---------|------|----------|
| Coding Docs | `coding-docs.md` | READMEs, API documentation, architecture decision records (ADRs), developer guides, code comments. |
| Data Lineage | `data-lineage.md` | Data flow diagrams, source-to-target mappings, transformation logic, data dictionaries, ETL/ELT docs. |
| Analytical Docs | `analytical-docs.md` | Methodology documentation, statistical analysis reports, model documentation, hypothesis docs. |
| Application Docs | `application-docs.md` | System architecture, deployment guides, runbooks, SLA documentation, user manuals, configuration guides. |
| Data Products | `data-products.md` | Data product specs, data contracts, SLA definitions, data quality frameworks, data mesh documentation. |
| AI Agents Docs | `ai-agents-docs.md` | Model cards, agent system prompts, evaluation reports, safety assessments, prompt engineering docs, RAG pipelines. |
| Skills Docs | `skills-docs.md` | Skill documentation, plugin specs, MCP server docs, integration guides, workflow documentation. |
| Version Control | `version-control.md` | Changelogs, release notes, migration guides, deprecation notices, semantic versioning docs. |
| Legal Disclaimers | `legal-disclaimers.md` | Terms of use, IP notices, license docs, data protection notices, liability disclaimers, copyright statements. |

---

## Analysis Templates

For consulting analysis deliverables, see `references/analysis-templates/_index.md`:

| Template | File | Use When |
|----------|------|----------|
| Use Cases | `use-cases.md` | Business/technology use case presentations, implementation scenarios, ROI justification. |
| Benchmark Analysis | `benchmark-analysis.md` | Competitive benchmarking, performance comparisons, best practice identification. |
| Gap Analysis | `gap-analysis.md` | Current vs. desired state assessment, capability gaps, compliance gaps, process maturity gaps. |
| SWOT Analysis | `swot-analysis.md` | Strategic planning, strengths/weaknesses/opportunities/threats assessment. |
| Cost-Benefit Analysis | `cost-benefit-analysis.md` | ROI calculations, TCO analysis, investment justification, business cases. |
| Maturity Assessment | `maturity-assessment.md` | Capability maturity models, digital/data/process/technology maturity scoring. |
| Stakeholder Analysis | `stakeholder-analysis.md` | Power/interest grids, influence mapping, RACI matrices, communication planning. |

---

## Structure of Each Channel File

Every channel template follows this consistent pattern:

1. **Goal** — Single clear success criterion
2. **Success Metrics (2025-2026)** — 3-5 quantified benchmarks
3. **Limits & Restrictions** — Technical + Platform/Context + GGP Etiquette
4. **Template** — Markdown structure for copy-paste starting point
5. **Examples** — Good (with explanation) + Bad (with explanation)
6. **Tactical Guidance** — Timing, frequency, formatting, best practices
7. **Pre-Publication Checklist** — Validation items including GGP markers check

---

**GGP markers and tone rules**: See QUICK_REFERENCE.md §1 (markers), §8 (professional etiquette), §9-12 (reputation protection) — always loaded before this file.

---

**Last Updated**: February 2026
**GGP Version**: 4.5
**Channels**: 22 templates + 7 analysis templates
**Maintenance**: Review quarterly for platform algorithm changes

---

## GGP Return to Main Flow

This index helped you identify the target channel. Next steps:

1. Load the specific channel file: `[channel].md`
2. The channel file contains its own return instruction → **Return to SKILL.md Phase 3**

**Do NOT skip the channel file. The index is navigation; the channel file is execution.**



---

# 2. Academic Journal
> Source: `references/channels/academic-journal.md`

# Academic Journal

## Goal

Peer-reviewed research contribution advancing knowledge in your field. Establish scholarly credibility, inform practice-oriented research communities, and create durable reference material in academic infrastructure.

## Success Metrics (2025-2026)

- **Publication**: Accepted in tier-1 peer-reviewed journal in field
- **Citations**: Cited 10+ times within 24 months of publication (field-dependent baseline)
- **Impact Factor**: Published in journal with impact factor >1.5 (field-dependent)
- **Scholar Community**: Article appears in scholar.google.com and cited in PhD research
- **Teaching**: Used as required or recommended reading in graduate programs
- **Credibility**: Strengthens academic and professional reputation; enables thought leadership

## Limits & Restrictions

### Technical
- **Word count**: 8000-15000 words (journal-specific; check guidelines)
- **Structure**: IMRaD (Introduction, Methods, Results, Discussion) or field-specific structure
- **Citations**: APA (social sciences), Chicago (humanities), or discipline-specific format
- **Methodology**: Transparent reporting of methods, sample size, statistical analysis
- **Data transparency**: Raw data or synthetic data made available (field norms vary)
- **Format**: Follows journal template exactly; deviation can cause desk rejection

### Platform/Algorithm
- **Editorial cycle**: 4-12 months from submission to acceptance/rejection (field-dependent)
- **Peer review**: 2-4 reviewers evaluate manuscript; revisions typically required
- **Desk rejection**: 30-50% of submissions desk rejected without review (based on fit/quality)
- **Resubmission**: If rejected, can revise and submit to alternative journal (typically 2-3 month wait)
- **Open access**: Some journals are open access (free); others charge subscription fees
- **Preprint culture**: Some fields allow preprint servers (arxiv, SSRN); check journal policy

### GGP Etiquette
- **Methodological transparency**: Report all methods, statistical tests, sensitivity analyses, limitations
- **Conflict of interest disclosure**: Declare funding sources, financial relationships, author contributions
- **Reproducibility**: Provide enough detail for other researchers to reproduce findings
- **Ethical approval**: Obtain IRB approval for human subjects research; report in methods
- **Data integrity**: Report null results as seriously as positive results
- **Fair attribution**: Cite prior work; acknowledge intellectual debt to prior researchers

## Template

```
[TITLE — Specific, hypothesis-containing]
The Role of Organisational Slack in Digital Transformation Adoption: A Multi-Level Analysis of 200 Firms

[ABSTRACT — 150-250 words, structured]
Background: Digital transformation initiatives often fail despite apparent organisational readiness. We propose that organisational slack—financial, human, and managerial resources available beyond those required for current operations—moderates adoption success. Methods: We conducted a multi-level analysis of 200 organisations over 36 months, measuring organisational slack dimensions and tracking digital transformation initiative outcomes. Analysis employed hierarchical linear modelling to account for firm-level clustering. Results: We found that organisations in the upper quartile of financial slack had 2.8x higher adoption rates (95% CI: 2.1-3.5) compared to lower-quartile organisations. Managerial slack showed similar effects. Human slack (talent availability) had weaker effects. Our moderation analysis indicated that slack effects were strongest in organisations with unclear digital strategy. Discussion: These findings suggest that transformation success depends not just on strategy clarity, but on resource availability to absorb implementation disruption. Implications are presented for practitioners and researchers. Keywords: organisational slack, digital transformation, adoption, change management

[INTRODUCTION SECTION — Build case for research]
[Broad context - narrowing]
Paragraph 1: Digital transformation has become strategic priority for enterprises globally. [Citation: Westerman et al., 2019; Berman et al., 2020]. Yet 60-70% of transformation initiatives fail to achieve objectives [Citation]. Understanding success factors is critical.

Paragraph 2: [What's known]
Prior research identifies several adoption drivers: executive commitment [Citation], clear strategy [Citation], change management capability [Citation]. However, researchers have underexplored the role of organisational resources in supporting transformation adoption.

Paragraph 3: [What's missing - your contribution]
We argue that organisational slack—financial resources, talent availability, and managerial attention beyond routine operations—plays a critical moderating role. Organisations with abundant slack can absorb implementation disruption; resource-constrained organisations may struggle despite strong strategy.

[Research question/hypothesis]
Research Question: To what extent does organisational slack predict digital transformation adoption? Hypothesis 1: Organisations with higher financial slack will demonstrate faster transformation adoption. Hypothesis 2: Managerial slack (leadership time and attention) will moderate the relationship between strategy clarity and adoption. Hypothesis 3: These effects will be strongest in organisations with unclear digital strategy.

[Contribution statement]
This research contributes to transformation literature by introducing organisational slack as a moderating variable. Methodologically, we employ multi-level analysis to account for firm-level clustering. Practically, findings suggest firms should assess resource availability alongside strategy clarity when planning transformations.

[METHODS SECTION — Transparency and replicability]
[Participants/Sample]
"Participants: We studied 200 organisations across five industries (software, retail, financial services, healthcare, manufacturing) with annual revenue between $50M and $5B. Industries chosen for variance in transformation readiness and digital maturity. Organisations were recruited through industry associations and consulting firm client lists. Participation was voluntary; firms received benchmark reports as incentive."

[Data Collection]
"Data collection occurred over 36 months (2021-2024) in three waves: baseline (2021), midpoint (2022), and endpoint (2024). Baseline survey assessed organisational slack dimensions (financial, human, managerial) and digital strategy clarity. Midpoint assessment tracked adoption milestones. Endpoint assessment measured adoption outcomes and business impact. We also conducted 30 in-depth case study interviews at organisations representing high-adoption and low-adoption cohorts."

[Measurement]
"Organisational Slack: We measured financial slack as ratio of liquid assets to total expenses (following Bourgeois, 1981). Managerial slack measured as actual hours leadership allocated to transformation initiative divided by routine management responsibilities (survey-based estimate). Human slack measured as percentage of workforce with digital capability and availability for transformation work.

Digital Strategy Clarity: Measured using 5-item scale assessing leadership alignment on digital objectives, investment priorities, and success metrics. Items: 'Our executive team has explicit agreement on what digital transformation means for our organisation' (1=strongly disagree, 5=strongly agree).

Adoption Outcomes: Measured as percentage of targeted organisational units that deployed new digital processes by endpoint. Also tracked speed of adoption (time from pilot to full deployment) and user engagement (utilisation rates of new systems)."

[Statistical Analysis]
"We employed hierarchical linear modelling (HLM) to account for firm-level clustering. Level 1 units were individuals/departments; Level 2 units were firms. Models tested main effects of slack dimensions and moderating effects on adoption rates. We computed 95% confidence intervals and reported effect sizes (Cohen's d or standardised beta coefficients). Sensitivity analyses tested robustness to alternative model specifications."

[Quality/Limitations]
"Limitations: This research is limited to larger organisations ($50M+ revenue); findings may not generalise to smaller firms. Industry sample is primarily developed-market firms; findings may not apply to emerging markets. Self-reported survey data on managerial slack; objective time-tracking data was not available. Time lag between baseline and endpoint may obscure temporal dynamics."

[Ethics]
"This research was approved by the [University] Institutional Review Board (#IRB2021-XXX). All participants provided informed consent. Data was anonymised and aggregated to protect firm identity."

[RESULTS SECTION — Data-forward, clear]
[Descriptive statistics]
"Descriptive statistics for organisational slack dimensions appear in Table 1. Mean financial slack ratio was 0.34 (SD=0.18), range 0.08-0.92. Organisations were relatively heterogeneous on managerial slack (mean 18% of leadership time allocated to transformation, SD=12%, range 3-45%). Human slack showed similar variance (mean 32% of workforce available, SD=24%, range 5-78%)."

[Main findings - hypothesis testing]
"Hypothesis 1 (Financial slack predicts adoption): Organisations in upper quartile of financial slack (ratio >0.46) achieved average adoption rate of 78% (95% CI: 72-84%) compared to lower quartile (ratio <0.22) which achieved 28% (95% CI: 21-35%). This difference is statistically significant (t(198)=6.2, p<.001), with Cohen's d=1.1 (large effect). Supporting Hypothesis 1."

"Hypothesis 2 (Managerial slack moderates strategy effect): We modelled adoption as function of strategy clarity, managerial slack, and their interaction. Main effect of strategy clarity: β=0.42, p<.001. Main effect of managerial slack: β=0.38, p<.001. Interaction effect: β=0.31, p=.003. In organisations with high managerial slack, the relationship between strategy clarity and adoption was 2.3x stronger than in organisations with low managerial slack. Supporting Hypothesis 2."

"Hypothesis 3 (Slack effects strongest with unclear strategy): Post-hoc analysis tested this prediction. We stratified sample by strategy clarity quartile. In lowest-clarity organisations, slack explained 48% of adoption variance (R²=.48). In highest-clarity organisations, slack explained 19% of variance (R²=.19). This pattern supports Hypothesis 3."

[Null findings or complexity]
"Human slack showed weaker effects than predicted. Main effect of human slack on adoption: β=0.18, p=.08 (not significant at p<.05). Exploratory analysis suggests human slack matters primarily for transformation velocity, not final adoption rate. Organisations with higher human slack achieved faster adoption (r=.34, p<.001) but similar endpoint rates as low-slack organisations."

[DISCUSSION SECTION — Interpretation and implication]
[Interpretation of findings]
"Our findings suggest that organisational slack serves as a buffer against transformation implementation disruption. Organisations with financial slack can maintain parallel systems during transition; those without it face binary choice (invest in new system, lose productivity). Organisations with managerial slack have leadership capacity to resolve adoption obstacles; resource-constrained leaders must choose among competing priorities.

Why did human slack show weaker effects than expected? We propose two mechanisms: (1) Digital transformation often requires different skills than current workforce possesses, making available talent less valuable than new talent; (2) Human slack matters for speed, not ultimate adoption; eventually organisations can redistribute work to enable adoption."

[Implications for theory]
"This research extends organisational slack theory into digital transformation context. Prior research established slack's role in organisational innovation [Citation]; our findings show similar effects in transformation adoption. We extend mechanisms: slack enables organisations to absorb short-term productivity losses during implementation disruption."

[Implications for practice]
For transformation leaders: When assessing organisational readiness, evaluate not just strategy clarity and sponsorship, but resource availability. If resources are constrained, either: (1) increase resources through reallocation or hiring; (2) reduce transformation scope; or (3) extend timeline to reduce implementation pace demands.

For practitioners designing transformation approaches: In resource-constrained organisations, adopt phased approaches that allow maintained productivity while transformation occurs. Avoid big-bang deployments that demand parallel operations and surge resource demand.

For executives funding transformations: Allocate not just technology budget, but implementation-support budget that accounts for productivity loss and leadership time. Underfunded transformations will fail regardless of strategy quality.

[Limitations and future research]
"Limitations we noted: generalisability beyond large firms, industry concentration, self-reported time allocation. Future research should examine whether slack effects vary by transformation type (technology-driven vs. process-driven vs. cultural). Longitudinal research tracking slack and adoption over longer timescales could clarify whether initial slack advantage sustains or erodes. Research in smaller organisations, emerging markets, and non-Western contexts would improve generalisability."

[CONCLUSIONS]
"Digital transformation success depends on alignment of three factors: clear strategy, executive commitment, and available resources. This study demonstrates that organisational slack—financial, managerial, and human resources available beyond routine operations—plays a critical moderating role. Organisations with abundant slack can absorb implementation disruption; resource-constrained organisations must adopt alternative approaches. For theory, we extend organisational slack into transformation context. For practice, we provide actionable diagnostic criteria for transformation readiness assessment."

[REFERENCES — Comprehensive, properly formatted]
[Sample citations in APA format]
Bourgeois, L. J. (1981). On the measurement of organisational slack. Academy of Management Review, 6(1), 29-39.

Westerman, G., Bonnet, D., & McAfee, A. (2019). Rethinking the internet of things: A scalable approach to connecting everything. Harvard Business Review Press.

[... additional citations, 40-60 typical for research article ...]

[APPENDIX — Supplementary material]
Table A1: Measurement scales (items and response options)
Table A2: Correlation matrix of all variables
Figure A1: Sample comparison to industry statistics
Robustness check results (alternative model specifications)
```

## Examples

### Good Abstract Example

"Workplace diversity initiatives frequently fail to improve decision-making quality or inclusion outcomes. This study tests whether explicit cognitive diversity (diverse thinking styles) differentially improves outcomes compared to demographic diversity alone. We analysed 85 product development teams over 18 months, measuring cognitive style diversity (via Kirton Adaption-Innovation inventory), demographic diversity (gender, ethnicity, background), and three decision outcomes: solution novelty, stakeholder satisfaction, and implementation success. Results: Cognitive diversity significantly predicted solution novelty (β=.42, p<.001) and implementation success (β=.38, p<.001); demographic diversity did not predict outcomes when cognitive diversity was controlled. Interaction analysis revealed that demographic diversity's effects were mediated by cognitive diversity. Findings suggest diversity initiatives should prioritise thinking-style diversity in team composition. Implications for hiring, team design, and diversity program effectiveness are discussed."

---

**Why this works:**
- Opens with practical problem and research question
- Reports sample, variables, and outcomes clearly
- Includes effect sizes and statistical significance
- Surprising insight: cognitive > demographic diversity for outcomes
- Clear implications

## Tactical Guidance

**Journal Selection Strategy:**

```
Tier 1: High impact (review cycles 6-12 months, acceptance rate 15-25%)
Examples: Academy of Management Journal, Organization Science, Strategic Management Journal
Risk: High rejection rate; but publication is most prestigious

Tier 2: Mid-tier (review cycles 4-8 months, acceptance rate 30-40%)
Examples: Journal of Management, Information & Organization, Organizational Research Methods
Balance: Good reach + reasonable publication likelihood

Tier 3: Specialized (review cycles 3-6 months, acceptance rate 40-50%)
Examples: [Field-specific journals in your specialty]
Benefit: Faster path; allows targeting niche audience

Strategy: Submit to Tier 1 first. If rejected, revise and submit to Tier 2. If rejected, submit to Tier 3. Allows authors to aim high while creating fallback path.
```

**Manuscript Quality Checklist Before Submission:**

- Methods section should be detailed enough for replication
- Results section should present data first, interpretation minimal
- Discussion should connect findings to literature and implications
- Acknowledge limitations explicitly; don't hide them
- Statistical reporting should be transparent: report null results with same vigor as positive results
- Peer-review manuscript with colleague in field before submission (increases quality, reduces revision cycles)

**Responding to Reviewer Comments:**

- View feedback as collaborative; reviewer goal is improving manuscript, not rejection
- Respond systematically to every comment; don't dismiss feedback
- If you disagree with comment, explain reasoning clearly (don't be defensive)
- When revising, track changes and provide response letter explaining each change
- Resubmission letter should reference specific comments and page numbers of changes

## Pre-Publication Checklist

- [ ] Title is specific and hypothesis-containing (not generic "A Study of X")
- [ ] Abstract is 150-250 words, follows IMRaD structure, includes sample size and key findings with effect sizes
- [ ] Introduction establishes gap in literature and clear research question
- [ ] Methods section is detailed enough for replication: participants, measures, procedures, analysis
- [ ] Measurement section justifies choice of instruments; reports reliability/validity data
- [ ] Statistical analysis approach explained with rationale (why HLM vs. OLS, etc.)
- [ ] Limitations section acknowledges constraints on generalisability
- [ ] Ethics approval obtained and reported (if human subjects research)
- [ ] Results section presents data first; minimal interpretation
- [ ] All statistical findings include: effect size, confidence intervals, p-values, sample size
- [ ] Null findings reported with same rigor as positive findings
- [ ] Discussion interprets findings in light of prior literature
- [ ] Implications addressed: theoretical and practical
- [ ] Future research directions identified
- [ ] References are complete and properly formatted per journal style
- [ ] Manuscript follows journal template exactly (tables, figures, headings, citations)
- [ ] Conflict of interest disclosure included (funding sources, financial relationships)
- [ ] Author contributions clearly stated
- [ ] GGP markers check: All claims supported by data [VERIFIED]. Effect sizes and confidence intervals reported [ESTIMATED]. Limitations acknowledged [CAUTION]. Inferences labelled [INFERENCE], not presented as findings.
- [ ] Peer-reviewed by colleague in field before submission
- [ ] Plagiarism-checked (use Turnitin or similar)

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 3. AI Agents Docs
> Source: `references/channels/ai-agents-docs.md`

# AI Agents & Model Documentation

## Goal
Enable reproducible AI system evaluation and safe deployment through transparent model cards, system prompt documentation, benchmark reporting, and bias assessment so stakeholders understand capabilities, limitations, and risks.

## Success Metrics (2025-2026)
- 100% of models have documented benchmark scores with confidence intervals
- All evaluation methodologies reproducible (code available, random seeds pinned)
- Zero hallucinations from models due to undocumented training data gaps (marked [INFERENCE])
- Agent safety assessments completed for 100% of production systems
- Bias testing documented for all protected attributes (gender, race, age)

## Limits & Restrictions

### Technical
- Benchmark scores must be reproducible with pinned code and seed
- Training data dates, sizes, and composition documented (not vague)
- Evaluation methodology must be testable (available benchmarks, defined metrics)
- Model limitations explicitly declared (no "works for all use cases")
- Hallucination risks identified and mitigated with specific guardrails

### Platform/Context
- Lives in model registry (MLflow, Weights & Biases), knowledge base, Hugging Face Model Cards
- Primary audience: ML engineers, product managers, compliance/risk teams, end users
- Lifecycle: Updated when model retrains; benchmarks re-run quarterly; safety reassessed annually
- Discoverability: Indexed by task, performance metric, safety rating

### GGP Etiquette
- Never fabricate benchmark scores — always report against named benchmarks with evidence
- Never hide evaluation methodology — declare sampling, train/test splits, cross-validation
- Mark all assumptions as [INFERENCE] (training data, model behaviour, failure modes)
- Flag hallucination risk areas explicitly (low-confidence outputs, extrapolation)
- Document bias testing for protected attributes; declare confidence in fairness claims

## Template

```markdown
# Model Card: [Model Name]

## Overview
[What task does this model perform? What is its primary use case?]

## Intended Use
- **Intended Users:** [Who should use this model?]
- **Intended Applications:** [Specific use cases]
- **Out-of-Scope Uses:** [What this model should NOT be used for]

## Model Details
- **Architecture:** [Model type, size, framework]
- **Training Data:** [Dataset name, source, date range, size]
- **Training Approach:** [Supervised/unsupervised, preprocessing, augmentation]
- **Validation:** [Train/test/val split, cross-validation strategy]

## Performance Metrics

### Benchmark Results

| Benchmark | Metric | Score | Confidence | Notes |
|---|---|---|---|---|
| [Benchmark Name] | Accuracy | 94.2% | 95% CI: [93.5%, 94.9%] | N=10,000 |
| [Benchmark Name] | Precision | 92.1% | 95% CI: [91.2%, 93.0%] | Class: positive |
| [Benchmark Name] | Recall | 89.3% | 95% CI: [88.1%, 90.5%] | Class: positive |

### Reproducibility
- **Code:** [GitHub commit hash]
- **Seed:** [Random seed value]
- **Environment:** Python 3.10, PyTorch 2.0.1
- **Compute:** GPU (NVIDIA A100), runtime ~24 hours

## Limitations & Biases

### Known Limitations
1. [Limitation and impact]
2. [Limitation and impact]

### Bias Assessment
- **Testing:** Yes, documented below
- **Protected Attributes Evaluated:** Gender, race, age, geography
- **Test Data:** [Dataset name, size, composition]

| Attribute | Group | Accuracy | Disparity | Notes |
|---|---|---|---|---|
| Gender | Male | 94.5% | +0.3% vs. baseline | Minimal |
| Gender | Female | 94.2% | 0.0% (baseline) | Baseline |
| Race | White | 94.8% | +0.6% vs. baseline | Minimal |
| Race | Black | 93.0% | -1.2% vs. baseline | [INFERENCE]: Sample size N=200; recommend larger validation set |

## Agent Configuration (If Applicable)

### System Prompt
\`\`\`
[Full system prompt; declare guardrails, escalation rules]
\`\`\`

### Tools & Capabilities
- [Tool 1]: [Purpose, input/output]
- [Tool 2]: [Purpose, input/output]

### Failure Modes & Guardrails
- Hallucination: [Detection method, mitigation]
- Out-of-distribution: [Edge cases, escalation rule]
- Adversarial: [Known jailbreak vectors, defenses]

## Safety Assessment

| Risk | Likelihood | Impact | Mitigation | Status |
|---|---|---|---|---|
| Hallucination | Medium | High | Fact-checking guardrail | [CONFIRMED] Implemented |
| Bias | Low | Medium | Fairness monitoring | [INFERENCE] Partial (gender tested, race TBD) |
| Prompt Injection | Medium | High | Input validation | [CONFIRMED] Implemented |

## Changelog
- v2.0.0 (2025-01-15): [Changes]
- v1.0.0 (2024-06-01): Initial release
```

## Examples

### Good Example
```markdown
# Model Card: Sentiment Classification v2.1

## Overview
RoBERTa-based sentiment classifier for customer feedback (positive/negative/neutral). Deployed in feedback analysis pipeline processing 50k+ messages/day.

## Intended Use
- **Intended Users:** Product and customer success teams
- **Applications:** Categorizing customer feedback for dashboards, trend analysis
- **NOT Intended For:** Individual decision-making, moderation enforcement, bias assessment research
- **Constraints:** English text only; accuracy on non-English drops to 67%

## Model Details
- **Architecture:** RoBERTa-large (355M parameters), fine-tuned classification head
- **Framework:** PyTorch 2.0.1; HuggingFace transformers 4.35.0
- **Training Data:**
  - Source: Customer feedback database + Twitter sentiment corpus
  - Size: 250,000 labelled examples
  - Date Range: Jan 2023 - Dec 2024 (current year training data only)
  - Split: 70% train, 15% validation, 15% test
  - Preprocessing: Lowercasing, URL removal, emoji handling (mapped to tokens)

## Performance Metrics

### Benchmark Results (Test Set)
| Benchmark | Metric | Score | 95% CI | Sample Size |
|---|---|---|---|---|
| Internal Test Set | Accuracy | 93.8% | [93.2%, 94.4%] | N=10,800 |
| Internal Test Set | Weighted F1 | 0.939 | [0.933, 0.945] | N=10,800 |
| Internal Test Set | Precision (Positive) | 94.2% | [93.1%, 95.3%] | N=3,200 (positive examples) |
| Internal Test Set | Recall (Positive) | 92.1% | [90.8%, 93.4%] | N=3,200 |

### Real-World Performance (Production Monitoring, 2025-01-15)
| Metric | Value | Trend |
|---|---|---|
| User Feedback Disagreement Rate | 8.2% | ↓ (was 9.1% in Dec) |
| False Positives (negative misclassified as positive) | 5.1% | ↔ Stable |
| False Negatives (positive misclassified as negative) | 2.8% | ↓ (improved after v2.0) |

### Reproducibility
- **Code:** [GitHub: customer-feedback-models/sentiment-v2.1](https://github.com/company/models)
- **Commit:** a1b2c3d (Jan 10, 2025)
- **Random Seed:** 42
- **Training Script:** \`scripts/train_sentiment.py\`
- **Environment:** Python 3.10, PyTorch 2.0.1, CUDA 11.8
- **Compute:** NVIDIA A100 GPU, 18-hour training run
- **Data Snapshot:** Frozen 2025-01-10; available in S3: s3://ml-data/sentiment-v2.1/

**Reproduction Command:**
\`\`\`bash
git clone https://github.com/company/models.git
cd models && git checkout a1b2c3d
python scripts/train_sentiment.py --seed 42 --config configs/sentiment-v2.1.yaml
# Produces checkpoint with same test accuracy ± 0.1%
\`\`\`

## Limitations & Known Issues

1. **Language Limitation:** Trained on English only
   - Non-English accuracy: ~67% (not recommended)
   - Mitigation: Filter non-English with language detection model first
   - Impact: HIGH — produces incorrect labels on non-English text

2. **Sarcasm Handling:** Often misclassifies sarcastic negative feedback as positive
   - Frequency: ~15% of sarcastic examples misclassified
   - Mitigation: Manual review queue for human-uncertain predictions (confidence <0.70)
   - Impact: MEDIUM — affects trend analysis but caught by human review

3. **Demographic Data Gap:** Training set 68% US-based feedback; international usage introduces bias
   - Impact: MEDIUM — see bias assessment below
   - Mitigation: Quarterly retraining with balanced geographic data
   - Status: [INFERENCE] — confidence low; recommend validation study

## Bias Assessment

### Methodology
- **Data:** 3,000-sample stratified random subset of test set
- **Attributes:** Gender (inferred from name), geography (IP location), age (inferred from writing style)
- **Method:** Accuracy computed per demographic group; disparity measured vs. majority group

### Results

| Attribute | Group | Sample Size | Accuracy | Disparity vs. Baseline | Assessment |
|---|---|---|---|---|
| **Inferred Gender** | Female | 1,400 | 92.1% | -1.7% (significant) | [INFERENCE]: Gender classification inferred; low confidence |
| | Male | 1,450 | 93.8% (baseline) | — | Baseline |
| **Geography** | US | 2,100 | 94.2% | +0.4% (baseline) | [CONFIRMED] VERIFIED |
| | International | 900 | 91.8% | -2.4% (significant) | [GAP] GAP: Recommend collecting more intl. data |
| **Estimated Age** | 18-35 | 1,200 | 94.0% | +0.2% | [CONFIRMED] VERIFIED |
| | 35-55 | 1,100 | 93.9% | +0.1% | [CONFIRMED] VERIFIED |
| | 55+ | 700 | 92.1% | -1.9% (significant) | [INFERENCE]: Age inference low confidence; small sample |

### Fairness Status
- **Gender:** [INFERENCE] — Disparity detected but inference method unreliable; recommend human-labelled fairness study
- **Geography:** [GAP] GAP — International performance gap; priority for next training cycle
- **Age:** [CONFIRMED] VERIFIED — Minimal disparity; acceptable (age inference method validated separately)

### Mitigation Plan
1. Collect 5,000 international examples for v3.0 training (ETA: Q2 2025)
2. Use validated human gender labels (not inferred) for fairness evaluation
3. Quarterly disparity monitoring (automated test suite)

## Agent Configuration

### System Prompt
\`\`\`
You are a customer feedback sentiment analyser. Your job is to classify feedback as POSITIVE, NEGATIVE, or NEUTRAL.

Rules:
1. Analyse the literal text; don't infer intent beyond what's written.
2. If uncertain (confidence < 0.60), respond with "UNCERTAIN: [reason]"
3. Never make up context; cite specific words from feedback.
4. If feedback is primarily sarcasm, mark as SARCASM_ALERT.

Example:
Input: "This product is amazing!" → POSITIVE
Input: "Yeah, just what I needed, another bug." → SARCASM_ALERT, likely NEGATIVE
Input: "The app works." → NEUTRAL

Output Format:
{
  "sentiment": "POSITIVE|NEGATIVE|NEUTRAL|UNCERTAIN|SARCASM_ALERT",
  "confidence": 0.0-1.0,
  "reasoning": "[2-3 sentences explaining classification]"
}
\`\`\`

### Tools
- **Tool 1: Confidence Check** — If confidence < 0.60, escalate to human review
- **Tool 2: Sarcasm Detection** — Flag likely sarcasm; confirm with human
- **Tool 3: Language Detection** — Alert if non-English detected

### Failure Modes & Guardrails

| Failure Mode | Symptom | Likelihood | Mitigation |
|---|---|---|---|
| Hallucination (fabricated reasoning) | Explanation doesn't match input text | Low (fine-tuned model) | [CONFIRMED] Validated: Only uses words from input |
| Adversarial Prompt Injection | Model misled by embedded instructions in feedback | Medium | [INFERENCE] — Guardrail in place; not stress-tested against sophisticated attacks |
| Out-of-Distribution (non-English) | Low confidence but produces label | Medium | [CONFIRMED] Language detection filters non-English before classification |
| Bias Amplification | Demographic disparity in real-world usage | Medium (see bias assessment) | [INFERENCE] Monitoring enabled; remediation in progress |

## Safety Assessment

| Risk | Type | Likelihood | Impact | Status | Owner |
|---|---|---|---|---|---|
| Hallucinated Reasoning | Integrity | Low | Medium | [CONFIRMED] Mitigated: Guardrail verified | @ml-team |
| Bias Against Minorities | Fairness | Medium | Medium | [INFERENCE] Partial: Monitoring active; full remediation planned Q2 | @ml-team |
| Non-English Misclassification | Accuracy | Medium | Low | [CONFIRMED] Mitigated: Language detection filter | @ml-team |
| Prompt Injection | Security | Medium | Low | [INFERENCE] Partial: Basic guards; not tested vs. adversarial | @security-team |

## Changelog

**v2.1 (2025-01-10)**
- Improvement: Fine-tuned on sarcasm examples (15% → 8% sarcasm misclassification rate)
- Improvement: Confidence calibration improved (ECE reduced from 0.08 to 0.04)
- Addition: Language detection guardrail
- Data: +20k examples from Q4 2024 feedback

**v2.0 (2024-09-01)**
- Change: Switched from BERT to RoBERTa (accuracy +2.1%)
- Improvement: Added neutral class (was binary pos/neg)
- Data: Retrained on 2024 feedback only (removed 2023 data)

**v1.0 (2024-01-15)**
- Initial release; binary classification (pos/neg)

## Questions & Support
- Questions about performance: @ml-team
- Questions about bias/fairness: @ml-team + @ethics-committee
- Questions about deployment: @ml-ops
```

**Why this works:** Benchmarks reproducible. Training data documented (date range, size). Bias tested and results reported transparently. Failure modes listed with mitigations. Confidence intervals provided. Sarcasm limitation explicit.

---

### Bad Example
```markdown
# Sentiment Model

Model: RoBERTa-based

Performance: Very good, achieves 94% accuracy

Training: Done on feedback data

Limitations: Sometimes gets sarcasm wrong

Notes: Works well. Ready for production.
```

**Why this fails:** No benchmark name, no CI. Training data vague (when, what, how much?). Bias assessment missing. Failure mode ("gets sarcasm wrong") not quantified. No reproducibility. No safety assessment.

## Tactical Guidance

**Model Cards:**
- Document training data: size, date range, composition (how representative?)
- Benchmark against named, standard benchmarks (not custom measures)
- Confidence intervals required (shows statistical rigor)
- Declare intended use; don't claim "works for everything"

**Hallucination Prevention:**
- Limit model responses to fact-checking against known sources
- Require confidence scores; flag low-confidence outputs for human review
- Monitor production outputs for semantic drift
- Provide guardrails: "Only use information from: [sources]"

**Bias Testing:**
- Define protected attributes upfront (gender, race, age, geography)
- Use stratified sampling; ensure adequate group sizes
- Report disparity metric (% difference vs. baseline or between groups)
- Document confidence in fairness claims (small samples = low confidence)

**Agent Safety:**
- System prompt: explicit guardrails, escalation rules, tool usage limits
- Tool definitions: clear purpose, inputs, outputs, error modes
- Monitoring: log all uses, flag edge cases, alert on anomalies
- Quarterly safety review: test for adversarial inputs, jailbreak attempts

## Pre-Publication Checklist
- [ ] Benchmark scores reported with confidence intervals (not point estimates)
- [ ] Training data documented: source, size, date range, preprocessing
- [ ] Evaluation methodology reproducible: code available, random seed pinned
- [ ] Model limitations documented; intended use clear
- [ ] Bias testing completed for all protected attributes
- [ ] Bias test sample sizes adequate (≥300 per group); confidence documented
- [ ] Hallucination risk areas identified and guardrailed
- [ ] Agent system prompt includes escalation rules and tool limits
- [ ] Safety assessment completed; risks and mitigations documented
- [ ] Failure modes testable and reproducible (not hypothetical)
- [ ] Real-world performance monitored and compared to benchmarks
- [ ] Changelog updated with each version
- [ ] GGP markers applied: [CONFIRMED] for verified benchmarks/bias results, [INFERENCE] for inferred limitations, [GAP] for gaps
- [ ] Peer review or external validation completed

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 4. Analytical Docs
> Source: `references/channels/analytical-docs.md`

# Analytical Documentation

## Goal
Enable reproducible statistical analysis through transparent methodology, declared assumptions, and verifiable findings so readers can assess credibility and replicate results.

## Success Metrics (2025-2026)
- 100% of statistical claims include confidence intervals or p-values
- All sample sizes disclosed upfront (no retroactive post-hoc analysis claims)
- Methodology reproducible: code available, random seeds pinned
- Assumptions documented and tested (normality, homogeneity of variance)
- Zero misleading findings: effect sizes reported alongside p-values (not p-values alone)

## Limits & Restrictions

### Technical
- Statistical results must include sample size, confidence interval, and effect size
- P-values require pre-registered hypotheses (no p-hacking, multiple comparison corrections)
- Models require feature descriptions, training/test splits, and cross-validation results
- Data assumptions (normality, independence, homogeneity) must be tested and reported
- All analyses reproducible with pinned random seed and versioned code

### Platform/Context
- Lives in reports, research wikis, Jupyter notebooks, model documentation
- Primary audience: analysts, data scientists, business stakeholders, regulators
- Lifecycle: Versioned with analysis date; updated if methodology changes
- Discoverability: Linked from dashboards; searchable in knowledge base

### GGP Etiquette
- Never report p-value without sample size (appears significant by luck at scale)
- Never report p-value without effect size (statistically significant ≠ practically significant)
- Declare all comparisons made and corrections applied (Bonferroni, FDR)
- Flag assumptions: if violated, are conclusions still valid?
- Mark findings as [INFERENCE] if extrapolating beyond sample population

## Template

```markdown
# Analysis: [Title]

## Executive Summary
[One paragraph: what did we ask, what did we find, what should decision-makers do?]

## Objective
[Research question in testable form]

## Methodology

### Data Sources
- Dataset: [name, version, date range]
- Sample size: N = [exact number, not "large"]
- Population: [geographic, temporal, demographic scope]

### Analysis Method
[Statistical test, model type, or methodology]

### Assumptions
1. [Assumption] — Tested: [Yes/No], Result: [passed/violated]
2. [Assumption] — Tested: [Yes/No], Result: [passed/violated]

### Limitations
- [Known constraint affecting generalisability]

## Findings

### Primary Result
[Finding] (95% CI: [lower, upper], p < 0.05, n=1200)

**Effect Size:** [Cohen's d = X, η² = Y, odds ratio = Z]

### Secondary Results
[Supporting findings with full statistics]

## Recommendations
[Action-oriented conclusions]

## Reproducibility
- Code: [GitHub link, commit hash]
- Random Seed: 42
- Environment: Python 3.9, pandas 1.3.2
```

## Examples

### Good Example
```markdown
# Analysis: Impact of Email Frequency on Unsubscribe Rate

## Executive Summary
Testing 3 email frequencies (daily, weekly, monthly) on 12,000 users shows weekly emails reduce unsubscribes by 15% (95% CI: 8-22%) vs. daily emails. Monthly emails increase unsubscribes 8% vs. weekly. Recommend weekly cadence.

## Objective
Does email frequency (daily vs. weekly vs. monthly) causally affect unsubscribe rate?

## Methodology

### Data Sources
- **Dataset:** User email interactions, Jan 2024 - Dec 2024
- **Sample size:** N = 12,000 users
- **Assignment:** Randomized controlled trial, 4,000 per group
- **Duration:** 90-day treatment period post-assignment

### Analysis Method
- **Statistical Test:** Two-sample proportions test with chi-square (Pearson's χ²)
- **Multiple Comparisons:** Bonferroni correction (3 pairwise comparisons → α = 0.0167)
- **Robustness:** 10,000 bootstrap resamples

### Assumptions
1. **Independence:** Users don't interact (no network effects) — Tested: Yes, Verified ✓
2. **Random assignment:** Groups balanced on baseline metrics (age, tenure, engagement) — Tested: Yes, ANOVA p > 0.05 ✓
3. **No user attrition bias:** <2% dropped out equally across groups — Tested: Yes ✓

### Limitations
- Sample: US English speakers only; may not generalise to international audiences
- Duration: 90 days; long-term effects (6+ months) unknown
- External: Conducted during low-volume promotional period; peak season behaviour may differ

## Findings

### Primary Result: Unsubscribe Rate by Frequency

| Email Frequency | Unsubscribe Rate | 95% CI | Count |
|---|---|---|---|
| Daily | 8.2% | (7.1%, 9.3%) | 4000 |
| Weekly | 7.0% | (5.8%, 8.2%) | 4000 |
| Monthly | 7.6% | (6.4%, 8.8%) | 4000 |

**Weekly vs. Daily:**
- Difference: -1.2 percentage points (15% relative reduction)
- 95% CI: (-2.1%, -0.3%)
- χ² (1, N=8000) = 6.12, p = 0.0135
- **Effect Size:** Cohen's h = 0.054 (small effect, but meaningful at scale)
- [CONFIRMED] **Conclusion:** Statistically significant; survives Bonferroni correction

**Monthly vs. Weekly:**
- Difference: +0.6 percentage points (8% relative increase)
- 95% CI: (-0.4%, 1.6%)
- χ² (1, N=8000) = 1.84, p = 0.1748
- [CONFIRMED] **Conclusion:** Not significant; cannot reject null hypothesis

### Secondary Findings: Engagement Metrics

| Metric | Daily | Weekly | Monthly | p-value |
|---|---|---|---|---|
| Open Rate | 34.2% | 41.8% | 38.5% | 0.0002 |
| Click Rate | 12.1% | 15.3% | 13.2% | 0.0031 |
| Spam Reports | 2.1% | 0.8% | 1.2% | 0.0001 |

**Finding:** Weekly emails show highest engagement (opens +22% vs. daily, p < 0.001). Daily emails incur 2.6x more spam complaints.

## Recommendations

1. **Shift majority to weekly cadence** — Reduces unsubscribes while improving engagement
2. **Discontinue daily emails** — High unsubscribe and spam rates outweigh volume benefits
3. **Consider monthly for re-engagement:** Test monthly cadence on inactive segments (> 30 days no open)
4. **Monitor seasonality:** Replicate analysis during peak promotional periods (Nov-Dec)

## Reproducibility

- **Code:** [GitHub: analytics-team/email-frequency-test](https://github.com/analytics-team/email-frequency-test)
- **Commit:** a1b2c3d (Jan 15, 2025)
- **Notebook:** \`analysis/email_frequency_rct.ipynb\`
- **Random Seed:** 42
- **Environment:** Python 3.10, scipy 1.9.3, numpy 1.23.5
- **Data Snapshot:** 2025-01-15 (frozen for reproducibility)

### How to Reproduce
\`\`\`bash
git clone https://github.com/analytics-team/email-frequency-test.git
cd email-frequency-test
pip install -r requirements.txt
python -m jupyter notebook analysis/email_frequency_rct.ipynb
# Run all cells; outputs match results above ✓
\`\`\`

## Peer Review
- **Reviewed by:** @statistical-council (2025-01-20)
- **Methodology:** Approved ✓
- **Findings:** Approved ✓
- **Recommendations:** Approved ✓
```

**Why this works:** Sample size upfront. All statistics include CI and effect size. Assumptions tested. Limitations transparent. Bonferroni correction applied. Code reproducible and pinned. Peer reviewed.

---

### Bad Example
```markdown
# Email Analysis

## Findings
Email frequency matters. Weekly emails are better than daily (p = 0.01). Monthly emails might work too.

## Recommendation
Use weekly emails. This will improve performance.
```

**Why this fails:** No sample size. No confidence interval. No effect size. No assumptions documented. P-value without context suggests p-hacking. Vague "might work." No code. Not reproducible.

## Tactical Guidance

**Hypothesis Registration:**
- Declare primary hypothesis before seeing data (pre-registration on OSF, AsPredicted)
- Distinguish pre-planned from exploratory analyses
- Exploratory findings labelled [INFERENCE] and marked for validation study

**Statistical Reporting:**
- Always report: n, mean (or proportion), SD (or CI), test statistic, p-value, effect size
- Confidence intervals more informative than p-values alone
- Report effect sizes in standardised units (Cohen's d, η², OR) for cross-study comparison

**Model Documentation:**
- Features: name, type, source, transformation, missingness handling
- Training data: date range, size, sampling strategy, class balance
- Validation: train/test split ratio, k-fold details, holdout set date range
- Performance: accuracy, precision, recall, AUC-ROC (not accuracy alone for imbalanced data)
- Bias: stratified performance by demographic groups

**Assumption Testing:**
- Normality: Shapiro-Wilk test for small n, Q-Q plots for visual inspection
- Homogeneity: Levene's test (ANOVA) or Welch's test (robustness)
- Independence: Check for repeated measures, clustering, autocorrelation
- Document violation impact: "If violated, use non-parametric test"

**Code Reproducibility:**
- Pin Python/R version, all package versions in requirements.txt or renv.lock
- Use fixed random seed (report it)
- Commit code before running analysis; don't commit generated results
- Publish data snapshot (or script to regenerate it) alongside code

## Pre-Publication Checklist
- [ ] Research question stated as testable hypothesis
- [ ] Sample size disclosed upfront
- [ ] All statistical results include 95% CI (not just p-value)
- [ ] Effect sizes reported (Cohen's d, η², odds ratio, etc.)
- [ ] All assumptions listed and tested (normality, independence, etc.)
- [ ] Multiple comparisons corrections applied (Bonferroni, FDR) if >1 test
- [ ] Limitations section acknowledges generalisability constraints
- [ ] Findings marked [CONFIRMED] if within registered hypothesis, [INFERENCE] if exploratory
- [ ] Code and random seed pinned and reproducible
- [ ] Peer review completed and approved
- [ ] No p-hacking language ("looked at the data and found...") — pre-registered findings only

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 5. Application Docs
> Source: `references/channels/application-docs.md`

# Application Documentation

## Goal
Provide operators, developers, and incident responders with complete, tested deployment and runbook documentation so they can deploy, troubleshoot, and recover systems without guesswork.

## Success Metrics (2025-2026)
- All deployment commands tested in staging before publication (0% "this should work")
- 100% of URLs/ports/configs verified against actual infrastructure
- Runbooks reduce mean time to resolution (MTTR) by 30%
- New operator can deploy from documentation alone in <60 minutes
- Zero incidents caused by stale documentation (quarterly verification)

## Limits & Restrictions

### Technical
- All commands must be tested against actual environment (staging, then canary production)
- URLs, ports, hostnames, credentials placeholder format validated before publication
- System architecture diagrams must match current deployment (verified by deployment tool outputs)
- Configuration examples must work with pinned versions (Node 18.12.0, not ^18)
- Rollback procedures must be executable and tested quarterly

### Platform/Context
- Lives in `/docs/deployment/`, `/docs/runbooks/`, wiki, README
- Primary audience: platform engineers, SREs, on-call engineers, security teams
- Lifecycle: Updated with each infrastructure change; tested quarterly
- Discoverability: Indexed by incident type in runbook system; linked from alerts

### GGP Etiquette
- Never fabricate system specifications (memory, CPU, disk) — source from CloudFormation, Terraform
- Never invent port numbers or URLs — verify against actual service registry or DNS
- Flag single points of failure explicitly (SPOF: database failover, DNS provider)
- Mark all assumptions about runtime environment as [INFERENCE] until verified
- Declare deployment risks: breaking changes, data loss potential, service interruption window

## Template

```markdown
# [System Name] — Architecture & Deployment

## System Overview
[What does this system do? Who depends on it?]

## Architecture Diagram
\`\`\`
[ASCII or Mermaid diagram showing components]
\`\`\`

## Components

| Component | Type | Role | Failover | Scaling |
|---|---|---|---|---|
| API Server | Compute | Request handling | ALB + ASG | Horizontal (2-10 instances) |
| RDS PostgreSQL | Database | Transactional store | Multi-AZ failover (automatic) | Vertical only |
| Redis Cache | Cache | Session/query cache | Single node; no failover [RISK] | Manual provisioning |
| S3 Bucket | Storage | Asset storage | Multi-region replication | Unlimited |

## Dependencies & Critical Services
- AWS RDS Postgres (required: v14.7+)
- Redis cluster (required; no local fallback)
- SQS queue for async jobs (degraded mode: process inline)

## Deployment Prerequisites
- Terraform v1.4.0+
- AWS CLI v2.13.0+
- SSH access to bastion host
- Valid AWS credentials (assume-role: prod-deploy)

## Deployment Guide

### Staging Deployment
\`\`\`bash
# 1. Validate infrastructure changes
terraform plan -var-file=staging.tfvars

# 2. Apply and verify
terraform apply -var-file=staging.tfvars
terraform output api_url

# 3. Run smoke tests
curl -s https://[api_url]/health | jq .
\`\`\`

### Production Deployment
\`\`\`bash
# 1. Create change ticket: [Jira link]
# 2. Announce deployment window: #devops Slack
# 3. Verify no deploys in flight: aws codepipeline list-pipelines

# 4. Blue-green deployment
terraform plan -var-file=prod.tfvars -target=aws_autoscaling_group.blue
# Verify blue group healthy before switching

# 5. Monitor
watch -n 5 'aws autoscaling describe-auto-scaling-groups --query "AutoScalingGroups[*].[AutoScalingGroupName, Instances[*].HealthStatus]"'
\`\`\`

### Rollback Procedure
\`\`\`bash
# 1. Identify broken version: git log --oneline | head -5
# 2. Revert: git revert [commit-hash]
# 3. Redeploy: terraform apply (reverted code)
# 4. Verify: aws elasticloadbalancing describe-target-health
\`\`\`

## Runbook: [Incident Type]

### Symptoms
[What does the user/monitoring observe?]

### Severity
- **SEV-1:** Service down, data loss risk
- **SEV-2:** Degraded (higher latency/errors), users impacted
- **SEV-3:** Minor issue, workaround available

### Root Causes
[Possible causes with detection steps]

### Resolution Steps
1. [Step]
2. [Step]
3. Verify: [How to confirm resolution]

### Escalation
If unresolved after 15 minutes → page on-call engineer (PagerDuty)
```

## Examples

### Good Example
```markdown
# Payment API — Deployment & Runbooks

## System Overview
Processes customer payments via Stripe; validates, authorises, and captures transactions. Dependency for checkout and billing systems (2 downstream services). Handles $2M/day in transactions.

## Architecture
\`\`\`
       API Gateway (ALB)
             ↓
   [api-1, api-2, api-3] (ASG: min=2, max=10)
             ↓
   RDS PostgreSQL (Multi-AZ failover)
       ↓           ↓
   [Primary]   [Replica]
             ↓
   Redis Sentinel cluster (3 nodes, auto-failover)
             ↓
   External: Stripe API (via API key rotation)
\`\`\`

## Critical Dependencies
| Service | Required | Failover | Location |
|---|---|---|---|
| RDS Postgres v14.7 | YES | Automatic (Multi-AZ) | us-east-1 |
| Redis Sentinel | YES | Automatic (2/3 quorum) | us-east-1 |
| Stripe API | YES (real-time auth) | None (degrades to manual review) | External |
| CloudWatch Logs | NO (degraded: console logs only) | None | us-east-1 |

## Pre-Deployment Checklist
- [ ] Code review approved (2+ approvals)
- [ ] Unit tests passing (pytest 95%+ coverage)
- [ ] Integration tests passing (Stripe sandbox environment)
- [ ] Performance tests run (p95 latency < 500ms)
- [ ] Staging deployment successful
- [ ] Database migration tested (rollback script available)

## Production Deployment

### Blue-Green Deployment Process
\`\`\`bash
# 1. Build and push image
docker build -t payment-api:v2.3.1 .
docker push 123456789.dkr.ecr.us-east-1.amazonaws.com/payment-api:v2.3.1

# 2. Create new "blue" launch template (parallel to current "green")
aws ec2 create-launch-template \
  --launch-template-name payment-api-blue-v2.3.1 \
  --version-description "Stripe rate-limit handling, bug fix"

# 3. Launch blue ASG (parallel to green)
# ASG: payment-api-blue, 2-4 instances, same AZs as green

# 4. Verify blue instances healthy
# ✓ All instances: running, passing ELB health checks
aws elb describe-instance-health --load-balancer-name payment-api-lb

# 5. Gradual traffic shift (ALB: 10% → 50% → 100%)
# Monitor: CloudWatch error rate, latency, business metrics (transaction success rate)

# 6. If green stable: terminate green ASG
# If issues detected: re-route 100% to green, keep blue for debugging
\`\`\`

### Deployment Windows & Blackout Periods
- Daily: Mon-Fri 9am-4pm UTC (peak transaction time) — HIGH RISK
- Blackout: Dec 20 - Jan 5 (holiday shopping)
- Preferred: Sun-Mon 2am-4am UTC (lowest volume)

### Post-Deployment Verification
\`\`\`bash
# 1. Health check
curl -s https://payment-api.internal/health | jq .

# 2. Smoke test (Stripe sandbox)
TEST_CARD=4242424242424242 TEST_AMOUNT=1050 pytest tests/smoke_test.py

# 3. Monitor metrics
# - Error rate: target <0.1%
# - p95 latency: target <500ms
# - Stripe API errors: target 0
watch -n 10 'curl -s https://prometheus.internal/api/v1/query?query=payment_api_error_rate'
\`\`\`

---

## Runbook: Database Connection Pool Exhaustion

### Symptoms
- Errors: "FATAL: too many connections"
- API response time: > 5 seconds (normally 100-200ms)
- CloudWatch alarm: `payment-api-db-connections-exhausted`

### SEV-2 (Degraded; transactions delayed but not lost)

### Root Causes
1. Database query hanging (lock contention) → connections not released
2. Deployment with leaking connection pooling code
3. RDS max_connections exceeded (default 100; should be 500+)

### Immediate Resolution (5-10 min)
\`\`\`bash
# 1. Check pool state
psql -h payment-db.internal -U admin -c "SELECT count(*) FROM pg_stat_activity WHERE state='active';"
# Expected: 20-50 active; if >200, pool leak suspected

# 2. Identify long-running queries
psql -h payment-db.internal -U admin -c "\
SELECT query, duration FROM pg_stat_statements \
WHERE mean_exec_time > 5000 \
ORDER BY mean_exec_time DESC LIMIT 10;"

# 3. If suspected connection leak in code:
# Redeploy previous stable version
git log --oneline | head -5  # Identify last known-good commit
git revert [commit-hash]
terraform apply -auto-approve

# 4. Monitor recovery
watch -n 2 'curl -s https://prometheus.internal/query?expr=pg_stat_activity_count'
# Target: <100 active connections, error rate drops below 0.1%
\`\`\`

### If Unresolved After 15 Minutes
1. Page on-call database engineer (PagerDuty: #database-oncall)
2. Prepare for manual intervention: kill idle connections, scale RDS up temporarily
3. Initiate incident post-mortem (SEV-2 requires root cause analysis within 24h)

### Post-Resolution
- [ ] Root cause identified and fixed
- [ ] Code review: connection pool logic
- [ ] Monitoring alert sensitivity reviewed
- [ ] Post-mortem documented in [incident ticket]
```

**Why this works:** Architecture matches actual deployment. All commands tested (not pseudocode). Runbook includes specific symptoms, detection, resolution steps, and escalation. Failover modes documented. Rollback procedure executable.

---

### Bad Example
```markdown
# Payment API Deployment

## How to Deploy
\`\`\`bash
push the code to prod
terraform apply (if needed)
\`\`\`

## If Something Goes Wrong
Restart the service or check the logs. Database might need more memory sometimes.

## Runbook
If payment errors happen, check if Stripe is down. Then restart the app maybe?
```

**Why this fails:** Commands not executable (pseudocode). No testing mentioned. No prerequisites. No actual error handling. Runbook vague and untestable. Single point of failure (database) not mentioned.

## Tactical Guidance

**Architecture Diagrams:**
- Use CloudFormation/Terraform outputs to generate; don't hand-draw
- Link to actual infrastructure as code (GitHub commit hash)
- Update with each deployment; stale diagrams are dangerous

**Deployment Commands:**
- Test in staging first, capture actual output
- Use IaC tools (Terraform, CloudFormation) to ensure repeatability
- Document environment variables and their values
- Include verification steps after each major step

**Runbooks:**
- Start with actual alert rules (extract symptoms from Datadog/CloudWatch configuration)
- Test resolution steps in staging before publication
- Include estimated time to resolve (5 min, 30 min, etc.)
- Escalation threshold: if unresolved after X minutes, page on-call engineer

**Configuration Management:**
- Version all configuration (git-tracked or SSM Parameter Store)
- Document "as-deployed" values (not templates) quarterly
- Alert if config drifts from version control

**Disaster Recovery:**
- Test failover quarterly (documented, scheduled tests)
- Document RTO (Recovery Time Objective) and RPO (Recovery Point Objective) per service
- Keep runbook for data recovery scenarios

## Pre-Publication Checklist
- [ ] All commands tested in actual staging environment (not pseudocode)
- [ ] URLs, ports, hostnames verified against actual infrastructure
- [ ] Configuration examples match deployed version (not templates)
- [ ] Architecture diagram matches current deployment (CloudFormation/Terraform output)
- [ ] Deployment tested end-to-end with verification steps
- [ ] Rollback procedure tested and executable
- [ ] Runbook symptoms based on actual alert rules
- [ ] Resolution steps tested (not best-guesses)
- [ ] Escalation path documented (who to page, when)
- [ ] Single points of failure identified and marked [RISK]
- [ ] Database/breaking changes documented with migration steps
- [ ] Deployment windows specified (peak vs. off-peak times)
- [ ] Quarterly verification scheduled (document in team calendar)

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 6. Book Proposal
> Source: `references/channels/book-proposal.md`

# Book Proposal

## Goal

Pitch-ready proposal to agents and publishers demonstrating book concept, market demand, author platform, and draft execution quality. Secure publishing contract and advance for long-form narrative exploration of research or expertise.

## Success Metrics (2025-2026)

- **Agent Acquisition**: 2-3 agents committed to pitching manuscript to publishers
- **Publisher Interest**: 2+ publishers enter negotiations or request full manuscript
- **Advance**: Secure advance payment reflecting market confidence in concept
- **Timeline**: Contract signed within 6 months of agent engagement
- **Platform Growth**: Proposal submission drive builds audience to 10K+ email subscribers or social followers
- **Book Sales**: Published book sells 5K+ copies in first year (varies by category)

## Limits & Restrictions

### Technical
- **Proposal length**: 25-40 pages total
- **Components**: Overview, market analysis, competitive titles, chapter outline, author platform, 1-2 sample chapters
- **Formatting**: Double-spaced, 12pt font (Times New Roman or similar), 1-inch margins
- **Sample chapters**: 25-50 pages, polished and professional
- **Visual elements**: 2-3 sample graphics or diagrams (if relevant)
- **Author bio**: 1 page, positioning for target audience
- **Submission**: PDF format; sent to agents with query letter

### Platform/Algorithm
- **Agent ecosystem**: 200+ agents across major publishers; targeting matters (match genre/category)
- **Response timeline**: Agents respond within 6-12 weeks (often longer); persistence required
- **Multiple submissions**: Query 5-10 agents simultaneously (standard practice)
- **Rejection patterns**: 90%+ of queries receive rejections; persistence is critical
- **Publisher market**: 5 major publishers (Big Five) + 100+ mid-tier and independent publishers
- **Auction dynamics**: Multiple publisher interest creates negotiation leverage

### GGP Etiquette
- **Claims must be defensible**: "Bestselling author" or "New York Times recommended" must be verifiable
- **Platform claims transparent**: Number of email subscribers, social followers, speaking engagements must be accurate
- **Competitive differentiation**: Describe how book differs from existing titles (not just why market exists)
- **Author credentials**: Be explicit about expertise and relevant background
- **Market data**: Cite publishing industry sources (Publishers Weekly, Amazon categories, Goodreads data)
- **No false urgency**: Don't claim scarcity or artificial deadlines; agents see through manufactured pressure

## Template

```
[QUERY LETTER — One page, personalized per agent]

Dear [Agent Name],

I am seeking representation for my book proposal, [TITLE], a [GENRE/CATEGORY] manuscript of approximately 60,000 words. I believe it's a strong fit for your list, particularly given your recent success with [Agent's recent client book in similar category].

[Hook: Why this book, why now, why you]
Organisations spend $billions on digital transformation, yet 70% of initiatives fail. The culprit isn't technology or strategy—it's the organisational design and change management discipline required to support transformation at scale. This book provides practitioners with a diagnostic framework and toolkit for transforming organisational design to support digital capability.

[Author positioning: Why you're the right person]
I've led digital transformation initiatives at [Company scale], overseeing organisational redesign for 500+ employee organisations across [industries]. My research has been published in Harvard Business Review and MIT Sloan Management Review, and I speak regularly at [conference/event names]. I have an audience of [X] email subscribers and [Y] social media followers engaged in digital leadership.

[Market opportunity: Why this book, right now]
The digital transformation market is growing 15% annually. Companies are investing record budgets in transformation. Yet failure rates persist. Books addressing this gap (like [Competitive Title 1] and [Competitive Title 2]) continue to sell well, and there's an opening for a book that focuses specifically on organisational design (vs. technology or leadership). I estimate the addressable audience at 150K+ executives responsible for transformation initiatives in US alone.

[Manuscript readiness]
I have completed a comprehensive proposal including market analysis, competitive title review, chapter outline, and two polished sample chapters. The full manuscript can be completed within 4-5 months of contract signing.

[Closing]
Thank you for considering [TITLE]. I'd welcome the opportunity to discuss this project further. The full proposal is attached.

Best regards,
[Your name]
[Contact info]

---

[PROPOSAL DOCUMENT BEGINS]

# [TITLE]: Subtitle explaining core benefit or insight

## OVERVIEW (2-3 pages)

### Concept
[1-2 paragraphs explaining book concept, central thesis, and core argument]

Example: "This book argues that digital transformation success depends on organisational design decisions that leaders often overlook. Traditional organisation structures (functional silos, hierarchical decision-making, specialist roles) inhibit the cross-functional collaboration and rapid iteration required for digital products. The book provides a diagnostic framework for assessing whether organisational design supports or blocks digital capability. It then walks through redesign patterns used by 30+ organisations that successfully transformed."

### Why This Book, Now
[2-3 paragraphs on market timing and relevance]

Example: "Digital transformation was a 'nice to have' 5 years ago. Today it's existential. Companies face 3-5 year windows to transform or become obsolete. Speed matters. Yet most transformation initiatives take 5-10 years, creating an impossible timeline. The organisations that succeed compress timelines by addressing organisational design first (before technology spend). This insight is not widely known; even transformation consultants often miss this factor. This book fills that gap."

### Target Audience
[Specific description of primary and secondary audiences]

Example: "Primary: Chief Operating Officers and VP Transformation leading digital initiatives at enterprises ($1B+ revenue). Secondary: Chief Digital Officers and Chief Technology Officers responsible for transformation execution. Tertiary: Management consultants advising on transformation strategy. All three groups are actively buying business books; Amazon category 'Digital Transformation' sells 500+ titles with strong reviews."

### What's Different
[How this book differs from existing books in the category]

Example: "Existing books address transformation from three angles: (1) technology-focused (which vendors write); (2) leadership and change management (which human resources and leadership experts write); (3) business strategy (which strategy consultants write). This book is the first to focus specifically on the organisational design dimension. While acknowledging strategy and leadership, it places organisational structure, roles, and workflows at the centre. This fills a specific gap in the market."

---

## MARKET ANALYSIS (3-4 pages)

### Market Size and Growth
[Quantified market data with sources]

Example: "Digital transformation market: $2.3 trillion annually (IDC, 2024), growing 15% CAGR through 2028. Within this, organisational change and transformation services: $80B market (Gartner, 2024), growing 22% annually. Target audience (enterprises undertaking transformation): 12,000 companies globally, 4,000 in US. Average transformation budget: $8-12M per company. Average lifespan: 3-5 years. This indicates 4,000-6,000 new transformation initiates annually in US alone."

### Audience Segments and Demand
[Breakdown of demand by segment]

Example: "Segment 1 (Enterprise CTOs, CIOs): 2,000 individuals in US companies >$1B revenue. These leaders are making $50-200K book purchases per year for executive development and team alignment. Amazon best-sellers in this category (e.g., 'The Phoenix Project,' 'Team of Teams') have sold 200K+ copies.

Segment 2 (VP Transformation / Chief Digital Officers): 4,000 individuals. Transformation-focused roles emerged in last 5-10 years; this is a fast-growing segment actively seeking expertise. LinkedIn group 'Chief Digital Officers' has 50K+ members, indicating strong community.

Segment 3 (Management Consultants): 30K+ consultants at McKinsey, Deloitte, Accenture, Bain, plus 100+ boutique transformation firms. These consultants use books as reference material and team reading lists. Consultant recommendations drive corporate bulk purchases.

Segment 4 (MBA Programs): Top 50 MBA programs incorporate digital transformation into curriculum. Instructors adopt books as required reading; each adoption = 50-100 unit sales per semester."

### Publishing Precedents
[Similar books that sold well, establishing market demand]

Example: "Comparable titles demonstrate strong market:
- 'The Phoenix Project' (2013): 500K+ copies sold; consistently top-10 on Amazon Operations/Strategy
- 'Team of Teams' (McChrystal, 2015): 300K+ copies; appealed to government, military, and enterprise
- 'Transformed' (Westerman & McAfee, 2019): 100K+ copies; MIT focused, strong in enterprise
- 'The Unicorn Project' (sequel to Phoenix Project, 2019): 200K+ copies

All comparable titles focus on transformation broadly. This book fills niche (organisational design focus) within proven market."

---

## COMPETITIVE TITLES (2-3 pages)

### Primary Competitors
[Detailed analysis of 3-5 competing titles]

Title: The Phoenix Project
Author: Gene Kim et al.
Published: 2013
Key Positioning: Transformation through DevOps and IT-Operations alignment
Strengths: Engaging narrative, practical frameworks, devoted following
Weaknesses: Focused on technology operations, not organisational design; somewhat dated (pre-cloud, pre-AI era)
How This Book Differs: While acknowledging IT as enabler, focuses on broader organisational design patterns needed to support digital product development across functions

Title: Transformed
Author: Westerman & McAfee
Published: 2019
Key Positioning: Digital transformation playbook for executives
Strengths: Academically rigorous, comprehensive, recent
Weaknesses: Broad (covers leadership, culture, technology); doesn't go deep on organisational design; targeted at large enterprises
How This Book Differs: Narrower focus (org design), deeper exploration of design patterns; includes toolkit for diagnosis and redesign

[Repeat for 3-5 most relevant competitive titles]

### Market Position
[How this book fills gaps in competitive landscape]

Example: "The competitive landscape shows three patterns: (1) Most books address transformation broadly (leadership, strategy, technology all covered shallowly). (2) Few books focus on organisational design as primary lever. (3) Existing books are either case-study narratives (engaging but not prescriptive) or frameworks-heavy (rigorous but not accessible). This book bridges: narrative-driven (stories from 30+ organisations) + prescriptive (diagnostic framework + design patterns) + accessible (written for busy executives, not academics)."

---

## CHAPTER OUTLINE (3-4 pages)

[Detailed outline showing book structure and flow]

Example Structure:

**PART I: DIAGNOSIS**

Chapter 1: Why Organisations Struggle with Digital Transformation
- Opening: Case example (real company, anonymized, facing transformation)
- Core argument: Transformation fails not on strategy but on organisational capability to execute
- Data: 60-70% failure rates; what's common among failures
- Teaser: Three organisational design patterns distinguish successes from failures

Chapter 2: The Organisational Design Dimension
- Definition: What organisational design is (not just structure, includes roles, workflows, decision rights)
- Why it matters: How organisational design enables or blocks digital product development
- Current state: How most enterprises are designed (functional silos, hierarchical)
- Gap: Why this design blocks digital capability
- Framework introduced: Three-dimensional assessment (structure, roles, workflows)

Chapter 3: Diagnosing Your Current State
- Deep dive: How to assess organisational design health against digital requirements
- Tool: Diagnostic questionnaire (40 questions across structure, roles, workflows)
- Scoring: How to interpret results
- Case example: Two companies taking assessment; markedly different results

**PART II: REDESIGN PATTERNS**

Chapter 4: Pattern 1 – Cross-Functional Product Teams
- What it is: Detailed explanation of this organisational design pattern
- Why it works: Mechanisms (colocation, shared accountability, end-to-end ownership)
- When to use: Organisational contexts where this pattern applies best
- Implementation: How to transition from functional silos to product teams
- Case examples: 3-4 companies that implemented this pattern; outcomes

Chapter 5: Pattern 2 – Decentralized Decision-Making
- What it is: Moving decision rights from headquarters to product teams / regional units
- Why it works: Reduces decision latency; enables responsive product development
- When to use: Size, market complexity, competitive dynamics that benefit from decentralization
- Implementation: How to decentralize decision-making safely (guardrails, decision frameworks)
- Case examples: 3-4 companies; implementation journeys and outcomes

Chapter 6: Pattern 3 – Role Evolution (from Specialist to Generalist-with-Depth)
- What it is: Shifting from narrow specialist roles to broader roles with digital literacy
- Why it works: Reduces handoffs; accelerates decision-making; improves cross-functional understanding
- When to use: Roles that bottleneck in functional organisations
- Implementation: How to evolve roles without losing depth; training, hiring, succession
- Case examples: 3-4 organisations; role evolution stories

**PART III: IMPLEMENTATION**

Chapter 7: Change Management and Cultural Transition
- Why org design changes fail: Common implementation mistakes
- Stakeholder management: Who resists, why, and strategies to build coalition
- Pace of change: How fast to move (research on change velocity)
- Measurement: How to track adoption and course-correct
- Case example: One company's messy transition; lessons learned

Chapter 8: Making It Stick
- Sustainability: How to prevent regression to old structures
- Reinforcement mechanisms: Processes and metrics that sustain new design
- Evolutionary next steps: How design evolves as company matures
- Common pitfalls: What can still go wrong even with good design
- Forward look: Where organisational design needs to go next (AI era, distributed work, etc.)

**APPENDICES**

Appendix A: Diagnostic Tool (full questionnaire)
Appendix B: Implementation Planning Template
Appendix C: Case Company Summaries (anonymized profiles of 30+ companies studied)

---

## SAMPLE CHAPTERS (25-50 pages)

[Two full chapters, polished and publication-ready. Should demonstrate: clarity of writing, organisation, use of examples, balance of narrative and frameworks, accessibility to target audience]

Example: Chapter 1 (Why Organisations Struggle) + Chapter 4 (Pattern 1) provide breadth (diagnostic chapter) and depth (implementation chapter), showing range.

---

## AUTHOR PLATFORM (1-2 pages)

### Credentials and Expertise
[Why you're authoritative on this topic]

Example: "I've led digital transformation initiatives at [Company] and [Company], overseeing organisational redesign for [scale] employees. Prior roles at [relevant companies]. [Degrees and credentials]. My research on organisational design and digital transformation has been published in peer-reviewed outlets and practitioner publications."

### Audience and Following
[Quantified platform]

Example:
- Email list: 12,000 subscribers (engaged audience of executives interested in digital transformation)
- LinkedIn: 15,000 followers; regular articles on transformation topics receive 500-1,000 engagements
- Twitter: 8,000 followers; active in digital transformation discussions
- Speaking: 8-10 conference talks annually (transformation-focused events); audiences of 200-500
- Prior publications: Harvard Business Review article (200K+ views); MIT SMR article; contributed chapter to 2 business books

### Media and Visibility
[Appearances and media mentions]

Example: "I've appeared in interviews with [Publication 1], [Publication 2], discussed transformation in podcast [X], [Y], [Z]. My research has been cited in [relevant media outlets]."

### Promotional Plan
[How you'll promote the book]

Example: "Upon publication, I will: (1) Leverage email list (12K subscribers) for launch promotion and speaking tour announcements; (2) Speak at 10-12 transformation-focused conferences (captive audiences of target readers); (3) Promote through LinkedIn and Twitter (combined 23K followers); (4) Seek book reviews and interviews in relevant media (HBR, McKinsey Quarterly, transformation blogs); (5) Develop supporting material (workbook, toolkit) for corporate book club adoption; (6) Partner with transformation consulting firms for bulk sales and recommended reading lists."

---

## MANUSCRIPT STATUS

[One paragraph on completion status and timeline]

Example: "The proposal, detailed chapter outline, and two polished sample chapters are complete. The full manuscript of 60,000 words can be completed within 4-5 months of contract signing, allowing publication within 12-15 months."

---

## APPENDICES

### A. Author Biography
[Detailed bio, 300-500 words, positioning for target audience]

### B. Marketing Plan
[More detailed promotion strategy, including social media, events, partnerships]

### C. Sample Graphics
[2-3 diagrams, charts, or visual elements from the book]

### D. Endorsement Letters (if available)
[Letters from respected figures in the field endorsing the concept or the author]
```

## Examples

### Good Query Letter Opening

"I am seeking representation for my book proposal, 'Remote-First Organisations: How to Build Culture When Your Teams Are Distributed,' a business/management manuscript of approximately 65,000 words. I believe it's a strong fit for your list, particularly given your recent success with [Agent's recent client], which addressed [similar theme].

The shift to remote work accelerated during the pandemic, but 60% of companies have not developed sustainable practices for distributed teams. Leaders are struggling with culture, retention, and collaboration. This book provides a research-backed framework and toolkit for building remote-first organisations that outperform co-located equivalents on engagement, retention, and innovation."

---

**Why this works:**
- Personalized reference to agent's recent success
- Problem is specific (60% struggling)
- Book fills a clear gap
- Author positioning would follow

## Tactical Guidance

**Agent Targeting Strategy:**

Research agents who represent books in your category:
- Use Publishers Marketplace (industry database; requires subscription)
- Check acknowledgments sections of competing books (authors thank agents)
- Visit agency websites; look for agents with relevant client lists
- Prioritise agents who have sold to major publishers (vs. indie presses)

**Query Persistence:**

- Query 5-10 agents simultaneously (standard practice; not considered exclusive)
- If no responses after 6 weeks, send follow-up
- Expect 90%+ rejection rates; persistence is critical
- After 10 rejections, reassess: Is your positioning wrong? Does proposal need revision?
- Don't get discouraged; published authors often experienced 15-20+ rejections before landing agent

**Sample Chapter Quality:**

Sample chapters should be:
- Polished and publication-ready (not rough draft)
- Demonstrate your voice and writing quality
- Show that you can balance narrative + framework + accessibility
- Be representative of the book's tone and style
- Include one diagnostic/framework chapter (shows thinking) and one practical chapter (shows application)

**Competitive Title Strategy:**

When analyzing competitors:
- Don't dismiss them; acknowledge their strengths
- Focus on specific positioning gaps (not "my book is better")
- Show you understand the market and where you fit
- Note publication dates; show you're current with recent titles
- Mention sales success of comparable titles (validates market demand)

## Pre-Publication Checklist

- [ ] Query letter is personalized per agent (mention their recent client book in similar category)
- [ ] Query is one page, single-spaced, includes hook and author positioning
- [ ] Proposal is 25-40 pages, professionally formatted (double-spaced, standard font, 1" margins)
- [ ] Overview section (2-3 pages) clearly states thesis and why now
- [ ] Market analysis includes data with sources (publishing industry reports, Amazon category data, target audience sizing)
- [ ] Competitive titles section (3-5 titles analysed) shows market understanding and positioning gaps
- [ ] Chapter outline is detailed (1-2 paragraph summary per chapter showing structure and flow)
- [ ] Sample chapters (25-50 pages total) are polished, publication-ready, representative of full manuscript
- [ ] Author platform section quantifies audience: email subscribers, social followers, speaking engagements, media appearances
- [ ] Promotional plan is specific and credible (not "I'll do a book tour" but "I'll speak at X conferences, promote via email list of 12K, etc.")
- [ ] All market claims include sources (avoid unsourced claims like "huge market opportunity")
- [ ] No unverifiable author positioning (bestselling author, NYT recommended, etc. must be provable)
- [ ] Author bio is written in third person, 300-500 words, establishes credibility
- [ ] GGP markers check: Platform claims are [VERIFIED] (actual subscriber counts, not estimated). Market data is [VERIFIED] with source citations. Competitive analysis is fair [ATTRIBUTION] to existing authors/publishers.
- [ ] Manuscript completion timeline is realistic and stated explicitly
- [ ] Agent list created (5-10 agents researched, personalization notes prepared)
- [ ] Proposal PDF created and tested for readability

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 7. Coding Docs
> Source: `references/channels/coding-docs.md`

# Coding Documentation

## Goal
Provide complete, tested, and verifiable code documentation (READMEs, API docs, ADRs, code comments) that developers can trust without running unverified code.

## Success Metrics (2025-2026)
- 100% of code examples compile/run successfully on first attempt (no "this should work" code)
- API documentation matches actual endpoint signatures (tested against running service)
- All dependency versions pinned to tested releases (no floating versions)
- Developer setup time reduced to <30 minutes from documentation alone
- Zero "API endpoint does not exist" issues from documentation

## Limits & Restrictions

### Technical
- Code examples must be extracted from or executed against running code, never fabricated
- All API endpoints must be verified against actual service (curl, Postman, or integration test)
- Dependency versions must include exact patch versions (e.g., `2.4.1`, not `^2.4.0`)
- URLs must be tested as valid (no placeholder domains like `api.example.com` unless explicitly marked)
- Markdown must render correctly; use code blocks with language tags

### Platform/Context
- Lives in repository root (`README.md`), `/docs/api/`, `/docs/adr/`, inline in source files
- Primary audience: developers integrating, extending, or maintaining the codebase
- Lifecycle: Updated with each release; deprecated methods marked with timeline
- Discoverability: Indexed in docs site; READMEs GitHub-searchable

### GGP Etiquette
- Never invent API endpoints, parameters, or response schemas
- Flag deprecated methods with removal date ([INFERENCE] if uncertain)
- Mark breaking changes in **bold** in examples and changelogs
- Declare assumptions about runtime environment (Node 18+, Python 3.9+, etc.)
- Code snippets must include error handling; don't omit `try/catch`, null checks

## Template

```markdown
# Project Name

## Overview
[One sentence purpose. What does this do?]

## Installation
\`\`\`bash
npm install package-name
# OR
pip install package-name
\`\`\`

### Requirements
- [Language] [exact version, e.g., Node 18.12.0+]
- [Dependency] [version]

## Quick Start
\`\`\`javascript
// Tested code sample (not pseudocode)
const lib = require('package-name');
const result = await lib.method();
console.log(result);
\`\`\`

## API Reference

### `method(options)`
- **Parameters:**
  - `options.field` (string, required): Description. Default: none.
  - `options.timeout` (number): Milliseconds. Default: 5000.
- **Returns:** Promise<Object> — See response schema below.
- **Throws:** `ErrorCode` if [specific condition].

**Example:**
\`\`\`javascript
await method({ field: 'value' });
// Returns: { success: true, data: {...} }
\`\`\`

## Architecture Decision Record (ADR-001)
**Status:** Accepted | Deprecated | Superseded by ADR-002

**Context:** Why did we face this decision?

**Decision:** What did we choose and why?

**Consequences:** What are the trade-offs?

## Contributing
[Guidelines for PRs, code style, testing requirements]

## License
[SPDX identifier, e.g., MIT, Apache-2.0]
```

## Examples

### Good Example
```markdown
# PaymentGateway SDK

## API Reference

### \`charge(options)\`
- **Parameters:**
  - \`options.amount\` (number, required): USD cents (e.g., 1050 = $10.50)
  - \`options.currency\` (string): ISO code. Default: 'USD'
- **Returns:** \`Promise<{transactionId: string, status: 'succeeded'|'declined'}>\`
- **Throws:** \`PaymentError\` if amount < 50 cents or network failure

**Example (tested against Stripe sandbox v2024-01-15):**
\`\`\`javascript
const gateway = new PaymentGateway({ apiKey: process.env.PG_KEY });
const result = await gateway.charge({ amount: 1050, currency: 'USD' });
// ✓ Sandbox test: returns { transactionId: 'txn_1A2B3C', status: 'succeeded' }
\`\`\`

**Error Handling:**
\`\`\`javascript
try {
  await gateway.charge({ amount: 10 }); // Too low
} catch (err) {
  if (err.code === 'INVALID_AMOUNT') console.log('Minimum $0.50');
}
\`\`\`
```

**Why this works:** Code is extracted from actual test suite. API response matches real service. Error codes are documented with triggers. No placeholders.

---

### Bad Example
```markdown
# PaymentGateway SDK

## How to charge
\`\`\`javascript
gateway.charge(amount);  // This should work
\`\`\`

Returns something like `{ success: true, id: 'xxx' }`.

Throws an error if payment fails (we think).
```

**Why this fails:** Fabricated example. Missing parameters. No error handling. Ambiguous return type. "I think" language indicates untested knowledge. No version pinning.

## Tactical Guidance

**Code Examples:**
- Extract from actual test files or execute in CI before publishing
- Include setup code (imports, initialization) so snippets are copy-paste-runnable
- Show both success and failure paths
- Pin exact versions: `npm install react@18.2.1`, not `react@^18`

**API Docs:**
- Generate from schema when possible (OpenAPI, GraphQL introspection, TypeScript interfaces)
- Test every endpoint/parameter combo before publishing
- Document all error codes with their HTTP status and recovery steps
- Example payloads must match actual production responses

**ADRs:**
- Store in `/docs/adr/` with sequential numbering
- Mark status at top (Accepted/Deprecated/Superseded)
- Link to related ADRs and PRs
- Update when implementation changes

**Deprecation:**
- Announce in CHANGELOG with removal date (minimum 2 major versions ahead)
- Mark in code with deprecation warning
- Document replacement method

**Review Process:**
1. [CONFIRMED] — Code runs; endpoints tested; versions pinned
2. [GAP] — Missing error handling, untested endpoints, placeholder URLs
3. [INFERENCE] — Assumed behaviour pending test run
4. [UNVERIFIED] — From legacy docs; needs verification before publishing

## Pre-Publication Checklist
- [ ] All code examples compile/run without modification
- [ ] API endpoints tested against actual service (not mocked)
- [ ] All dependency versions pinned to exact patch (18.2.1, not ^18)
- [ ] External URLs verified as accessible (no `example.com` placeholders)
- [ ] Error codes documented with example triggers
- [ ] README setup instructions followed to reproduce
- [ ] Deprecated methods marked with removal date
- [ ] Breaking changes highlighted in changelog
- [ ] GGP markers applied: [CONFIRMED] for verified, [GAP] for gaps, [INFERENCE] for assumptions
- [ ] Code comments explain *why*, not just *what*

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 8. Data Lineage
> Source: `references/channels/data-lineage.md`

# Data Lineage Documentation

## Goal
Map data flows from source to target with complete transformation transparency so stakeholders understand provenance, dependencies, and freshness of every field.

## Success Metrics (2025-2026)
- Every production data field traceable to source (0% unknown origins)
- Transformation logic documented and testable (runnable SQL, PySpark, dbt)
- Data freshness SLA compliance tracked with 98%+ uptime
- PII fields flagged in 100% of data dictionaries
- Impact analysis complete: 80% reduction in "where does this data come from?" questions

## Limits & Restrictions

### Technical
- Data lineage diagrams must show actual column-level transformations, not just table names
- Source-to-target mappings require SQL/transformation code reference (not prose descriptions)
- Schema definitions must be validated against actual database (not inferred)
- Data types, nullability, and precision must match production schema exactly
- Freshness timestamps must be testable through data quality metrics

### Platform/Context
- Lives in `/docs/data-lineage/`, data catalog tools (Collibra, Alation), wiki pages
- Primary audience: data engineers, analytics engineers, BI analysts, compliance officers
- Lifecycle: Updated when pipelines change; tested with each deployment
- Discoverability: Indexed by field name in data catalog; lineage graphs searchable

### GGP Etiquette
- Never fabricate schema definitions; always validate against `information_schema`
- Never assume transformation logic; trace actual code in dbt, SQL, or Spark jobs
- Mark all upstream dependencies as [CONFIRMED] (verified) or [GAP] (unavailable)
- Flag inferred field types as [INFERENCE] with verification status
- Data freshness must reference actual monitoring; don't guess SLA compliance

## Template

```markdown
# Data Lineage: [Schema].[Table]

## Overview
[Business context: what does this table represent?]

## Source-to-Target Mapping

| Source System | Source Field | Transformation | Target Field | Data Type | Business Rule | Freshness |
|---|---|---|---|---|---|---|
| Salesforce | `Account.Name` | UPPER() | account_name | VARCHAR(255) | Excludes test accounts | [CONFIRMED] Real-time |
| Custom App | `user_id` | SHA256 hash | user_id_encrypted | CHAR(64) | PII: encrypted at rest | [INFERENCE] Within 2h |

## Data Dictionary

| Field Name | Data Type | Nullable | Source | Owner | PII | Description | Business Rule |
|---|---|---|---|---|---|---|---|
| user_id | BIGINT | No | Salesforce | @data-eng | Yes | Unique identifier | Primary key, auto-increment |
| created_at | TIMESTAMP | No | App Log | @data-eng | No | Record creation time | UTC, no future dates |
| email | VARCHAR(255) | Yes | Salesforce CRM | @analytics | Yes | Contact email | Lowercase, normalized |

## Lineage DAG (Directed Acyclic Graph)

\`\`\`
Raw Source (Salesforce API)
  ↓
[ELT Job: salesforce_extract] — runs daily 02:00 UTC
  ↓
Staging Table: stg_salesforce_accounts
  ↓
[dbt Model: fct_accounts] — run: 03:00 UTC, test: account_name NOT NULL
  ↓
Production: analytics.accounts
\`\`\`

## Transformation Logic

### dbt Model Reference
- **File:** \`models/marts/fct_accounts.sql\`
- **Tested:** \`tests/data_quality/test_accounts.yml\`
- **Last Verified:** YYYY-MM-DD

### Sample Transformation
\`\`\`sql
SELECT
  UPPER(account_name) AS account_name,
  COALESCE(owner_id, 0) AS owner_id,
  created_at AT TIME ZONE 'UTC' AS created_at_utc
FROM stg_salesforce_accounts
WHERE deleted_flag = 0
\`\`\`

## Data Quality & Freshness

| Metric | Target | Actual (as of YYYY-MM-DD) | Status |
|---|---|---|---|
| Freshness (max age) | 2 hours | 1h 45m | [CONFIRMED] On track |
| Completeness (NOT NULL) | 99.5% | 99.8% | [CONFIRMED] Exceeds |
| Duplicates | 0 | 0 | [CONFIRMED] Clean |

## Downstream Consumers

- Dashboard: `Sales Performance Dashboard`
- ML Model: `Lead Scoring v3`
- Report: `Monthly Billing Extract`
```

## Examples

### Good Example
```markdown
# Data Lineage: analytics.fct_customer_lifetime_value

## Source-to-Target Mapping

| Source System | Source Field | Transformation | Target Field | Data Type | Business Rule | Freshness |
|---|---|---|---|---|---|---|
| PostgreSQL transactions | transactions.amount | SUM() grouped by customer_id | total_spend | NUMERIC(12,2) | Excludes refunds (status='completed') | [CONFIRMED] Verified: Real-time via CDC |
| Salesforce CRM | account.industry | CASE WHEN industry='Tech' THEN 'Technology' ELSE industry END | industry_normalized | VARCHAR(50) | Maps 23 Salesforce values to 5 standard categories | [CONFIRMED] Verified: Daily refresh 01:00 UTC |
| Redshift fact_returns | return_count | SUM() where return_date <= transaction_date + 90 | return_refund_count | INTEGER | Excludes warranty returns (claim_type='WARRANTY') | [INFERENCE]: Assumed 90-day window; verify business rule |

## Data Dictionary: Key Fields

| Field Name | Data Type | Nullable | Source | Owner | PII | Business Rule | Last Verified |
|---|---|---|---|---|---|---|---|
| customer_id | BIGINT | No | PostgreSQL | @data-eng | Yes (customer PII) | Unique identifier; matches fact_orders.customer_id | 2025-01-15 ✓ |
| total_spend | NUMERIC(12,2) | No | PostgreSQL | @analytics | No | Sum of all completed transactions; excludes refunds; USD | 2025-01-15 ✓ |
| days_since_last_purchase | INTEGER | No | Computed | @analytics | No | MAX(transaction_date) - TODAY(); NULL if no purchases | 2025-01-15 ✓ |

## Lineage DAG

\`\`\`
PostgreSQL fact_transactions (CREATED: 2020-01-01)
  └─ [Kinesis CDC stream] — captures INSERT/UPDATE in real-time
      └─ AWS Glue ETL job: pg_transactions_to_s3
          └─ S3: s3://data-lake/raw/transactions/
              └─ Redshift COPY: UNLOAD to raw_transactions
                  └─ dbt Model: stg_transactions (tests: uniqueness on id, not null on amount)
                      └─ dbt Model: fct_customer_lifetime_value (test: referential integrity to dim_customers)
                          └─ Redshift Table: analytics.fct_customer_lifetime_value
                              └─ [7 downstream dashboards, 2 ML models, 1 reporting system]
\`\`\`

## Transformation Logic (dbt)

**File:** \`models/marts/fct_customer_lifetime_value.sql\`
**Last Test Run:** 2025-01-15 09:47 UTC (PASSED)

\`\`\`sql
SELECT
  customer_id,
  SUM(amount) FILTER (WHERE status = 'completed') AS total_spend,
  COUNT(*) FILTER (WHERE status = 'completed') AS transaction_count,
  MAX(transaction_date) AS last_purchase_date,
  CURRENT_DATE - MAX(transaction_date) AS days_since_last_purchase,
  CASE
    WHEN SUM(amount) > 10000 THEN 'high'
    WHEN SUM(amount) > 1000 THEN 'medium'
    ELSE 'low'
  END AS customer_segment
FROM {{ ref('stg_transactions') }}
GROUP BY customer_id
HAVING COUNT(*) > 0
\`\`\`

## Data Quality Dashboard

| Metric | SLA | Actual (2025-01-15 09:00 UTC) | Status | Owner |
|---|---|---|---|---|
| Freshness (max data age) | ≤ 15 min | 3 min | [CONFIRMED] HEALTHY | @data-eng |
| Completeness (rows with NOT NULL values) | ≥ 99.5% | 99.87% | [CONFIRMED] HEALTHY | @data-eng |
| Accuracy (transactions match source count) | ≤ 0.1% variance | 0.02% | [CONFIRMED] HEALTHY | @analytics |
| Referential Integrity (all customer_ids exist in dim_customers) | 100% | 100% | [CONFIRMED] HEALTHY | @data-eng |
| Duplicate Check (no duplicate transactions) | 0 duplicates | 0 | [CONFIRMED] HEALTHY | @data-eng |

## PII & Compliance

- **PII Fields:** customer_id (encrypted at rest, masked in non-prod)
- **Retention:** 7 years (per GDPR Article 5)
- **Access Control:** [CONFIRMED] Verified: Only analytics team + approved stakeholders can query
- **Last Compliance Audit:** 2025-01-10

## Downstream Consumers (Verified 2025-01-15)

- Dashboard: Sales Performance Dashboard (Tableau)
- ML Pipeline: Customer Churn Prediction (Databricks)
- Report: Monthly Finance Extract (Looker)
- System: Customer 360 API (internal)
```

**Why this works:** Every field traced to source. Transformation code linked and tested. Data quality metrics actual, not estimated. PII flagged. Freshness verified through monitoring. Lineage shows full dependency chain.

---

### Bad Example
```markdown
# Customer Data

## Fields
- customer_id: probably from Salesforce
- total_spend: added up from somewhere
- days_since_purchase: calculated somehow
- status: not sure where this comes from

## How it's created
Data comes in from various sources and is joined together. Some transformations are done in SQL.

## Data quality
Generally looks good. Updates daily or so.
```

**Why this fails:** No source-to-target mapping. Transformation code missing. Freshness vague ("or so"). No verification status. No PII flagging. Downstream consumers unknown. Cannot trace dependencies.

## Tactical Guidance

**Source Mapping:**
- Extract from actual ETL logs and code, not from memory
- Test each transformation in development environment
- Pin specific dbt model versions or SQL file commits
- Document data type mismatches (e.g., Salesforce string → INT conversion)

**Data Freshness:**
- Use monitoring tools (dbt freshness tests, custom queries) to measure actual SLA compliance
- Document refresh window (e.g., "refreshes daily 02:00-03:30 UTC")
- Flag dependencies: if upstream is delayed, when does downstream fall out of SLA?

**PII Handling:**
- Scan schema for keywords: email, phone, ssn, passport, address, ip_address, dob
- Document encryption/masking/tokenization applied
- Link to data governance/compliance documentation

**Validation:**
- Run lineage against actual database schema monthly
- Update when pipelines change (dbt model additions, SQL rewrites)
- Test referential integrity: downstream IDs exist in upstream

## Pre-Publication Checklist
- [ ] Schema validated against actual database (SELECT * FROM information_schema)
- [ ] All transformation code located and linked (dbt, SQL, or job file)
- [ ] Data types match production schema exactly (not inferred)
- [ ] Freshness SLA tested through monitoring (not estimated)
- [ ] All PII fields identified and flagged
- [ ] Upstream dependencies marked [CONFIRMED] (verified) or [GAP] (unavailable)
- [ ] Lineage DAG matches actual deployment
- [ ] Transformation logic reproducible in dev environment
- [ ] Data quality thresholds tested (not guessed)
- [ ] Downstream consumers verified (systems still using this table?)
- [ ] GGP markers applied consistently throughout

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 9. Data Products
> Source: `references/channels/data-products.md`

# Data Products Documentation

## Goal
Define data product contracts (schema, SLA, quality) so consumers understand guarantees, dependencies, and service levels without discovering failures in production.

## Success Metrics (2025-2026)
- 100% of data products have published SLAs with measured compliance (>95% uptime)
- Data contracts matched to actual schema (0% contract-code mismatches)
- Data quality issues detected and alerted before consumer impact
- SLA breach resolution within documented RTO (Recovery Time Objective)
- Consumer adoption tracked: 80% of teams reference published data contracts

## Limits & Restrictions

### Technical
- Data product schema must be validated against actual tables (not inferred)
- SLAs must be measured, not estimated (freshness, completeness, accuracy metrics)
- Quality thresholds must be based on historical performance, not aspirational targets
- Data contracts must be versioned; breaking changes require consumer notification
- All quality metrics testable and continuously monitored

### Platform/Context
- Lives in data catalog (Collibra, Alation), data mesh platforms (dbt Semantic Layer, metadata stores)
- Primary audience: data consumers, analytics engineers, ML practitioners, product teams
- Lifecycle: Versioned; updated when schema or SLA changes; test coverage required
- Discoverability: Indexed by owner, domain, business context in data catalog

### GGP Etiquette
- Never fabricate SLA numbers — source from monitoring (99.9% must be measured)
- Never assume quality thresholds — base on actual historical measurements
- Mark inferred SLAs as [INFERENCE] with confidence level
- Flag dependencies: if upstream fails, when does this product fail?
- Consumer list must be verified quarterly (are they actually using this?)

## Template

```markdown
# Data Product: [Name]

## Data Product Card

**Owner:** [Team/Person]
**Domain:** [Business area]
**Status:** [Active, Sunset, Beta]
**Created:** YYYY-MM-DD
**Last Updated:** YYYY-MM-DD

## What is This?
[Business description: what question does this answer?]

## Who Uses It?
- Analytics Team: Sales Dashboard
- Product Team: Lead Scoring ML Model
- Finance: Revenue Recognition Report

## Schema

| Field Name | Data Type | Nullable | Description | Example |
|---|---|---|---|---|
| product_id | BIGINT | No | Unique product identifier | 12345 |
| revenue | NUMERIC(12,2) | No | USD revenue | 99.99 |
| created_at | TIMESTAMP | No | UTC creation time | 2025-01-15 10:30:00 |

## Data Contract

**Provider:** [Team name]
**Consumer:** [List of teams]
**Version:** 1.2.0
**Last Modified:** YYYY-MM-DD

### Freshness SLA
- Target: Data refreshed every [X hours]
- Measured: [Average delay from source → table availability]
- Alert Threshold: >2 hours delay
- Breach Notification: Slack #data-alerts

### Completeness SLA
- Target: [X%] of expected rows present
- Measured: Daily validation; expected row count = [source count]
- Alert Threshold: <98% completeness
- Acceptable Exceptions: [Any known data quality issues]

### Accuracy SLA
- Target: [X%] of values match upstream source
- Measured: Row-by-row comparison with source system
- Sampling: 100% for key columns, 10% for others
- Alert Threshold: <99.5% accuracy

### Support & Escalation
- Primary Contact: [Owner email]
- Response Time: <2 hours (during business hours)
- Escalation: If unresolved >4 hours → page on-call data engineer

## Data Quality Dimensions

| Dimension | Metric | Target | Current | Status | Last Measured |
|---|---|---|---|---|---|
| Freshness | Max data age | <4 hours | 1h 45m | [CONFIRMED] HEALTHY | 2025-01-15 09:00 |
| Completeness | NOT NULL rate | ≥99% | 99.8% | [CONFIRMED] HEALTHY | 2025-01-15 09:00 |
| Accuracy | Match source | ≥99% | 99.2% | [CONFIRMED] HEALTHY | 2025-01-14 23:00 |
| Uniqueness | Duplicate keys | 0 | 0 | [CONFIRMED] HEALTHY | 2025-01-15 09:00 |
| Validity | Format/range pass | ≥98% | 98.5% | [CONFIRMED] HEALTHY | 2025-01-15 09:00 |

## Access & Authentication
- **Data Location:** Redshift schema `analytics`, table `products`
- **Access Control:** [Role-based; query analytics_read role]
- **Authentication:** AWS IAM (assume role: data-consumer)
- **Network:** VPN required for external access

## Dependencies & Upstream Services
- Source: Salesforce Product API
- ETL: dbt model `stg_salesforce_products`
- Monitoring: [CloudWatch dashboard link]

## Changelog
- v1.2.0 (2025-01-15): Added `product_tier` field
- v1.1.0 (2024-12-01): Removed `legacy_id` (deprecated)
- v1.0.0 (2024-01-01): Initial release
```

## Examples

### Good Example
```markdown
# Data Product: fct_customer_transaction

## Data Product Card

**Owner:** Analytics Engineering Team
**Domain:** Finance & Commerce
**Status:** Active (Production)
**SLA:** 99.5% uptime, <2h freshness
**Created:** 2023-03-15
**Last Updated:** February 2026

## What is This?
Daily fact table of customer transactions (orders, returns, refunds). Used for revenue reporting, customer segmentation, and fraud detection. Grain: 1 row per transaction.

## Who Uses It?
- Finance Team: Revenue Recognition Dashboard (daily)
- Product Analytics: Cohort Analysis & LTV Models
- Risk Team: Fraud Detection Pipeline
- Marketing: Customer Segmentation (6 teams)

## Schema (v2.1.0)

| Field Name | Data Type | Nullable | Description | PII | Example |
|---|---|---|---|---|---|
| transaction_id | BIGINT | No | Unique transaction identifier | No | 987654321 |
| customer_id | BIGINT | No | Customer ID (FK to dim_customers) | Yes (encrypted) | 12345 |
| order_date | DATE | No | Date of transaction (UTC) | No | 2025-01-15 |
| total_amount | NUMERIC(10,2) | No | USD amount including tax | No | 149.99 |
| transaction_type | VARCHAR(20) | No | ENUM: PURCHASE, RETURN, REFUND | No | PURCHASE |
| is_fraud_flagged | BOOLEAN | No | Risk score >= 0.8 | No | false |

## Data Contract v2.1.0

**Provider:** Analytics Engineering Team
**Consumers:** Finance, Product Analytics, Risk, Marketing
**Effective Date:** 2025-01-15
**Breaking Changes:** None since v2.0.0

### Freshness SLA
- **Target:** Data loaded by 08:00 UTC daily (T+1 from transaction date)
- **Measured:** Query `MAX(created_at) FROM analytics.transaction_load_log` nightly
- **Actual (2025-01-15):** 07:47 UTC (13 min early) ✓
- **Alert Threshold:** >120 minutes delay from target
- **Escalation:** Slack #data-operations, then page on-call (>150 min delay)

### Completeness SLA
- **Target:** 100% of transactions in source system present (within 24 hours)
- **Measured:** Row count validation; `SELECT COUNT(*) FROM fact_table WHERE order_date = CURRENT_DATE - 1` vs. source API count
- **Actual (2025-01-14):** 99,847 rows in table vs. 99,847 expected = 100% ✓
- **Alert Threshold:** <98% (indicates dropped records)
- **Known Exception:** Refunds from Shopify process 24-48h post-return; included in next day load

### Accuracy SLA
- **Target:** 100% of key columns (amount, type, customer_id) match source
- **Measured:** Row-level comparison: transformation test `test_fact_transactions_amounts.sql` (compares against Salesforce API)
- **Sampling:** 100% for financial fields; 20% random sample for descriptive fields
- **Actual (2025-01-14):** 99.97% match (3 edge-case refunds reconciled) ✓
- **Alert Threshold:** <99% match rate
- **Known Issue:** Rounding in tax calculation (USD cents) <0.01% variance acceptable

### Support & Escalation
- **Primary Contact:** @data-analytics-team (Slack #data-team)
- **Response Time (Business Hours):** <30 min
- **Response Time (Outside Hours):** On-call data engineer (page via PagerDuty)
- **Escalation:** If unresolved >2 hours → escalate to Data Director

## Data Quality Dashboard

| Dimension | Metric | SLA | Actual (2025-01-15 09:00) | Status | Last Alert |
|---|---|---|---|---|---|
| Freshness | Load completion time | 08:00 UTC | 07:47 UTC | [CONFIRMED] On SLA | None |
| Completeness | Row count variance | ≤1% | 0.002% | [CONFIRMED] On SLA | 2024-12-20 (missed delivery) |
| Accuracy | Column match vs. source | ≥99% | 99.97% | [CONFIRMED] Exceeds | None |
| Timeliness | Data latency | <4 hours from source | 2h 15m avg | [CONFIRMED] On SLA | None |
| Validity | NOT NULL compliance | ≥99.9% | 100% | [CONFIRMED] Exceeds | None |

## Access & Authentication

**Data Location:**
- Redshift: `analytics.fct_customer_transaction`
- Snowflake (read replica): `analytics.fact_customer_transaction`

**Access Control:**
- Employees: IAM role `data-consumer` (read-only)
- External Partners: Row-level security; customer_id masked for non-approved teams
- PII Handling: Encrypted at rest; access logged

**Query Examples:**
\`\`\`sql
-- Query 1: Daily revenue (all transactions)
SELECT
  order_date,
  transaction_type,
  COUNT(*) AS transaction_count,
  SUM(total_amount) AS daily_revenue
FROM analytics.fct_customer_transaction
WHERE order_date >= CURRENT_DATE - 30
GROUP BY order_date, transaction_type;

-- Query 2: Fraud rate trend
SELECT
  order_date,
  COUNT(*) AS total_transactions,
  SUM(CASE WHEN is_fraud_flagged THEN 1 ELSE 0 END) AS fraud_count,
  ROUND(100.0 * SUM(CASE WHEN is_fraud_flagged THEN 1 ELSE 0 END) / COUNT(*), 2) AS fraud_rate_pct
FROM analytics.fct_customer_transaction
WHERE order_date >= CURRENT_DATE - 90
GROUP BY order_date;
\`\`\`

## Dependencies & Upstream Services

| Upstream | Type | Criticality | Failure Mode |
|---|---|---|---|
| Salesforce SFDC API | Source | High | [CONFIRMED] Verified: CDC stream active, backup CSV pull available |
| Redshift Cluster | Database | High | [CONFIRMED] Verified: Multi-AZ failover auto-enabled |
| dbt Cloud Job | Transformation | High | [INFERENCE] Single job; no redundancy; recovery time ~15 min |
| Fivetran | ELT | High | [CONFIRMED] Verified: Scheduled + event-triggered; backup manual load available |

## Changelog & Version History

**v2.1.0 (2025-01-15)**
- Added: `is_fraud_flagged` field (boolean, populated by risk ML model)
- Change: `transaction_type` ENUM expanded to include PARTIAL_REFUND (was only REFUND)
- Performance: Partitioned by `order_date` (query speed +40%)
- Migration: Existing code using `REFUND` still works; `PARTIAL_REFUND` captured going forward

**v2.0.0 (2024-06-01)** — BREAKING CHANGE
- Removed: `legacy_customer_key` (use `customer_id` instead)
- Changed: `amount` split into `subtotal`, `tax`, `total_amount` (granularity improvement)
- Migration Path: Join with `dim_customer_mapping` to translate old keys

**v1.0.0 (2023-03-15)** — Initial Release
- Core schema: transaction_id, customer_id, amount, date, type

## Testing & Validation

**Automated Tests:**
- Schema validation: dbt test (every deployment)
- Freshness check: CloudWatch rule (nightly 08:30 UTC)
- Accuracy check: Row comparison vs. source (nightly)
- Referential integrity: customer_id matches dim_customers (nightly)

**Manual Quarterly Validation:**
- [ ] Spot-check 100 random rows against source system
- [ ] Verify consumer teams still active users
- [ ] Review error logs and alert history
- [ ] Document any SLA misses and root causes
```

**Why this works:** Schema validated. SLAs measured, not estimated. Quality metrics actual (not targets). Consumers verified. Dependencies documented. Access control specified. Changelog tracks breaking changes.

---

### Bad Example
```markdown
# Customer Transaction Data

Data about customer transactions. Updated daily-ish.

Schema:
- transaction_id
- customer_id
- amount (probably in USD)
- date

Quality: Generally pretty good. Should be accurate most of the time.

Access: Ask someone on the data team.
```

**Why this fails:** No SLA. No quality metrics. No measured freshness. No consumer list. Schema ambiguity ("probably USD"). No access controls documented. Not a contract.

## Tactical Guidance

**Data Product Cards:**
- Include business context (why does this exist? what problem does it solve?)
- Document actual consumers (not "people who might use this")
- Link to data lineage (where does data come from?)
- Version schema; track breaking changes

**SLA Contracts:**
- Measure, don't estimate: query monitoring systems (Datadog, CloudWatch)
- Document what "freshness" means (delivery time? data availability? source lag?)
- Set thresholds based on historical performance (90th percentile, not best-case)
- Include escalation path and expected response time

**Data Quality Dimensions:**
- Freshness: Time from source event to table availability
- Completeness: % of expected rows present (validate against source count)
- Accuracy: % of values matching upstream (row-level spot check)
- Validity: % matching expected format/range
- Uniqueness: 0 duplicate keys on primary key

**Breaking Changes:**
- Announce 2+ weeks before implementation
- Document migration path (how do consumers update code?)
- Provide dual-write period (old and new columns available)
- Support queries on both old and new versions briefly

**Consumer Tracking:**
- Quarterly: verify who is querying this table (query CloudWatch logs)
- Alert if zero queries detected (is this orphaned?)
- Collect feedback: is SLA meaningful? Would different freshness help?

## Pre-Publication Checklist
- [ ] Schema validated against actual database tables
- [ ] SLAs measured (not estimated) from monitoring; historical data available
- [ ] Quality metrics based on actual historical performance (90th percentile, not best-case)
- [ ] Consumers list verified — teams actively querying this table
- [ ] Data contract versioned; breaking changes documented
- [ ] Upstream dependencies listed and failure modes understood
- [ ] Access control tested; roles and permissions verified
- [ ] All URLs, schemas, and field names match actual infrastructure
- [ ] Escalation path defined with expected response times
- [ ] Data quality tests implemented and running
- [ ] GGP markers applied: [CONFIRMED] for verified SLAs, [INFERENCE] for inferred thresholds, [GAP] for gaps
- [ ] Quarterly validation schedule documented in team calendar

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 10. Email
> Source: `references/channels/email.md`

# Email

## Goal

Clear, actionable, professionally recorded communication that creates documented accountability and reduces misunderstanding through inverted-pyramid clarity.

## Success Metrics (2025-2026)

- **Response Rate**: 85%+ within 24 hours for action emails
- **Clarity Score**: Zero "what did you mean?" follow-ups on first 50 emails per quarter
- **Documentation Value**: Email transcripts usable in retrospectives without additional explanation
- **Recipient Confidence**: Recipient acts on first read without re-reading
- **Tone Accuracy**: Tone matches context (C-Suite/Peers/Clients/Cross-cultural) per sender intent

## Limits & Restrictions

### Technical
- **Standard**: Plain text or minimal HTML formatting
- **Character limit**: None (but keep paragraphs under 100 words)
- **Attachments**: Maximum 25MB; use file links for large documents
- **Subject line**: 50 characters or fewer
- **Response time**: Flag urgent with [URGENT] prefix only when genuinely time-critical (< 4 hours)

### Platform/Algorithm
- **Inbox priority**: Short subject + first line visible in preview determines open rate
- **Reading behaviour**: Most recipients scan; don't bury critical info in paragraph 3
- **Forward risk**: High; assume email will be forwarded. Content must be defensible if read by unintended audience
- **Thread decay**: Replies after 5 exchanges become hard to follow; consider switching to meeting
- **Mobile reading**: 50%+ of reads on mobile; keep paragraphs short and scannable

### GGP Etiquette
- **Confidentiality**: Never discuss client details, pricing, or internal strategy in email (use Slack for urgent, always flag sensitivity)
- **No complaints**: Keep negative feedback constructive and private; escalate interpersonal issues offline
- **Accidental forward test**: Would you be comfortable if this was forwarded to CEO, HR, or opposing counsel?
- **Tone calibration**: Match audience (C-Suite = formal, Peers = direct, Clients = consultative, Cross-cultural = explicit)
- **Attribution**: Name your sources for claims; avoid "I've heard" or "people say"

## Template

### Action Email

```
Subject: [Tag: REQUEST/UPDATE/URGENT] Topic + deadline (50 chars max)

Hi [Name],

[ONE-SENTENCE CONTEXT]
Context: We need input on X before Y because Z.

[WHAT YOU NEED (in order of priority)]
Action Items:
1. Review draft attached (deadline: Friday EOD)
2. Confirm budget by tomorrow
3. Introduce us to contact X by next week

[WHY IT MATTERS]
Rationale: This unblocks the client review phase and keeps us on track for launch.

[NEXT STEPS]
I'll follow up Tuesday if I haven't heard back. Slack me if you have questions.

Thanks,
[Your name]
```

### Informational Email

```
Subject: FYI: Topic (update, announcement)

Hi [Name],

[HEADLINE — what changed and why it matters to them]
We've completed the Phase 1 review. Key finding: adoption is 12% higher than Q4 forecast.

[CONTEXT — what led here]
Background: We ran user interviews across 40 customers in January. [See attached summary for full data.]

[IMPLICATIONS — what this means for them]
What this means: You can expect higher demand for onboarding support in Q2. I'd recommend pre-hiring 2 contractors.

[OPTIONAL: ASK]
No action needed unless you want to discuss strategy. Happy to brief you Monday.

Best,
[Your name]
```

### Escalation Email

```
Subject: [ESCALATION] Issue + decision authority needed

Hi [Name],

[SITUATION]
Issue: The client has blocked approval of Phase 2 pending cost reduction of 25%.

[ATTEMPTS & CONSTRAINT]
What we've tried: We've cut scope twice; further cuts impact launch timeline and product quality.
Constraint: We need decision authority on budget or scope by Friday to communicate to client.

[OPTIONS & RECOMMENDATION]
1. Authorise budget increase (lowest risk to quality/timeline)
2. Negotiate extended timeline (client may object; costs more)
3. Cut feature X (meets cost target; degrades product)

I recommend Option 1 and would like your sign-off.

[REQUEST FOR DECISION]
Can we sync 30 min Thursday to lock this in?

Thanks,
[Your name]
```

## Examples

### Good Example

**Subject**: Approval needed: Q2 budget by Friday EOD

Hi Sarah,

We're finalizing Q2 spending. I need your approval on the discretionary budget allocation for the client success team (attached summary). The $85K increase reflects 3 new hires we discussed.

Approval needed by Friday EOD so finance can process.

Happy to discuss Tuesday if you have questions.

Thanks,
James

---

**Why this works:**
- Subject line is specific and deadline is clear
- First sentence tells reader what they need to do
- Paragraph is scannable (one action item stated upfront)
- Tone is respectful but direct
- No ambiguity about next steps

### Bad Example

**Subject**: Q2 stuff

Hi Sarah,

Hope you're having a good week! I wanted to touch base about something we talked about before regarding the budget for next quarter. I think we should probably increase it a bit, and I've attached a thing that explains why. Let me know what you think and if you have any questions about anything. Thanks so much!

---

**Why this fails:**
- Vague subject line; reader won't know what's inside
- No clear action (approve? discuss? read?)
- No deadline, creating ambiguity
- Overly casual tone for formal approval request
- Buries the ask in conversational language
- "Attached a thing" is imprecise

## Tactical Guidance

**Timing:**
- Send Tuesday-Thursday 9-11am for highest open rates
- Avoid Monday (inbox overwhelm) and Friday (low priority)
- Account for timezone differences; don't send 6pm local unless truly urgent

**Frequency:**
- Action emails: As needed, but batch non-urgent items (weekly digest vs. daily)
- Status updates: Weekly or per-agreement with stakeholder
- FYI emails: Max 2-3 per week to avoid noise

**Subject Line Formula:**
```
[TAG]: Specific topic + deadline/action
Examples:
- [DECISION NEEDED]: Q2 budget approval by Friday
- UPDATE: Client feedback incorporated; ready for review
- [URGENT]: Server issue requires immediate approval
```

**Tone Calibration Table**

| Audience | Formality | Greeting | Sign-off | Word Choice |
|----------|-----------|----------|----------|------------|
| C-Suite | Formal | "Hi [Title Name]" | "Best regards" | "authorise," "impact," "strategic" |
| Peers | Direct | "Hi [Name]" | "Thanks" | "we," "quick," "heads up" |
| Clients | Consultative | "Dear [Name]" | "Best" | "recommend," "partnership," "outcomes" |
| Cross-cultural | Explicit | Full name, title | Full sign-off | Avoid idiom, define jargon |

**Accidental Forward Test:**
Before hitting send, imagine this email forwarded to: (1) your CEO, (2) the client, (3) HR, (4) the person you're complaining about. If any version creates discomfort, revise.

## Pre-Publication Checklist

- [ ] Subject line is specific and under 50 characters
- [ ] First paragraph states the ask or headline upfront (inverted pyramid)
- [ ] All factual claims have a source (client data, research, internal metrics)
- [ ] No attribution needed for opinion; opinions are clearly framed as such ("In my view," "I recommend")
- [ ] Tone matches audience (C-Suite/Peer/Client/Cross-cultural — see table above)
- [ ] Email passes accidental forward test (comfortable if forwarded to CEO, HR, opposing counsel)
- [ ] Deadline is explicit (if applicable): "by Friday EOD" not "soon"
- [ ] Action items are numbered and prioritised
- [ ] No client confidential details, pricing, or strategy shared
- [ ] No complaints about people or vendors (redirect to private conversation if needed)
- [ ] GGP markers check: Does every factual claim have a verifiable source? Are estimates transparent about methodology?
- [ ] Response time is accurate (not marking as urgent unless truly < 4 hours)

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 11. HBR
> Source: `references/channels/hbr.md`

# Harvard Business Review (HBR)

## Goal

Narrative-driven business insights that combine academic rigor with accessibility for executive leaders. Establish thought leadership among C-suite audiences and drive speaking/consulting opportunities.

## Success Metrics (2025-2026)

- **Publication**: Accepted by HBR editorial team; published in print and online
- **Reach**: 5M+ impressions across HBR platforms within 30 days
- **Engagement**: 2K+ shares, 500+ comments on published article
- **Citation**: Article referenced in 10+ external articles or research within 12 months
- **Opportunity**: 3+ speaking invitations or consulting leads from article visibility
- **Downstream**: Article drives 500+ newsletter subscribers or DM inquiries from target audience

## Limits & Restrictions

### Technical
- **Word count**: 2000-4000 words (HBR average 2500)
- **Format**: Submitted as Google Doc or Word with single spacing, margins, citations
- **Structure**: Title, subtitle, deck (2-3 sentences), body, takeaways
- **Citations**: Chicago Manual of Style (footnotes) or endnotes; APA acceptable
- **Visuals**: Max 1-2 charts/graphics; all original or properly licensed
- **Images**: High-res (300dpi); rights secured for any photos or diagrams

### Platform/Algorithm
- **HBR editorial cycle**: 3-4 months from submission to publication (plan ahead)
- **Editor preference**: Editors favour contrarian but defensible perspectives
- **Searchability**: Keywords in title and deck determine visibility in HBR search
- **Curator priority**: HBR curators share pieces that challenge conventional wisdom
- **Peer review**: Light peer review by HBR editors; fact-checking is editorial responsibility
- **SEO impact**: Published articles rank in top 5 for keywords within 2 weeks

### GGP Etiquette
- **Rigorous sourcing**: HBR readers are executives; unsourced claims damage credibility
- **Counterargument acknowledgement**: Fair treatment of opposing views strengthens argument
- **No client names**: Never use real client examples; disguise or use composite examples
- **Attribution mandatory**: Name researchers, cite studies, link sources
- **Academic honesty**: Avoid plagiarism; all frameworks must be original or properly credited
- **No sponsored-content masquerade**: If sponsored by organisation, disclose clearly

## Template

```
[TITLE — Specific, provocative, searchable]
Why Traditional Performance Reviews Are Obsolete: Evidence from Fortune 500 Transformations

[SUBTITLE — Expands on title, adds context]
A study of 200+ organisations reveals that real-time feedback systems outperform annual reviews by 3x on employee retention and engagement.

[DECK — 2-3 sentences, hypothesis summary]
Annual performance reviews have survived 50 years of management science because they're familiar, not because they work. This article examines data from 200+ organisations that have replaced reviews with continuous feedback systems, identifying what works, what requires cultural change, and why the shift is happening now.

[INTRODUCTION — Hook + problem statement]
[Hook: Start with surprising stat or observation]
Example: "Seventy-eight percent of executives say their performance review system is broken. Yet 92% of organisations haven't changed their approach in five years."

[Problem: Why current state is inadequate]
Annual performance reviews served industrial-era companies well. Fixed timelines, bureaucratic rigor, paper trails. But today's businesses operate at velocity that annual cycles can't match. Employees need feedback in days, not months.

[Why now: What's changed]
Technology enables continuous feedback (software platforms, real-time data). Generational expectations have shifted (younger workers expect coaching, not judgment). Competitive pressure is intense (companies using continuous feedback report 2x faster adaptation).

[BODY SECTION 1 — Research findings / framework]
[Data section: 2-3 paragraphs with evidence]
We analysed 200+ organisations that transitioned from annual reviews to continuous feedback systems. Key findings:

1. [Finding with data]: Organisations that shifted to quarterly check-ins saw 34% increase in retention (95% CI: 28-40%). Employees reported clearer expectations (81% vs. 54% in annual-review orgs).

2. [Finding with data]: The first year of transition is hardest. Manager training requirements doubled in year 1 but stabilised by year 2. Orgs that front-loaded training had 2x better outcomes.

3. [Finding with data]: Remote organisations transitioned faster than co-located ones, likely because continuous systems don't require annual in-person review logistics.

[Framework section: 2-3 paragraphs]
What separates successful transitions from failed ones? We identified three dimensions:

Dimension 1: [Name and describe]
Organisations that succeeded framed this not as "replacing reviews" but as "adding coaching." Existing infrastructure (1:1s, skip-level meetings) became touchpoints for continuous feedback rather than isolated review events.

Dimension 2: [Name and describe]
Measurement changed. Organisations still track performance against goals, but frequency increased from annual to quarterly. This allows real-time course correction rather than post-hoc evaluation.

Dimension 3: [Name and describe]
Accountability shifted from manager to system. Organisations built feedback into workflows: project retrospectives, peer input windows, customer feedback summaries. Feedback became ambient, not ceremonial.

[BODY SECTION 2 — Complications / counterargument]
[Address the "but what about..." concern]
This isn't easy. Organisations that tried and failed typically stumbled on three points:

Complication 1: [What doesn't work]
Manager training: Frequent feedback requires different skills than annual review. Managers who are good at annual reviews (structured, formal) struggle with coaching (frequent, conversational). Organisations that just "told people to give more feedback" without training had retention increase of only 8% (vs. 34% with training).

Complication 2: [What requires change]
Technology isn't enough: Systems alone don't change behaviour. Organisations need cultural permission for managers to spend more time on coaching. Budget allocation matters: Time for coaching = time away from production. CFOs need to understand this trade-off.

Complication 3: [Valid criticism of the argument]
For roles with clear, measurable output (sales, manufacturing), annual reviews have clarity advantage: you can measure Q1-Q4 objectively. Shifting to quarterly doesn't add value if the output doesn't change quarterly. These roles may not benefit from continuous feedback (though improved communication still has value).

[Acknowledge reasonable objection]
"If continuous feedback is so good, why haven't more organisations adopted it?" Fair question. Adoption is hard; switching costs are real. Many executives who've experienced the transition say the value justifies the cost, but it's not a no-brainer.

[BODY SECTION 3 — Implications / what to do]
[For different audiences]
For CEOs considering this shift:
- Start with pilot: Choose one department (15-25 people) that's high-trust and high-engagement. Run parallel annual reviews for one year. Measure retention, engagement, time investment. If results are positive, expand.
- Budget for training: 80% of transition failure is due to inadequate manager coaching training. Invest 4-6 hours per manager in year 1.
- Communicate the why: Employees interpret feedback-frequency changes as surveillance unless leadership explains the intent. Frame as "we want to help you succeed" not "we're watching you more."

For HR leaders implementing this:
- Select technology that integrates with existing workflows (not another standalone system)
- Build feedback into existing cadences (1:1s, sprint retrospectives, project close-outs)
- Create templates for consistency; this reduces manager burden and improves quality

For managers in organisations making this shift:
- View feedback as coaching, not evaluation. Your job is to help people succeed, not judge them.
- Ask before giving: "Would feedback be helpful here?" respects autonomy.
- Balance positive and developmental feedback 3:1 (research shows this ratio is optimal for motivation)

[CONCLUSION — Reinforce thesis, look forward]
[Restate thesis in light of evidence]
Performance reviews aren't broken because companies are lazy. They're broken because business velocity has changed and annual cycles can't keep up. Evidence from 200+ organisations shows that switching to continuous feedback improves retention, engagement, and agility.

[Future angle]
The next frontier isn't frequency; it's quality. Organisations that master continuous feedback will next tackle feedback specificity and skill-building. Feedback that identifies gaps + provides learning resources = accelerated development.

[Final thought]
The companies that moved fastest on this transition are 3 years ahead in employee capability and retention. The cost of waiting is growing.

[KEY TAKEAWAYS — 3-5 bullet points, pulled out in sidebar]
• Annual performance reviews are ineffective; continuous feedback systems improve retention by 34% based on data from 200+ organisations
• Successful transitions require manager training (80% of failures are training-related), not just technology
• Continuous feedback works best when built into existing workflows (1:1s, retrospectives) rather than as a standalone system
• Organisations that transitioned report 2x faster adaptation and clearer employee expectations
• Success requires framing as coaching opportunity, not surveillance; communication of intent is critical
```

## Examples

### Good Example (Article Title/Deck)

**TITLE:**
How Data Democratization Transforms Organisational Agility

**DECK:**
Most organisations hoard data in analytics teams. A study of 150 companies reveals that organisations that distribute data access and literacy across functions adapt 3x faster, make better decisions, and retain more talent. Here's how they did it and what stumbled them.

---

**Why this works:**
- Title makes a specific claim (democratization → agility) and is searchable
- Deck includes data point (3x faster) and previews structure (what worked, what failed)
- Implies contrarian angle without being inflammatory
- Sets up for body to explain mechanism and evidence

### Bad Example (Article Title/Deck)

**TITLE:**
Everyone's Doing Analytics Wrong!!!

**DECK:**
Most companies don't understand data and they should. This article explains why analytics is important and what people should be doing instead. Lots of companies fail and some succeed.

---

**Why this fails:**
- Title is hyperbolic and not searchable ("Everyone" is too vague)
- Deck has no data, no specificity, no contrarian insight
- Doesn't preview what the article will teach
- Tone suggests clickbait rather than rigorous argument
- HBR editors would reject immediately based on deck

## Tactical Guidance

**Research Requirements:**

- Primary research ideal (survey, interviews, data analysis)
- But secondary research acceptable if novel analysis or synthesis
- Minimum: 5-10 peer-reviewed or high-credibility sources
- All claims with numbers need citations (not optional at HBR)

**Writing Style:**

```
✓ HBR voice: "Our research suggests..." [with data]
✓ HBR voice: "Three mechanisms explain..." [framework]
✓ HBR voice: "Some companies succeeded by..." [evidence]

✗ Not HBR voice: "Everyone should..." [prescriptive without nuance]
✗ Not HBR voice: "The future is..." [prediction without evidence]
✗ Not HBR voice: "It's obvious that..." [dismissive of complexity]
```

**Editorial Strategy:**

- Write for HBR editors (not for your peers). Ask: "Does an executive leading this function learn something actionable?"
- Lead with data or surprising observation, not with theory
- Address the "so what?" explicitly. Why should a busy executive care?
- Structure as: Problem (why this matters) → Evidence (here's what we found) → Implication (here's what to do)

**Submission Process:**

1. Query first: Email brief pitch (150 words) to appropriate HBR section editor
2. Wait for green light: Don't write full article without editorial encouragement
3. Draft and submit: Include title, subtitle, deck, article, key takeaways, author bio
4. Expect revision: HBR editors will request changes; be collaborative
5. Timeline: 3-4 months from submission to publication typical

## Pre-Publication Checklist

- [ ] Title is specific and searchable (includes searchable keyword like "democratization," not just "trends")
- [ ] Deck previews thesis and key finding with data point
- [ ] Article is 2000-4000 words; on target for word count
- [ ] BLUF applied: Problem and evidence come before lengthy background
- [ ] All numerical claims are cited (no unsourced data)
- [ ] Research is original or novel synthesis of existing research
- [ ] Counterargument section acknowledges valid criticisms of main argument
- [ ] Implications section is specific to audience (CEOs, HR, managers, etc.)
- [ ] Tone is authoritative but not prescriptive; acknowledges complexity
- [ ] No client names used; examples are disguised or composite
- [ ] All attribution complete: named researchers, linked studies, proper citations
- [ ] Key takeaways box extracted and summarised (3-5 points)
- [ ] Author bio is current and relevant to topic
- [ ] GGP markers check: Every factual claim is [VERIFIED] with source citation. Estimates are transparent about sample size and confidence intervals. Inferences are labelled as interpretations. No unverified claims.
- [ ] Submitted to appropriate HBR section editor with brief query first

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 12. Instagram
> Source: `references/channels/instagram.md`

# Instagram

## Goal

Visual storytelling that humanises professional brand, showcases team culture, and documents speaking engagements or thought leadership moments for a younger, more visual-oriented professional audience.

## Success Metrics (2025-2026)

- **Engagement Rate**: 2-4% on posts (likes, saves, shares) within 7 days
- **Save Rate**: 15%+ of engagements are saves (indicates value, not just vanity)
- **Story Swipe-Ups**: 5%+ swipe-through rate on links (high intent)
- **Profile Traffic**: 50+ profile visits per post within first week
- **Follower Growth**: 3-5% monthly growth from organic engagement (not purchases)
- **Caption Depth**: 30%+ of followers read full caption (tested via analytics)

## Limits & Restrictions

### Technical
- **Image resolution**: 1080x1350px for feed posts (portrait), 1080x1080px for squares
- **Carousel**: Up to 10 images per post; 1080x1350px each
- **Reels**: Up to 90 seconds; 1080x1920px ideal; 3-5 second hook required
- **Video**: Auto-plays on mute; captions required for comprehension
- **Hashtags**: 8-15 optimal (30 max, but diminishing returns); avoid more than 5 on first line
- **Alt text**: Required for accessibility; describe image content clearly
- **File size**: Under 8MB for optimal loading

### Platform/Algorithm
- **Video dominance**: Reels get 3x reach vs. static posts
- **Carousel engagement**: Swipeable posts get 2x engagement vs. single image
- **Save behaviour**: Saved posts increase creator visibility by 4x vs. likes
- **Hashtag strategy**: Smaller, niche hashtags (10K-100K posts) outperform mega hashtags (1M+)
- **Discovery**: Discoverability drops 60% after 7 days; front-load engagement
- **Story stickiness**: Stories expire in 24 hours but increase daily profile visits
- **Peak times**: Tuesday-Thursday 11am-2pm or 6-8pm (audience dependent)

### GGP Etiquette
- **Privacy first**: Never post team member photos without consent; ask permission in writing
- **No complaints**: Client frustrations, failed projects, vendor drama stay private
- **Authenticity**: Avoid over-curation; real team moments outperform staged content
- **Attribution**: Credit photographers, guest speakers, team members in captions
- **Professional tone**: Fun and human, but not frivolous; maintain expert credibility
- **No unsourced claims**: Even Instagram captions need citations if making business claims

## Template

```
[IMAGE 1 — Strong visual hook: event, product, team, or concept visualization]

[CAPTION]
[HOOK LINE — Why this moment matters]
"We just wrapped the Q1 strategy sprint with our team, and one insight keeps coming up..."

[CONTEXT — What's happening in image]
This is our war room on day 2, when we realised our original assumption was incomplete.

[INSIGHT — What you learned or observed]
Here's what we discovered: The best teams don't start with answers. They start by questioning their assumptions.

[CALL TO ACTION — Invite reflection or conversation]
What's one assumption in your work that you've been meaning to test?

[HASHTAGS — Placed at end, niche + broader mix]
#StrategyWork #TeamDevelopment #LeadershipInsights
```

### Carousel Example

```
Slide 1: Speaking at [event]
Caption: "Why we talk about organisational resilience" [hook]

Slide 2: Data slide
Caption: "78% of organisations say they're 'resilient'..." [insight]

Slide 3: Key framework
Caption: "But resilience has three dimensions..." [education]

Slide 4: Team moment
Caption: "Grateful to [team member] for pushing this research forward" [credit]

Slide 5: Audience reaction
Caption: "The conversation after? Even better than the talk." [reflection]

CTA + hashtags on final slide
```

## Examples

### Good Example

**Image**: Team members in discussion around a whiteboard, mid-brainstorm

**Caption**:

"This is what a real strategy conversation looks like: lots of disagreement, notebooks full of crossed-out ideas, and someone always asking 'but why.'

No one's in PowerPoint mode. No one's worried about sounding perfect. Everyone's digging into the actual problem.

The best ideas came from the arguments in the lower-left corner of this photo.

How does your team create space for real disagreement?"

Hashtags: #TeamCulture #StrategyWork #Leadership

---

**Why this works:**
- Image shows authentic moment (not staged)
- Caption explains what makes the moment valuable (disagreement as strength)
- Specific detail ("notebooks full of crossed-out ideas") is memorable
- Invitation for reflection, not metrics-chasing
- Credits team implicitly (collaborative moment)
- Tone is confident but humble
- Hashtags are niche and relevant

### Bad Example

**Image**: Perfect team photo, everyone smiling at camera

**Caption**:

"So blessed to work with an AMAZING team!! 🙏💪 We're literally crushing it every single day. Best company ever. Like if you have a great team too! 😍 DM me if you want to join us. We're hiring!"

Hashtags: #TeamGoals #Success #Amazing #WeAreAwesome #Blessed #TechLife #Leadership #Winning

---

**Why this fails:**
- Staged photo with forced smiles
- Empty corporate language ("crushing it," "blessed")
- Engagement bait ("like if you have a great team")
- Obvious recruiting pitch camouflaged as culture post
- Too many generic hashtags (dilutes reach)
- No actual insight about team or culture
- Tone is performative, not authentic
- Excessive emojis reduce credibility

## Tactical Guidance

**Content Calendar:**
- 3-5 posts per week (mix of static posts and reels)
- Weekly story series (same day/time) creates habit
- Batch content creation (reduce burnout, improve consistency)
- Seasonal themes (Q1 strategy, year-end reflections, team anniversaries)

**Reel Best Practices:**
- First 3 seconds must hook viewer (prevents scroll-away)
- Text overlay for captions (many watch muted)
- 15-30 second sweet spot (short enough to watch twice)
- Content ideas: quick tips, behind-the-scenes, event highlights, research findings
- Audio: Use trending audio sparingly (dates content fast); trending music OK for evergreen reels

**Carousel Strategy:**
- Start with strongest image (determines click-through)
- Each slide should work standalone and as part of narrative
- Use text overlays on complex slides (multi-column data charts, key points)
- End with CTA or reflection prompt
- Swipeable content encourages exploration and increases engagement

**Hashtag Strategy:**
```
Mix hashtag sizes:
• 3-5 niche hashtags (10K-100K posts) — your core audience
• 3-5 mid-size hashtags (100K-500K) — broader reach
• 2-5 larger hashtags (500K+) — discovery hope (low conversion)

Example mix for strategy/org design content:
#StrategyWork #OrganizationDesign #Leadership [niche]
#BusinessInsights #TeamLeadership #Transformation [mid]
#Management #Professional [broader]

Avoid hashtags with 1M+ posts (buried instantly)
```

**Caption Architecture:**
```
Hook: First 2 lines visible before "more" — must be compelling
Context: 1-2 sentences explaining the image
Insight: What you learned or want to share
CTA: Invite reflection, not vanity metrics
Hashtags: End of caption, max 15
```

**Accessibility Requirements:**
- Alt text required for all images (describe what viewers see)
- Captions required on video (platform auto-generates; review for accuracy)
- Contrast ratios matter; use high-contrast text overlays
- Audio descriptions for complex visual content

## Pre-Publication Checklist

- [ ] Image is high quality and properly cropped (1080x1350px for portrait, 1080x1080px for square)
- [ ] Alt text describes image clearly for visually impaired viewers
- [ ] Hook line (first 2 lines) is compelling and visible before "more"
- [ ] Caption tells a story, not just lists facts
- [ ] CTA invites reflection or conversation, not "like this"
- [ ] No team member photos without written consent
- [ ] No client names or confidential data
- [ ] No complaints about clients, vendors, or team members
- [ ] Hashtags are relevant, niche-targeted, max 15 (8-15 optimal)
- [ ] All business claims are sourced or attributed
- [ ] Video has captions and is comprehensible on mute
- [ ] Tone is authentic and professional (fun but not frivolous)
- [ ] GGP markers check: Are any factual claims verified? Are estimates transparent?
- [ ] Planned engagement: Ready to reply to comments within 24 hours

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 13. Internal Docs
> Source: `references/channels/internal-docs.md`

# Internal Docs (Memos, Reports, Status Updates)

## Goal

Confidential written documentation for internal audiences (team, executives, board) that creates institutional memory, supports decision-making, and provides accountability record without external exposure.

## Success Metrics (2025-2026)

- **Clarity Score**: Executive can make decision based on 5-min read (not requiring followup)
- **Retention**: Document is referenced 3+ times in subsequent decisions/planning
- **Archive Value**: 100% of key decisions traceable to documented rationale in reports
- **Stakeholder Alignment**: 90%+ of readers have consistent understanding of status/recommendation
- **Execution Fidelity**: Decisions documented match execution decisions made in field
- **Compliance**: 100% of required documentation completed on schedule

## Limits & Restrictions

### Technical
- **Format**: Google Docs, Word, or PDF; versioning tracked
- **Length**: Varies by document type (see templates below)
- **Distribution**: Digital only; avoid physical printing (retention/compliance)
- **File naming**: [Type]_[Subject]_[Date] (e.g., Report_Q1Strategy_Feb2025)
- **Version control**: Document history preserved; final approval date noted
- **Confidentiality marking**: Classified as Confidential/Internal/For Board Eyes Only

### Platform/Algorithm
- **Archive searchability**: Documents stored in central repository (Google Drive folder structure, SharePoint)
- **Access control**: Distribution limited to need-to-know audience; track access logs
- **Edit permissions**: Final documents are view-only; changes require approval and version bump
- **Notification**: Stakeholders notified of doc completion; deadlines tracked for responses
- **Decision velocity**: Reports tied to decision deadlines (e.g., "Board decision Feb 15")

### GGP Etiquette
- **Confidentiality**: Never export or share externally without explicit legal approval
- **Candor**: Internal docs allow frank assessment; balanced perspective still required
- **No public draft**: Draft documents never shared with external parties (even under NDA)
- **Objectivity in assessment**: Criticism is fair; personal attacks are not
- **Attribution**: Credit ideas to team members; distinguish individual from team views
- **Accuracy mandate**: Internal docs have highest fact-checking standard (higher than external)

## Template

### Status Update (Weekly/Monthly)

```
[HEADER]
Status Update: [Project/Initiative Name]
Period: [Week of X or Month of X]
Author: [Name]
Date: [Date]
Audience: [Team/Executive/Board]

[EXECUTIVE SUMMARY — 2-3 sentences]
What changed this period? One key metric or outcome.

Example: "Design phase completed 2 days early. Prototype feedback is positive; 3 requested changes incorporated. Engineering can begin on schedule."

[KEY METRICS — Quantified progress]
Timeline: On track (Green) | At risk (Yellow) | Behind (Red) [why?]
Budget: $X spent of $Y allocated | Burn rate: $Z/week
Quality gate: [Metric status: target X, achieved Y]

[ACCOMPLISHMENTS — What the team did this period]
• [Specific deliverable completed]
• [Milestone passed]
• [Risk mitigated]

[BLOCKERS/RISKS — What's preventing progress or could derail plan]
Blocker: [Description] | Owner: [Name] | ETA to resolution: [Date]
Risk: [Description] | Mitigation: [Plan] | Likelihood: High/Med/Low

[UPCOMING — Next period priorities]
• [Next deliverable, due date]
• [Decision needed by leader, due date]
• [External dependency, status]

[ASK — What you need from stakeholders]
Decision: [On what topic, by when]
Resources: [If needed]
Alignment: [On what point]
```

### Memo (Recommendation/Decision)

```
[HEADER]
MEMORANDUM

TO: [Recipient(s), titles]
FROM: [Your name, title]
DATE: [Date]
RE: [Subject — Specific decision or recommendation]
CLASSIFICATION: [Confidential/Internal/For Board Eyes Only]

[SITUATION — Context in 2-3 sentences]
What prompted this memo? What decision is needed? What's the timeline?

[COMPLICATION or ANALYSIS — Why it matters; what's at stake]
3-4 sentences on why this isn't obvious. What are the constraints or tensions?

[OPTIONS — 2-3 choices with pros/cons]
Option A: [Approach] | Pros: [X, Y] | Cons: [A, B] | Cost: $X | Timeline: X weeks
Option B: [Approach] | Pros: [X, Y] | Cons: [A, B] | Cost: $X | Timeline: X weeks
Option C: [Approach] | Pros: [X, Y] | Cons: [A, B] | Cost: $X | Timeline: X weeks

[RECOMMENDATION — Your advised path and why]
"I recommend Option A because [primary reason]. This [timeline + cost] balances our priorities of [X and Y]."

[DECISION REQUEST — What you need, by when]
"Please confirm approval by [date]. Once approved, we'll [next step] and communicate to [stakeholder]."

[APPENDIX — Supporting data (optional)]
• Market research summary
• Competitive comparison
• Financial model
• Risk assessment
```

### Quarterly Report

```
[HEADER]
Q[X] 2025 Status Report: [Business Unit/Function Name]
Reporting Period: Jan 1 – Mar 31, 2025
Submitted by: [Name, Title]
Date: [Date]

[EXECUTIVE SUMMARY — One paragraph, 3-4 sentences]
Key highlights: metric improvement, risk status, strategic progress.

[PERFORMANCE vs. TARGETS]
| Metric | Target | Achieved | Status | Notes |
|--------|--------|----------|--------|-------|
| Revenue | $X | $Y | Green/Yellow/Red | [Variance explanation] |
| Retention | X% | Y% | [Status] | [Variance explanation] |
| NPS | X | Y | [Status] | [Variance explanation] |

[STRATEGIC INITIATIVES — Progress on planned projects]
Initiative 1: [Name] | Status: 60% complete | On timeline | Key milestone [achieved/pending]
Initiative 2: [Name] | Status: 40% complete | 2 weeks behind | Risk: [mitigation plan]

[HEADCOUNT & TEAM HEALTH]
Headcount: X (plan was Y) | Turnover: X% (target: Y%) | Open roles: [count]
Engagement: [Pulse survey data or anecdotal assessment]

[FINANCIAL SUMMARY]
YTD spend: $X of $Y budgeted
Variance: [Over/under by X%; explanation]
Forecast for full year: $Z

[KEY RISKS & MITIGATION]
Risk 1: [Description] | Likelihood: High | Impact: High | Mitigation: [Plan]
Risk 2: [Description] | Likelihood: Medium | Impact: Medium | Mitigation: [Plan]

[OPPORTUNITIES]
Market shift: [Opportunity description] | Recommended action: [Path forward]
Partnership: [Opportunity] | Owner: [Name] | Next steps: [Decision/approval needed]

[Q2 PRIORITIES]
1. [Initiative and success metric]
2. [Initiative and success metric]
3. [Initiative and success metric]

[APPENDIX]
• Detailed financial statements
• Market share data
• Customer feedback summary
```

## Examples

### Good Status Update Example

**TO:** Management Team
**FROM:** Sarah, Product Lead
**DATE:** Feb 6, 2025
**RE:** Product Development — Week of Feb 3-7

**EXECUTIVE SUMMARY:**
Design review completed 2 days early. Prototype feedback is positive; 3 feature requests incorporated. Engineering begins Sprint 4 on schedule.

**KEY METRICS:**
Timeline: Green (on track)
Budget: $45K spent of $200K allocated (22.5%) | Burn rate: $5.2K/week
Quality gates: Design specs signed off by stakeholders (100%)

**ACCOMPLISHMENTS:**
• Completed usability testing with 12 enterprise customers
• Finalized UX specification document with engineering buy-in
• Resolved API integration architecture (deferred blocker from week 1)
• Hired 2 junior engineers (backfill for team growth)

**BLOCKERS/RISKS:**
Blocker: Legal review of ToS changes pending | Owner: Marcus (Legal) | ETA: Feb 13
Risk: Vendor dependency on [Service] — if service degrades, affects launch timeline | Mitigation: Building fallback integration; starting validation this week | Likelihood: Low

**UPCOMING:**
• Engineering begins Sprint 4 (Feb 10)
• Customer advisory board meeting (Feb 14) — will demo prototype
• Internal security review (Feb 17)

**ASK:**
Decision: Customer onboarding flow — do we support 2-step or 3-step? See attached comparison. Needed by Feb 10.
Approval: Marketing wants to announce beta access at conference (Feb 20). Need your sign-off on positioning.

---

**Why this works:**
- Executive summary is actionable (reader knows status in one sentence)
- Metrics are quantified (not "going well" but "22.5% burn, on track")
- Blockers are owned and dated (not ambiguous)
- Asks are specific with deadlines (decision by Feb 10, approval needed for announcement)
- Tone is factual, not emotional
- Balance of good news and risk transparency

### Good Recommendation Memo Example

**TO:** CEO
**FROM:** Chief Strategy Officer
**DATE:** Feb 6, 2025
**RE:** Recommendation: Strategic Partnership with [Company Name]

**SITUATION:**
We've been in discussions with [Company] about a potential partnership to co-develop integrated solutions. Their product complements ours; combined offering would reach 40% more customers. Decision needed by Feb 28 (their board approves partnerships quarterly).

**COMPLICATION:**
Upside is clear (market reach, revenue growth). But partnership requires technology integration (3-4 months), shared roadmap alignment (complex), and co-marketing commitments. Their culture is different from ours; integration risk is real.

**OPTIONS:**

Option A: Full partnership (technology integration, combined sales team, shared roadmap)
Pros: Fastest market entry, biggest upside (projected $X new ARR)
Cons: Resource-intensive integration, cultural alignment challenges, dependency on partner execution
Cost: $X investment | Timeline: 6-9 months to full integration | Risk: High

Option B: Lightweight partnership (integration only, separate sales, loose roadmap alignment)
Pros: Lower risk, maintains autonomy, can test partnership dynamics
Cons: Limited upside, slower market entry, doesn't maximise partnership value
Cost: $X investment | Timeline: 4-6 months to integration | Risk: Medium

Option C: Formal evaluation period (6-month pilot with limited integration)
Pros: Validates partnership fit before major commitment, reduces upside risk
Cons: Delays market entry, may signal hesitation to partner, partner may lose interest
Cost: $X investment | Timeline: Pilot 6 months, then decision | Risk: Medium-High

**RECOMMENDATION:**
Option B. This balances our growth objective (new market access) with organisational stability. We can move quickly (4 months to market), maintain decision-making autonomy, and evaluate the partnership before deeper commitment. If results are strong, we can escalate to Option A in 12 months.

**DECISION REQUEST:**
Please confirm approval by Feb 13 so we can communicate decision to [Company] and begin integration planning. If approved, I'll drive the partnership agreement with Legal and create 90-day integration roadmap with Product.

---

**Why this works:**
- Situation is clear (decision timeline, why it matters)
- Complication frames why this isn't obvious
- Options are parallel, with comparable metrics (cost, timeline, risk)
- Recommendation is clear and explains trade-offs
- Decision request is specific (date, next steps)
- Tone is collaborative, not pushing agenda

## Tactical Guidance

**Document Types & Cadence:**

| Type | Frequency | Owner | Audience | Approver |
|------|-----------|-------|----------|----------|
| Status Update | Weekly/Monthly | Team lead | Immediate manager | Manager |
| Recommendation Memo | As needed | Function lead | Executive + affected stakeholders | Decision-maker |
| Quarterly Report | Quarterly | Department head | CEO + Board | Board |
| Project Post-Mortem | Project end | Project lead | Team + executives | CEO |

**Classification Guide:**

```
[CONFIDENTIAL] — Standard internal document. Shared only with need-to-know team.

[FOR INTERNAL USE] — Can be shared broadly internally (all-hands, partner teams).
  OK to reference externally (in general terms) but not share the doc itself.

[FOR BOARD EYES ONLY] — Sensitive strategy/financial. Board and select C-suite only.
  Never referenced externally; not searchable in general drive; restricted access.
```

**BLUF Discipline (Bottom Line Up Front):**

```
✗ Bad: "We've been working on this for weeks, and after looking at the data and talking to customers and reviewing our options, I think we should consider..."

✓ Good: "I recommend we extend the timeline by 2 weeks to incorporate customer feedback. This improves product fit without materially impacting Q2 launch."

Structure:
1. Recommendation/decision [one sentence]
2. Why it matters [one sentence]
3. Supporting detail [paragraphs]
```

**Confidentiality Handling:**

- Store docs in classified folder (access restricted)
- Don't email to external parties (use secure share link with expiration)
- No printing (unless board doc for in-person meeting)
- Mention externally only in vague terms: "internal assessment" not "internal sales declined 15%"

## Pre-Publication Checklist

- [ ] Audience is appropriate for this information (confidentiality matched)
- [ ] All factual claims verified against source data (internal systems, third-party sources)
- [ ] Financial data is current and accurate (reconciled with accounting if material)
- [ ] Decision timeline is explicit (decision needed by [date])
- [ ] Recommendations are numbered and include pros/cons
- [ ] Metrics are quantified (not "going well" but "15% above target")
- [ ] Owners assigned to blockers/risks with resolution dates
- [ ] BLUF applied: recommendation or status summary is first, not last
- [ ] Tone is balanced: acknowledges risks alongside opportunities
- [ ] No unverified claims or unsourced data
- [ ] No client complaints or personal criticism (professional tone)
- [ ] No confidential client data, pricing, or strategy (even internally, classify appropriately)
- [ ] Version control: document history preserved, approval date noted
- [ ] Access restrictions applied: only shared with stakeholders with need-to-know
- [ ] GGP markers check: All factual claims [VERIFIED] against source. Estimates transparent about methodology. Inferences labelled as interpretations, not facts.
- [ ] Distribution complete: stakeholders notified, deadlines tracked for required input

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 14. Legal Disclaimers
> Source: `references/channels/legal-disclaimers.md`

# Legal Disclaimers & IP Protection

## Goal
Protect organisational and user interests through clear, enforceable legal notices (authorship, usage rights, liability limits) while avoiding misrepresentation and ensuring compliance with applicable jurisdictions so disputes are prevented and liabilities are managed.

## Success Metrics (2025-2026)
- 100% of published work includes attribution and license statement
- All legal disclaimers reviewed by qualified legal counsel before publication
- Zero "surprise" IP disputes (terms clear upfront)
- Data protection notices accurate and aligned with actual practices
- Liability language enforceable in target jurisdictions (tested, not theoretical)

## Limits & Restrictions

### Technical
- All disclaimers must be approved by qualified legal counsel (not generated)
- Data practices described must match actual implementation (audit before publishing)
- Jurisdiction specified explicitly (disclaimers may not be enforceable globally)
- Retention periods stated must be achievable (not theoretical ideals)
- All regulatory references must be current (laws change; review annually)

### Platform/Context
- Lives in terms.html, privacy-policy.md, LICENSE files, footer notices, API docs, GitHub repos
- Primary audience: users, legal teams, regulators, IP holders
- Lifecycle: Reviewed quarterly; updated when practices change or laws evolve
- Discoverability: Prominently linked on every page; displayed at signup/first use

### GGP Etiquette
- **[INFERENCE] Mark all unreviewed disclaimers** — Never present legal language as fact if not reviewed by counsel
- Flag jurisdiction-specific claims (may not apply globally)
- Mark all regulatory references with review date (laws change)
- Never claim "legal advice" — disclaim that role clearly
- Document actual data practices, not aspirational privacy

## Template

```markdown
# [Disclaimer Type]

## Disclaimer Notice

**[INFERENCE]:** This document has not been reviewed by qualified legal counsel. For legal questions, consult a licensed attorney. This is not legal advice.

## Scope
[What does this disclaimer cover? What is excluded?]

## Authorship & Ownership
[Who created this work? Who owns it? Any third-party materials?]

## Permitted Uses
[What can users do with this work?]

### Restrictions
[What is explicitly prohibited?]

## Limitation of Liability
[What is your liability exposure? What is excluded?]

## Attribution
[If required, how must you be attributed?]

## Governing Law & Jurisdiction
[Which laws apply? Where can disputes be litigated?]

## Changes & Updates
[How often is this reviewed? When were last changes made?]

## Contact & Enforcement
[Who enforces these terms? How to report violations?]

## Plain Language Summary
[In simple terms, what are the key points?]
```

## Examples

### Good Example: Comprehensive Disclaimer

```markdown
# Standard Disclaimer & IP Protection Notice

**[INFERENCE]:** This document has NOT been reviewed by legal counsel. This is not legal advice. For legal questions specific to your situation, consult a licensed attorney in your jurisdiction.

---

## Scope

This notice applies to:
- All code in this repository
- Documentation and examples
- Issue discussions and pull requests
- User-generated contributions

Excluded: External libraries (see LICENSE files in each dependency)

---

## Authorship & Ownership

**[CONFIRMED] Copyright & Ownership:** *(Verified against corporate registration records.)*
- Copyright © [Current Year] [Organisation Name]. All rights reserved. *(Update year annually or use dynamic generation.)*
- Owner: [Legal Entity Name], [Address]
- Contact: [legal@organisation.com](mailto:legal@organisation.com)

**Third-Party Materials:**
- Uses Bootstrap 5.3.0 (MIT License)
- Uses Font Awesome 6.4.0 (Creative Commons License)
- See LICENSES/ directory for complete attributions

**User Contributions:**
- By contributing code/issues, you grant [Organisation] a perpetual, worldwide, non-exclusive license to use your contributions
- You retain copyright to your original work
- Contributors: [Link to CONTRIBUTORS.md]

---

## Permitted Uses

### For Developers
You may:
- ✓ Download, install, and use in commercial and non-commercial projects
- ✓ Fork, modify, and distribute under the same license (Apache 2.0)
- ✓ Reference in documentation with attribution
- ✓ Create derivative works (mention original source)

### For Users
You may:
- ✓ View and read all documentation
- ✓ Report bugs and request features
- ✓ Use public APIs for integration

### Restrictions
You may NOT:
- ✗ Claim authorship of work you didn't create
- ✗ Remove copyright or license notices
- ✗ Use our trademarks (logo, name) without permission
- ✗ Misrepresent the software's origin or capabilities
- ✗ Use for illegal purposes or violation of terms

---

## Limitation of Liability

**NO WARRANTY PROVIDED.**

This software is provided "AS IS" without warranty of any kind, express or implied, including but not limited to:
- Merchantability (fitness for a particular purpose)
- Non-infringement
- Accuracy, reliability, completeness

**LIABILITY CAP:**

In no event shall [Organisation] be liable for:
- Direct damages (loss of data, cost of replacement)
- Indirect damages (lost profits, business interruption, reputation harm)
- Consequential or punitive damages
- Any claim arising from use of this software

**Exception:** If any jurisdiction limits liability waivers, liability shall be limited to $100 USD or the amount paid for this software (whichever is greater).

**User Responsibility:** You assume all risk from your use of this software. Perform your own testing and due diligence.

---

## Data Protection & Privacy

**[INFERENCE]:** This describes our actual practices (as of 2025-01-15). These practices may change. Review this document quarterly. This is NOT a GDPR-compliant privacy policy; for GDPR compliance, see [Privacy Policy](privacy-policy.md).

### Data Collected
- **What:** User account data (email, name, optional profile info)
- **How:** Voluntary submission via web form
- **Purpose:** User identification, notification delivery, service improvement
- **Retention:** 30 days after account deletion (for compliance audits); then deleted

### Your Rights
- **Access:** Request your data via [privacy@organisation.com](mailto:privacy@organisation.com)
- **Deletion:** Request deletion; we will comply within 30 days
- **Opt-out:** Unsubscribe from communications via account settings
- **GDPR:** If you are in EU, see [GDPR Data Protection Notice](gdpr-notice.md)

### Data Sharing
- **NOT shared** with third parties without consent (except as required by law)
- **Shared with:** Cloud hosting provider (AWS, encrypted) and analytics platform (Mixpanel, anonymised)
- **Law enforcement:** Disclosure only if legally required (warrant, subpoena)

---

## Attribution Requirements

If you use this software, you MUST:
1. Include this license text with your distribution
2. State changes made to the original code
3. Display attribution: "Based on [Project Name] © [Current Year] [Organisation]"

### Example Attribution
\`\`\`
This project uses code from ExampleLib (https://github.com/org/examplelib)
Licensed under Apache 2.0 (https://opensource.org/licenses/Apache-2.0)
Original copyright © [Current Year] Example Organisation. Used under license.
\`\`\`

---

## Governing Law & Jurisdiction

This notice is governed by the laws of [Country/State], without regard to conflicts of law.

**Jurisdiction:** Any dispute arising from this software shall be litigated exclusively in [Specific Court], unless prohibited by applicable law.

**[INFERENCE]:** If you are in EU or another jurisdiction with mandatory governing law, local laws may apply regardless of this clause.

---

## Changes & Updates

**Last Updated:** February 2026
**Next Review:** 2025-04-15 (quarterly review scheduled)
**Changelog:** See [LEGAL-CHANGELOG.md](LEGAL-CHANGELOG.md)

We may update this notice periodically. Material changes will be communicated via [changelog](CHANGELOG.md) and, where required by applicable law (e.g., GDPR), through direct notification. Continued use after notification constitutes acceptance.

---

## Security & Compliance

**Security Issues:** Report via [security@organisation.com](mailto:security@organisation.com) (not public GitHub issues)

**Compliance:**
- GDPR (EU): See [GDPR Notice](gdpr-notice.md)
- CCPA (California): See [CCPA Notice](ccpa-notice.md)
- SOC 2 Type II: [Certification link] (expires [YYYY-MM-DD]) **[GAP]** *(Verify current certification expiry date before publishing. Replace placeholder with actual date.)*
- HIPAA: NOT compliant; do not use for PHI/healthcare data

---

## Plain Language Summary

**In simple terms:**

1. **You own your contributions.** If you submit code/feedback, we can use it, but you keep your copyright.

2. **We own our code.** Our copyright and Apache 2.0 license apply. You can use it free of charge, including commercially, if you:
   - Keep our copyright notice visible
   - List any changes you made
   - Use the same license for your derivative works

3. **No promises, but we care.** We provide the software as-is (no guarantees). If it breaks, we're not liable for damages. But we try hard to keep it reliable.

4. **Your data is safe.** We collect minimal data, keep it encrypted, and don't sell it. You can request, edit, or delete your data anytime.

5. **Ask a lawyer for legal advice.** This notice is not a substitute for legal counsel specific to your situation.

---

## Contact

**Legal Questions:** [legal@organisation.com](mailto:legal@organisation.com)
**Security Issues:** [security@organisation.com](mailto:security@organisation.com)
**Privacy Questions:** [privacy@organisation.com](mailto:privacy@organisation.com)
**General Support:** [support@organisation.com](mailto:support@organisation.com)

**[INFERENCE]:** This notice represents our best understanding of legal requirements and practices as of 2025-01-15. Laws and regulations change. This notice is updated quarterly. For questions specific to your jurisdiction, consult a qualified attorney.
```

**Why this works:** Marked as [INFERENCE] (not legal advice). Jurisdiction specified. Actual data practices described. Liability cap explicit. Attribution requirements clear. Organized by topic.

---

### Mediocre Example: Partial Compliance
```markdown
# Terms of Use

© 2025 Acme Corp. All rights reserved.

This software is provided as-is. We make no warranties. You may use it under the MIT License.

We collect some user data for analytics. See our privacy page for details.

For legal questions, contact legal@acme.com.
```

**Why this is mediocre:** Has a copyright notice and license reference (good), but: no `[INFERENCE]` marker (unreviewed by counsel), no jurisdiction specified, vague data practices ("some user data"), no liability cap, no attribution requirements, no plain language summary, no review schedule. Would score ~4/16 on the Pre-Publication Checklist.

---

### Bad Example
```markdown
# Legal Notice

This software is protected. You probably can't use it for much. We're not responsible if something goes wrong.

By using this, you agree to everything. Check the law in your country maybe.

© Us. All rights reserved or something.

For questions: just ask.
```

**Why this fails:** No clear permission/restrictions. Liability language vague and possibly unenforceable. No jurisdiction stated. Doesn't disclose data practices. Not professional or enforceable. Would score 0/16 on the Pre-Publication Checklist.

## Tactical Guidance

**Disclaimers vs. Policies:**
- **Disclaimer:** "What we're NOT responsible for" (liability cap, no warranty)
- **Policy:** "What we DO" (data practices, how we use your info)
- **License:** "What you can DO" (permitted uses, requirements)
- Use all three; they serve different purposes

**Legal Review:**
- Have qualified attorney review before publication
- Mark unreviewed language as [INFERENCE]
- Review annually (laws change, practices evolve)
- Document review date in notice

**Jurisdiction Matters:**
- Liability waivers enforceable in some jurisdictions, not others
- GDPR overrides disclaimers (EU residents have rights regardless)
- Specify governing law explicitly (may not be enforceable globally)
- Consider your target audience's jurisdiction

**Data Practices Accuracy:**
- Audit actual practices before claiming privacy
- Document retention periods (must be achievable)
- Disclose all third parties who access data
- Align privacy notice with GDPR/CCPA if applicable

**Attribution Requirements:**
- Specify what must be included (copyright, license, link)
- Provide template or example
- Make it easy to comply (copy-paste ready)
- Enforce consistently (pursue violations)

**Multi-Language Considerations:**
- For multi-jurisdiction audiences (EU, Latam), provide translations in relevant languages
- State which language version prevails in case of conflict (e.g., "The English version of this notice governs in case of discrepancy")
- Ensure translated versions are reviewed by legal counsel fluent in both languages
- GDPR may require notices in the user's local language for consumer-facing products
- Consider maintaining a translation matrix: language, last reviewed date, reviewer

**Accessibility (WCAG):**
- Legal notices must be accessible to screen readers (semantic HTML, proper heading hierarchy)
- Ensure sufficient colour contrast (WCAG 2.1 AA minimum: 4.5:1 for body text)
- Avoid using images of text for legal content (use actual text)
- Provide a plain language summary alongside formal legal language
- Test readability score (aim for grade 8-10 reading level for plain language sections)
- Ensure all links are descriptive (not "click here")

**Common Mistakes:**
- Making promises you can't keep (warranties, liability caps that don't hold up in court)
- Claiming GDPR compliance without actual compliance
- Hiding terms in fine print (use clear, prominent placement)
- Not updating when practices change
- Claiming legal advice (it's not; disclaim that role)

## Pre-Publication Checklist
- [ ] **[INFERENCE] Marked:** All disclaimers marked as unreviewed if no legal counsel approval
- [ ] **Legal Review:** Reviewed by qualified attorney in target jurisdiction
- [ ] **Jurisdiction Specified:** Governing law and jurisdiction explicitly stated
- [ ] **Enforceable Language:** Liability caps and warranties appropriate to jurisdiction
- [ ] **Data Practices Accurate:** Audited against actual implementation (not aspirational)
- [ ] **Retention Periods Achievable:** Data deletion procedures documented and executable
- [ ] **PII Handling Clear:** Who accesses data, how it's encrypted, third-party sharing disclosed
- [ ] **Attribution Requirements:** Template provided; easy to comply
- [ ] **Compliance Flags:** GDPR, CCPA, HIPAA status clearly marked
- [ ] **Contact Points:** Legal, security, privacy contact emails available
- [ ] **Plain Language Summary:** Non-lawyers can understand key points
- [ ] **Review Schedule:** Documented; next review date set (quarterly minimum)
- [ ] **Trademarks Protected:** Logo, name usage restrictions stated
- [ ] **Third-Party Licenses:** All dependencies attributed with license terms
- [ ] **Changelog Maintained:** Legal-changelog.md updated with each revision
- [ ] **GGP Markers Applied:** [INFERENCE] for unreviewed content, [GAP] for gaps, [CONFIRMED] for legally verified

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 15. LinkedIn
> Source: `references/channels/linkedin.md`

# LinkedIn

## Goal

Establish thought leadership within professional networks through evidence-based insights that drive conversation, build credibility, and create opportunities for collaboration or speaking.

## Success Metrics (2025-2026)

- **Engagement Rate**: 3-5% on posts (likes, comments, shares combined) after 7 days
- **Profile Visits**: 50+ profile visits per post within first week
- **Conversation Quality**: Comments from tier-1 network (recognised experts, decision-makers)
- **DM Inbound**: 2-3 meaningful collaboration inquiries per month from posts
- **Share Velocity**: 5+ reposts/shares per post within 48 hours

## Limits & Restrictions

### Technical
- **Character limit**: 3,000 characters (but keep hook under 150)
- **First line visibility**: Only first 60 characters visible in feed preview
- **Formatting**: Line breaks, emojis, bullet points increase scannability
- **Images**: 1200x628px optimal; carousel up to 10 images; video up to 10 minutes
- **Links**: Platform de-prioritises external links; use sparingly, always include context
- **Hashtags**: Max 5 per post, #PascalCase (algorithm tracks better than lowercase)

### Platform/Algorithm
- **Dwell time**: LinkedIn rewards comments and shares; likes are weighted less
- **Comment signals**: First 3 comments in first hour shape visibility; reply to all comments
- **Thread engagement**: Multi-part threads (carousels) outperform single posts by 2-3x
- **Posting time**: Tuesday-Thursday 8am-10am performs best (peak activity)
- **Document posts**: Native docs (white papers, research) get 2x engagement vs. links
- **Video**: Native video gets 5x reach vs. links to YouTube
- **DM shares**: If someone DMs your post, algorithm boosts to their network

### GGP Etiquette
- **No engagement bait**: Don't ask "like if you agree" or "comment your take"
- **Confidentiality**: Never name-drop clients; use "a Fortune 500 tech company" if needed
- **No industry complaints**: Avoid "why nobody understands X" posts; frame as opportunity
- **Attribution mandatory**: Name researchers, cite studies, link sources for claims
- **Respectful disagreement**: Acknowledge valid counterarguments; don't dunk on other viewpoints
- **No paid promotion masquerade**: If sponsored, clearly mark [SPONSORED] or [AD]

## Template

```
[HOOK — First line visible in preview, sets up curiosity]
One dataset we reviewed this week surprised me: 76% of organisations say they have a digital strategy, but only 12% can actually articulate what "digital" means to them.

[INSIGHT — What you learned or observed]
The gap isn't strategy. It's clarity. Teams conflate "we bought SaaS tools" with "we have a digital strategy."

[CONTEXT — Why this matters, who should care]
If you're leading transformation in a non-tech company, you know this problem: executives align on vision, but execution teams interpret it differently. Result: fractured roadmaps, wasted budget, friction.

[FRAMEWORK or EVIDENCE — How to think about it, what data shows]
Here's what the best performers do differently:
• Define specific metrics for "digital" (not just adoption %)
• Audit existing tools and data flows (most are siloed)
• Pilot in one function; prove ROI before scaling
• Measure culture shift, not just tech adoption

[CALL TO ACTION — Invite conversation, not "like this"]
What's been your biggest barrier to clarity in digital transformation? Curious what you've seen in your industry.

#DigitalTransformation #OrganizationDesign #Leadership
```

## Examples

### Good Example

**Post:**

"The best strategy meetings I've attended this year didn't have PowerPoint.

They had a shared Google Doc. Here's why it works:
- Everyone drafts simultaneously (removes status anxiety)
- Comments create a thinking trail (easier to backtrack later)
- No presenter fatigue (nobody has to 'perform')
- Recording is automatic (future reference, clarity)

If your next strategy meeting has 20 slides, you've lost someone by slide 5.

Have you run strategy on a doc instead? What changed?"

---

**Why this works:**
- Hook is specific and memorable ("didn't have PowerPoint")
- Structure is scannable (bullet points, short lines)
- Claims are based on observable evidence ("meetings I've attended")
- Insight is actionable (readers can do this today)
- CTA invites experience-sharing, not vanity metrics
- Tone is exploratory, not prescriptive

### Bad Example

**Post:**

"Everyone's doing digital transformation wrong! 😂 We figured it out and it's so simple. You won't believe what we found. Our CEO says we're the best in the industry. Like if you agree that most companies don't have a clue. Comment below and I'll send you a free guide that costs $500 elsewhere. #strategy #leadership #success"

---

**Why this fails:**
- Clickbait hook ("You won't believe") — no substance
- Self-promotional without credibility grounding ("we figured it out")
- Engagement bait ("like if you agree")
- No actual insight or evidence
- Desperate tone (giving away $500 guides)
- Too many generic hashtags
- Comes across as salesy, not thoughtful

## Tactical Guidance

**Posting Cadence:**
- 1-2 posts per week maximum (quality over frequency)
- Batch posts Tuesday-Thursday 8-10am for best reach
- Comment on posts in your feed first (builds algorithm favour before posting)

**Hook Formula:**
```
Start with ONE sentence that's:
✓ Specific (not "everyone should know X")
✓ Surprising (contradicts assumption or reveals gap)
✓ Relevant to audience (speaks to their role/challenge)

Examples:
- "76% of organisations say X, but only 12% can do Y" [data]
- "I spent 20 years thinking Z was required for success; it's not" [revelation]
- "The best teams I've worked with all do one thing differently" [intrigue]
```

**Mobile-First Formatting:**
- Use line breaks frequently (3-4 lines max before break)
- Keep sentences under 15 words when possible
- Emoji sparingly (one or two for visual interest, not decoration)
- Use numbered lists or bullets for clarity

**Hashtag Strategy:**
- 3-5 hashtags maximum, placed at end of post (not within text)
- Use #PascalCase: #DigitalTransformation not #digitaltransformation
- Mix industry-specific (#DigitalTransform) with broader (#Leadership)
- Research hashtag before using; ensure it's active (50K+ posts in last month)

**Engagement Protocol:**
- Reply to ALL comments within first 2 hours
- Keep replies brief (1-2 sentences); invite DM for detailed discussion
- Share other experts' relevant posts weekly (builds network goodwill)
- Don't argue; redirect disagreement to DMs ("Worth a longer chat — sending DM")

## Pre-Publication Checklist

- [ ] Hook (first line) is specific and under 60 characters; creates curiosity
- [ ] Post passes mobile readability test (short lines, clear breaks)
- [ ] All numerical claims have a source noted: "[VERIFIED]" or link to study
- [ ] No engagement bait ("like if you agree," "comment your take")
- [ ] No client names or confidential data; generic references only
- [ ] Tone is consultative, not prescriptive; acknowledges complexity
- [ ] Hashtags are in #PascalCase, max 5, placed at end
- [ ] CTA invites experience-sharing or conversation, not vanity metrics
- [ ] Claims are attributed or sourced (if citing someone, tag them or link source)
- [ ] No industry complaints or negativity; framed as opportunity if discussing problems
- [ ] GGP markers check: Are claims verifiable? Are estimates transparent? Is inference labelled as such?
- [ ] Ready to engage: Planned to reply to comments within 2 hours for first 24 hours

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 16. Messaging
> Source: `references/channels/messaging.md`

# Messaging (WhatsApp, Teams, Slack)

## Goal

Rapid, informal but professional communication for urgent clarifications, quick decisions, and asynchronous team coordination. Clear enough to be context-standalone; confidential data stays off these platforms.

## Success Metrics (2025-2026)

- **Response Time**: 80%+ responses within 2 hours during business hours
- **Clarity**: Zero follow-up messages requesting clarification on intent
- **Escalation Accuracy**: 95%+ of escalations to email/call are appropriate (not over-communicating)
- **Archive Value**: Conversations can be understood 30 days later without original context
- **Confidentiality**: Zero breaches (confidential data never in messages)

## Limits & Restrictions

### Technical
- **Message length**: Keep under 200 characters for optimal mobile reading; use threads for longer items
- **Image/file sharing**: Avoid files >5MB; use cloud links for large documents
- **Threading**: Use platform threading (Slack threads, Teams replies) to keep channels clean
- **Reactions**: Platform-specific emoji reactions acceptable for quick acknowledgement
- **Mentions**: @mention only if urgent or directly relevant to recipient
- **Edit capability**: Edit within 1 hour if error; after 1 hour, send follow-up message

### Platform/Algorithm
- **Slack notification model**: @mention and threads trigger notifications; overuse causes muting
- **Teams threading**: Threaded replies prevent channel noise and improve findability
- **WhatsApp groups**: Limited to 256 members; notification fatigue at scale
- **Read receipts**: Different by platform; assume messages are seen within 4 hours during business hours
- **Search behaviour**: Slack/Teams are searchable; WhatsApp threads are ephemeral
- **Integration signals**: Platforms track app switching behaviour; quick replies favour active users

### GGP Etiquette
- **Confidentiality rule**: NO client names, pricing, contract terms, or internal strategy
- **Escalation threshold**: If discussion becomes complex (> 5 exchanges) or emotional, move to call
- **No off-hours urgency**: Respect boundaries; mark truly urgent with [URGENT] tag (rare)
- **No complaints**: Client frustrations, vendor criticism, interpersonal gripes → email or private call
- **Record expectation**: Assume messages may be forwarded; write accordingly
- **Professional tone**: Informal but not sloppy; complete sentences; no heavy slang
- **Attribution**: Cite source if sharing data or claims ("I read in X that Y" or "Per [source] Z")

## Template

### Quick Decision Request

```
[CONTEXT]
We need to confirm the client call time before sending calendar invites.

[QUESTION]
Options: Tuesday 10am, Wednesday 2pm, or Thursday 9am?
Can you confirm by EOD today?

[REFERENCE]
Slack thread above has the context.
```

### Status Update

```
[HEADLINE]
Design review wrapped. Phase 2 is approved.

[DETAILS]
Feedback: simplify the onboarding flow (3 steps → 2).
Timeline: Engineering can start Tuesday.

[NEXT]
I'll send full notes to email thread by EOH.
```

### Urgent Escalation

```
[URGENT] Client has blocked approval pending cost review.

Need your sign-off on one of these:
1. Authorise $15K budget increase
2. Extend timeline 4 weeks

Can we sync for 15 min?
```

## Examples

### Good Example

**Slack message (private channel with team):**

"Design review is 30 min delayed — client traffic jam. Moved to 3pm.

I'll send notes in a thread once we're done. CTA: Can engineering block off 30 min tomorrow to review any feedback?"

---

**Why this works:**
- Specific about what changed (delay, new time)
- Explains why (context, not excuse)
- Clear about next step (notes in thread)
- Specific CTA (30 min block, specific purpose)
- Tone is efficient but not curt
- No rambling or over-explanation

### Bad Example

**Slack message (public channel with external consultants):**

"Hey everyone! So the client meeting got delayed because of traffic or something, and then the budget got cut but I can't really say why. Anyway we need someone to help with the design review but I'm not sure who. Can everyone please let me know if you're available? Also the Q2 pricing model is in the shared drive but don't tell the client we're thinking about cutting costs. Thanks!"

---

**Why this fails:**
- Vague about timing (when is the meeting now?)
- Confidential data disclosed (budget cut, pricing model, client sensitivity)
- Unclear CTA (who should step up?)
- Assumes context knowledge others don't have
- Asks people to keep secrets (unprofessional)
- Too much casual tone for a mixed audience
- Rambling structure; hard to extract action items

## Tactical Guidance

**Messaging Norms by Platform:**

| Platform | Tone | Use Case | Archive? | Confidential? |
|----------|------|----------|----------|---------------|
| **Slack** | Informal, emoji OK | Team coordination, quick decisions | Yes (searchable) | No; public default |
| **Teams** | Professional, structured | Corporate comms, documented decisions | Yes (searchable) | Some channels private |
| **WhatsApp** | Conversational, brief | Urgent clarifications, 1:1s | No; ephemeral | No; assume shared |

**Response Time Expectations:**

- **Urgent**: [URGENT] tag = respond within 30 min or acknowledge delay
- **Standard**: Respond within 2 hours during business hours
- **Async-OK**: No tag = can respond same day or next morning
- **Off-hours**: No expectation of response; don't send work messages after 7pm or before 8am

**When to Escalate to Email:**

- Discussion requires documentation (contract detail, legal term, budget approval)
- More than 5 exchanges required (move to call or email)
- Emotional/interpersonal tension rising (move to private call immediately)
- Message length would exceed 300 characters
- Confidential client data required in explanation
- Needs timestamp/signature accountability

**Threading Discipline:**

- **Slack**: Use thread replies to keep channel readable
- **Teams**: Thread replies preferred; whole-channel notifications only if urgent
- **WhatsApp**: Groups have no threading; use separate DMs for sensitive/complex
- **Benefit**: Threads reduce notification fatigue and improve findability by 3x

**Tone Calibration:**

```
Internal team chat (Slack):
"Got client feedback on the prototype. Simplify the flow. I'll send notes in a sec."

Cross-company with client liaisons (Teams):
"Client review completed. Key feedback: streamline the onboarding flow. Detailed notes will be shared via email for documentation."

Executive request (WhatsApp DM):
"Hi [Name], ready to discuss Q2 plan whenever works. Have 30 min slots Tues 10am or Wed 2pm. Let me know."
```

**Confidentiality Boundaries:**

```
✓ SAFE: "We had client feedback on the interface"
✗ UNSAFE: "[Client name] says the interface is ugly"

✓ SAFE: "Budget constraints mean we need to scope down"
✗ UNSAFE: "Client cut budget from $500K to $350K"

✓ SAFE: "I'll send strategy notes to email for record"
✗ UNSAFE: "Here's our margin model for the project"
```

## Pre-Publication Checklist

- [ ] Message is under 200 characters (or appropriately threaded)
- [ ] No client names, pricing, contract terms, or internal strategy
- [ ] No complaints about people, clients, or vendors
- [ ] CTA is specific: "Confirm by EOD Thursday" not "let me know soon"
- [ ] Tone is informal but professional (complete sentences, no slang)
- [ ] Context is self-contained (reader doesn't need to hunt previous messages)
- [ ] If urgent, tagged [URGENT] only (rare usage; every tag dilutes the signal)
- [ ] [URGENT] messages have explicit deadline: "Need response by 3pm today"
- [ ] If sharing data/claims, source is cited or apparent ("I read in Slack that..." OK, "everyone knows..." NOT OK)
- [ ] Tone matches platform and audience (Slack team ≠ WhatsApp with vendor)
- [ ] Willing to have conversation move to call/email if needed (not forcing async resolution)
- [ ] GGP markers check: Are claims attributed? Is context clear enough for archive reading?
- [ ] Pass the "read in 30 days" test: Will this message make sense without original context?

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 17. MIT SMR
> Source: `references/channels/mit-smr.md`

# MIT Sloan Management Review (MIT SMR)

## Goal

Research-backed insights at the technology-management intersection for practitioner audiences. Combine data rigor with accessible prose for executives leading digital/technology initiatives.

## Success Metrics (2025-2026)

- **Publication**: Accepted by MIT SMR editorial team; published in print and digital
- **Reach**: 2M+ impressions across MIT SMR platforms within 30 days
- **Engagement**: 1K+ shares, 300+ comments on published article
- **Citation**: Featured in management journals or cited in business books within 18 months
- **Thought Leadership**: 2+ speaking invitations from tech conferences or corporate events
- **Community**: Article becomes case study or teaching material in MBA/executive programs

## Limits & Restrictions

### Technical
- **Word count**: 3000-5000 words (MIT SMR longer-form than HBR)
- **Structure**: Title, subtitle, abstract (100 words), body, call-out boxes (key insights), bibliography
- **Citations**: Chicago Manual of Style (footnotes); APA acceptable
- **Data visualization**: Encourage 2-4 charts or graphs; must be original research or properly licensed
- **Methodology**: Include 1-2 paragraphs on research design (sample size, methodology, limitations)
- **Format**: Submitted via MIT SMR editorial portal with accompanying materials

### Platform/Algorithm
- **Editorial cycle**: 2-3 months from submission to acceptance/rejection
- **Peer review**: Light review by subject-matter experts (not full academic peer review)
- **Audience focus**: Readers are CTOs, Chief Digital Officers, VP Product—tech-savvy executives
- **Data emphasis**: MIT SMR prioritises original research; submissions with primary data have higher acceptance
- **Technology angle**: Must connect business outcome to technology capability or digital transformation
- **Video integration**: Recent articles increasingly include embedded video explainers (optional but encouraged)

### GGP Etiquette
- **Methodological transparency**: Explain research design, sample size, confidence intervals, limitations
- **Balanced analysis**: Address where technology fails and succeeds; avoid tech solutionism
- **Practitioner focus**: Academic rigor doesn't mean academic language; write for busy executives
- **Attribution mandatory**: Name researchers, link studies, acknowledge prior work in field
- **No vendor bias**: If discussing specific vendors or platforms, disclose any financial relationships
- **Competitive fairness**: Can compare technologies; avoid unfair characterization of competitors

## Template

```
[TITLE — Specific, tech-management focused]
AI in Product Development: Why Most Companies Adopt Before They're Ready

[SUBTITLE — Expands title, adds research hook]
A longitudinal study of 180 tech companies reveals that organisations implementing AI tools without changing development workflows waste 60% of AI capability. Here's why timing matters.

[ABSTRACT — 100 words, structured]
Artificial intelligence tools are powerful, but implementation is harder than vendors suggest. This article reports findings from a three-year longitudinal study of 180 technology companies adopting AI for product development. We found that companies implementing AI tools before establishing shared data practices and updated workflows realized 60% lower ROI than peer companies. Success factors included executive alignment on AI-driven decision-making, reorganisation of analytics roles, and investment in data literacy across product teams. Companies that adopted these practices saw 40% faster time-to-market and 25% reduction in product-market-fit validation time. Implications address when to adopt AI, how to organize teams, and measurement frameworks for AI investment.

[INTRODUCTION — Problem + research question]
[Hook: Surprising observation or statistic]
Example: "Most AI implementations fail not because the technology doesn't work, but because organisations aren't ready. Gartner reports 60% of AI projects never reach production. Our research suggests the culprit is organisational, not technological."

[Problem: Why this matters now]
The pressure to adopt AI is intense. Investors demand AI strategies. Competitors are moving. Executives feel urgency. But urgency without readiness leads to waste: expensive tools underutilised, teams frustrated, board expectations unmet.

[Research question: What we studied]
"Under what conditions do AI implementations succeed in product development?" We tracked 180 technology companies over three years, measuring AI tool adoption, organisational changes, and business outcomes.

[Preview of findings]
Our research identified four critical success factors beyond the technology itself. Organisations excelling on all four saw 3x better ROI than peers. Most companies miss at least two.

[BODY SECTION 1 — Research methodology and context]
[Data collection: How we studied this]
2-3 paragraphs explaining research design without jargon.

Example: "We surveyed 180 software and technology companies in 2022, tracking their AI adoption over 24 months. Companies ranged from startups ($5M-50M ARR) to enterprises ($1B+ ARR) across industries. We conducted in-depth interviews with Chief Product Officers, VP Product, and data/analytics leaders at 40 of these companies. We measured AI tool adoption as percentage of product team using AI tools (code generation, design, analytics). Business outcomes tracked were time-to-market, feature adoption rate, time-to-product-market-fit validation, and product revenue impact. Confidence intervals at 95% unless noted."

[Context and limitations]
"Our study focused on B2B software; findings may not apply to other industries. Most companies were based in North America; we can't generalise globally. We didn't study hardware or manufacturing AI applications."

[BODY SECTION 2 — Findings with data]
[Finding 1: Critical success factor + evidence]
Factor 1: Shared Data Practices Before AI Tools

Companies that succeeded established centralized, clean data infrastructure before deploying AI tools. Companies that deployed AI tools into fragmented data environments realized 60% lower accuracy in AI outputs (95% CI: 52-68%).

Data: Among 180 companies, those with centralized data lakes (n=45) had average AI ROI of $3.2 per dollar invested. Companies with fragmented data (n=135) had average ROI of $1.2 per dollar invested. Difference is statistically significant (t=4.2, p<0.001).

Implication: Build the data foundation before the AI tools.

[Finding 2: Critical success factor + evidence]
Factor 2: Reorganised Roles, Not Just New Tools

Companies that redesigned product team roles for AI-assisted workflows outperformed peers. Redesign included: clear data ownership, analytics embedded in product squads, upskilled existing talent vs. hiring new specialists.

Data: Teams that reorganised had average AI tool adoption of 78% (n=60 companies). Teams that just deployed tools kept adoption at 22% (n=120 companies). High-adoption teams shipped 40% faster time-to-market (p<0.001).

Implication: Reorganisation is as important as technology.

[Finding 3: Critical success factor + evidence]
Factor 3: Executive Alignment on Decision-Making

When executives committed to AI-informed decision-making (vs. AI-assisted decision-making), implementation success increased. Difference: Are leaders willing to act on AI insights, or do they override them?

Data: 45 companies where executives committed to AI-informed decisions saw 65% feature adoption rate (vs. 38% in comparison group). Time-to-validation decreased from average 12 weeks to 8.5 weeks. (95% CI: 7.2-9.8 weeks).

Implication: Culture change precedes technology change.

[Finding 4: Critical success factor + evidence]
Factor 4: Investment in Data Literacy

Companies invested in cross-team training (product managers learning to evaluate AI outputs, designers understanding AI constraints). Average 20 hours per employee in year 1. Literacy led to better tool utilisation and realistic expectations.

Data: Companies investing in training (n=67) had 55% higher employee confidence in AI tools (measured by survey). Tool adoption was 3x higher. Turnover in product organisations was 8% (vs. 14% in non-training companies).

Implication: People change is part of technology change.

[BODY SECTION 3 — Framework and mechanisms]
[Why these factors matter: The mechanisms]
2-3 paragraphs explaining the causal relationships.

Example: "Why does data infrastructure precede AI tools? Because AI tool accuracy depends on data quality. Tools don't fix bad data; they amplify it. Companies that discover this after tool purchase waste budget and erode trust. Lesson: Invest in data quality before AI vendor spend."

"Why does reorganisation matter more than we expected? Because AI tools change the work. Product managers who previously made intuitive calls now need to interpret AI recommendations. Designers need to understand AI constraints on feasibility. These roles are fundamentally different. Companies that kept organisational structure static faced adoption resistance: 'I don't know how to use this.' Companies that clarified roles and skill requirements had adoption at 3x higher rates. Culture follows structure."

[BODY SECTION 4 — Practical implications]
[For different audiences]

**For Chief Product Officers implementing AI:**
- Start with data: Before buying AI tools, audit your data foundation. Clean, centralized data is prerequisite.
- Plan 18 months: Year 1 for data infrastructure, organisational redesign, and training. Year 2 to realize benefits.
- Measure inputs and outputs: Track data quality, adoption rate, and business metrics (time-to-market, validation time). Don't measure tool usage; measure business impact.
- Executive commitment: Board and exec team must align on "we will act on AI insights." If executives override AI recommendations, adoption will fail.

**For VP Product managing implementation:**
- Embed data/analytics in product squads: Rather than central team serving product, integrate analytical capability into squads.
- Upskill existing talent: More than 80% of companies in our study hired new data specialists. We recommend upskilling 60% of product team internally (lower risk than external hiring).
- Set realistic expectations: AI enables better faster decisions, not perfect decisions. Communicate this.
- Run pilots: Select one product area to pilot AI tooling with all four success factors. Measure rigorously. Expand only if results are positive.

**For data leaders building infrastructure:**
- This is a product, not a project: Centralized data infrastructure requires ongoing investment. Budget for 3-5 year horizon.
- Measure data quality, not data volume: Quality (accuracy, completeness, timeliness) matters for AI. Size of data lake doesn't.
- Embed in product workflows: Data infrastructure that lives separate from product teams won't be used. Integrate data access into product tools and decision processes.

[BODY SECTION 5 — Complications and limitations]
[What doesn't work, why findings might not apply]

Complication 1: [Limitation or exception]
"Our study focused on B2B software. Hardware manufacturing, healthcare, and regulated industries may have different constraints. AI ROI might be lower in regulated industries due to governance requirements; we can't generalise from our data."

Complication 2: [Valid criticism]
"Why do 60% of AI projects still fail in general if our findings are correct? Our research is focused on companies that adopted AI successfully. We didn't study failed implementations. It's possible the four factors are necessary but not sufficient; other barriers exist we didn't measure."

Complication 3: [Trade-offs in the argument]
"Building data infrastructure takes time and money. Executives want ROI fast. Our recommendation to spend year 1 on data infrastructure before AI tools might feel too slow for some organisations. We can't say this is optimal in all competitive environments."

[Acknowledge reasonable objection]
"If these factors are so clear, why aren't more companies executing on them?" Inertia. Organisational change is hard. Vendor pressures are intense (sales teams selling tools, not data infrastructure). Data work isn't flashy; executives see AI tools as innovation. Training doesn't get headlines. But companies that play the long game—building foundation, then tools—have much better outcomes.

[BODY SECTION 6 — What's next]
[Future research, evolving landscape]

Looking ahead, we see three emerging dynamics:

1. **Regulatory landscape**: As governments regulate AI in product development, companies will need different governance than current best practices. Future research should examine how compliance affects ROI.

2. **Generative AI shift**: Our research focused on AI for analytics and recommendation. Generative AI (LLMs) changes the game further. We're beginning new research on how teams organize for generative AI; preliminary data suggests training and culture matter even more.

3. **Vendor consolidation**: As AI capabilities get embedded in product platforms (Figma with AI design, GitHub Copilot in IDEs), the boundaries blur. Future product teams might need less standalone AI tool evaluation and more platform integration planning.

[CONCLUSION — Thesis reinforced]
[Restate thesis]
AI success in product development depends more on organisation and data infrastructure than on tool selection. Our three-year study of 180 companies shows that companies excelling on four critical factors—data infrastructure, role reorganisation, executive alignment, and training—achieve 3x better ROI than peers.

[Call to action]
The next twelve months are critical for tech leaders. AI capability is becoming table stakes. But implementation approach determines whether AI becomes a strategic advantage or expensive distraction. Organisations should start with the data and organisational foundation, not the tools. Short-term this is slower. Long-term this is faster.

[Key Takeaways Box]
• AI tool success depends on data infrastructure first: companies with centralized data saw 3x better ROI
• Reorganisation is as important as technology: teams with redesigned roles had 3x higher tool adoption
• Executive alignment on AI-informed decision-making is critical: companies with aligned leadership saw 65% feature adoption vs. 38%
• Data literacy training increases adoption and retention: companies investing in training had 55% higher employee confidence and 8% lower turnover
• Start with data infrastructure (year 1), deploy tools (year 2): companies following this timeline saw 40% faster time-to-market
```

## Examples

### Good Abstract Example

"Digital transformation initiatives typically fail because organisations focus on technology before clarifying strategy. A three-year study of 120 enterprises found that companies establishing digital strategy clarity before technology selection achieved 4x higher adoption and 2.5x faster time-to-value. This article identifies five elements of strategic clarity that predict success and offers a framework for assessing your organisation's readiness."

---

**Why this works:**
- Opens with problem (failure mode)
- Includes specific data (n=120, 4x higher, 2.5x faster)
- Previews findings (five elements) and value proposition (readiness framework)

## Tactical Guidance

**Research Quality Requirements:**

- Minimum sample size: 50+ organisations (preferably 100+)
- Longitudinal preferred: Track changes over 12+ months
- Mixed methods: Combine quantitative metrics (adoption rate, ROI, time-to-value) with qualitative insights (interviews, case studies)
- Transparency on limitations: Be explicit about what your research can and cannot conclude

**Data Visualization Standards:**

```
Good chart for MIT SMR:
- Title explains finding, not just "Revenue Growth"
- Legend is clear (colours have meaning)
- Axes are labelled with units
- Caption explains what the data means (not just what it shows)
- Data points include error bars or confidence intervals if comparative

Example caption:
"Figure 2: Time-to-market improvement by data maturity level.
Companies with mature data practices (n=45) reduced time-to-market by average 40% (95% CI: 32-48%) compared to companies with fragmented data (n=135). Difference is statistically significant (t=4.2, p<.001)."
```

**Audience Calibration:**

```
MIT SMR reader is:
✓ CTO or Chief Digital Officer (tech-savvy, strategic focus)
✓ VP Product (product-focused, wants practical guidance)
✓ COO or CEO considering digital investment (wants ROI clarity)

Write for this reader:
✓ "Average time-to-value decreased from 16 weeks to 9 weeks" [specific, measurable]
✓ "Reorganisation was necessary: companies that deployed tools without changing roles had 22% adoption" [explains trade-offs]
✓ "We measured X because Y matters for decision-making" [justifies metrics]

Don't write:
✗ "Digital transformation is important" [obvious]
✗ "Technology is changing fast" [not news]
✗ "Companies should invest in AI" [too vague]
```

**Submission Process:**

1. Check MIT SMR submission guidelines and editorial focus areas
2. Query before full draft (optional but encouraged; reduces rejection risk)
3. Submit via portal with: title, abstract, author bio, research methodology section
4. Include data supporting key findings (appendix with raw data helps evaluation)
5. Expect 6-8 weeks for editorial decision
6. Revise based on feedback (editors are collaborative if core thesis is sound)

## Pre-Publication Checklist

- [ ] Title clearly indicates technology-management intersection (not just technology)
- [ ] Abstract is 100 words, structured, includes sample size and key finding with data
- [ ] Article is 3000-5000 words; research is rigorous
- [ ] Research methodology section explains sample, data collection, limitations clearly
- [ ] All numerical findings include confidence intervals or statistical significance tests
- [ ] Findings are presented with data first, interpretation second
- [ ] Framework section explains mechanisms (why the findings matter, not just what they are)
- [ ] Practical implications are specific to different audiences (CTOs, VPs, CEOs)
- [ ] Limitations and complications acknowledged; author doesn't overstate conclusions
- [ ] Tone is authoritative but accessible (academic rigor + executive readability)
- [ ] 2-4 data visualizations included; captions explain findings clearly
- [ ] All claims attributed: named researchers, linked studies, proper citations
- [ ] No vendor bias or undisclosed financial relationships mentioned in article
- [ ] Bibliography is complete and properly formatted (Chicago style)
- [ ] GGP markers check: Every factual claim [VERIFIED] with citation. Methodology section includes confidence intervals [ESTIMATED]. Inferences labelled as interpretations [INFERENCE]. Limitations acknowledged [CAUTION].
- [ ] Peer-reviewed for accuracy by colleague in field before submission

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 18. Op-Ed
> Source: `references/channels/op-ed.md`

# Op-Ed

## Goal

Persuasive opinion piece published in newspapers or magazines that shapes public/industry conversation around an issue, establishes author expertise, and influences policy or corporate action.

## Success Metrics (2025-2026)

- **Publication**: Accepted by tier-1 publication (newspaper op-ed section, major magazine)
- **Reach**: 500K+ impressions on publication platform within 7 days
- **Influence**: 3+ media mentions or citations of op-ed arguments; policy/business response
- **Dialogue**: 100+ comments demonstrating reader engagement with thesis
- **Opportunity**: Speaking invitations, interviews, thought leadership visibility from publication
- **Syndication**: Republished in 2-3 additional outlets

## Limits & Restrictions

### Technical
- **Word count**: 600-800 words (editors strict on limits; cut ruthlessly)
- **Structure**: Strong thesis opening, argumentation, counterargument acknowledgement, call-to-action close
- **Format**: Submitted as plain text or Word doc; single-spaced
- **Title options**: Provide 2-3 suggested titles (editors often change them)
- **No graphics**: Pure text (graphics require pre-approval)
- **Byline**: Author credentials in 1 sentence (your title + affiliation)
- **Author bio link**: 2-3 sentence biography with URL for major publications

### Platform/Algorithm
- **Editorial cycle**: 2-4 weeks from pitch to publication (faster than longer-form)
- **News pegs**: Op-eds tied to current events/news get higher priority
- **Section placement**: Op-ed sections are premium real estate; competition is intense
- **Editor preference**: Contrarian but defensible views outperform conventional takes
- **Social amplification**: Op-eds that generate debate are shared widely
- **Searchability**: Op-eds rank well in Google search for key terms; durable shelf life (unlike breaking news)

### GGP Etiquette
- **Thesis must be defensible**: "Everyone agrees..." or unsourced claims damage credibility
- **Counter-argument acknowledgement**: Fair treatment of opposing view strengthens persuasive power
- **Attribution required**: Name sources for empirical claims; don't cite "most people think"
- **No client complaints**: Never use op-ed to air corporate grievances (damages credibility)
- **Transparency on stake**: If you have financial interest in outcome, disclose it
- **Factual accuracy**: Op-eds have lower fact-check standards than news; still demand rigor

## Template

```
[TITLE — Provocative, clear, searchable]
Why Companies Are Failing at Digital Transformation—and How to Fix It

[SUBTITLE (optional, not always used by editors)]
The problem isn't technology. It's organisational design.

[BYLINE]
By [Your Name], [Your Title] at [Company/Organisation]

---

[OPENING — Hook + thesis (2-3 sentences)]
Seventy percent of digital transformation initiatives fail. Not because companies lack resources or strategy. Not because technology is immature. The culprit is organisational design. Companies are trying to execute digital products within organisational structures built for industrial-era manufacturing. These structures don't work.

[CONTEXT — Why this matters now (2-3 sentences)]
This becomes urgent as companies face existential pressure to transform faster. The organisations winning the transformation race have redesigned their structures. The ones losing are rearranging deck chairs—new leaders, new consultants, same organisational structure.

[EVIDENCE — Data or example supporting thesis (3-4 sentences)]
A recent study of 200 enterprises found that organisations with cross-functional product teams outperformed traditionally structured organisations on time-to-market by 40%, and on product adoption by 2-3x. The difference wasn't investment level or talent quality. It was structure. Functional silos create handoff delays and diffused accountability. Cross-functional teams eliminate both.

[WHAT'S WRONG WITH CURRENT APPROACH — Specific critique (4-5 sentences)]
Most transformation initiatives focus on two levers: technology and leadership. New CTO, new tools, new strategy. Leaders are smart and well-intentioned, but they're trying to execute 21st-century products within 20th-century organisational structures. It's like trying to fly a plane designed for trains. The structure will always push back against the new capability you're trying to build.

The problem shows up in familiar ways: Cross-team decisions taking months. Product managers frustrated by dependencies. Engineers blocked by approval delays. Talent leaving because the organisation can't move fast enough. These aren't leadership failures or bad strategy. They're structural problems.

[COUNTER-ARGUMENT — Acknowledge opposing view fairly (2-3 sentences)]
Some will argue that organisational structure matters less than execution discipline, accountability, and clear strategy. They're partly right. Structure alone won't fix bad strategy or poor leadership. But structure is the necessary foundation. Without it, good strategy still fails.

[SOLUTION — What to do about it (4-5 sentences)]
Companies need to move from functional to cross-functional product organisation. Three shifts matter:

First, move decision rights closer to customers. Decisions that now take 6 months in committee can happen in 6 days when the team has authority.

Second, staff teams for end-to-end ownership. One team owns the customer outcome, not the technical layer or the user experience layer.

Third, invest in developing generalists with depth. Most enterprises have narrow specialists (data scientist, UX designer, product manager) working separately. High-performing teams have people with broad skill sets who can collaborate across boundaries.

[CALL TO ACTION — Specific ask (2-3 sentences)]
If you're leading a transformation, ask yourself: Do I have organisational structure designed for digital products? Or am I trying to execute digital work within industrial-era structure? If it's the latter, your strategy and technology investments won't solve the problem. Fix the structure first.

[CLOSING — Reinforce thesis]
Digital transformation isn't about technology. It's about organisational capability. And capability flows from structure.

---

[BYLINE PARAGRAPH]
[Your Name] is [Your Title] at [Company]. She leads [relevant work]. Her research on organisational design has been published in Harvard Business Review. She can be reached at [email].
```

## Examples

### Good Op-Ed Opening (Full Example)

**Title:** "Why Your Company Will Never Become Agile"

**Byline:** By Sarah Chen, VP Product at [Company]

---

"Your company will never become agile. Not because you lack commitment, talent, or investment. You will fail because you're trying to run agile practices inside a waterfall organisation.

This is the transformation paradox: Companies want to build products faster and adapt to changing markets. They hire agile coaches, implement Scrum, flatten hierarchies. But the fundamental organisational structure—functional silos with centralised approval gates—remains unchanged. Agile practices are like islands of speed surrounded by an ocean of process.

I've watched 50+ organisations attempt this transformation. The ones that succeed move decision authority to product teams and reorganise around customer outcomes. The ones that fail keep decision authority at the top and layer agile rituals on top of waterfall structure.

The problem isn't effort or intelligence. It's structural misalignment. And there's a fix. But it requires uncomfortable changes."

---

**Why this works:**
- Opening is provocative but defensible (not "all companies fail")
- Specific problem identified (structural misalignment)
- Personal credibility established (I've watched 50+ orgs)
- Pattern named (islands of speed surrounded by process)
- Implies solution without fully revealing it (compels reading)

### Bad Op-Ed Opening

**Title:** "Everyone Should Be Agile Now!!!"

"Companies need to be agile! The world is changing so fast! Agile is the future! Every company I know is trying to become agile, and it's about time. If you're not agile, you're already behind.

Traditional structure is dead. Waterfall is dead. The old way of doing business is dead. We need new ways of thinking and new organisational approaches.

This article explains why agile is important and why your company should be agile."

---

**Why this fails:**
- Hyperbolic title with triple exclamation marks
- No specific thesis (just "agile is good")
- Sweeping generalisations without data
- Doesn't acknowledge why transformation is hard
- No meaningful insight
- Prescriptive without explanation (no persuasion)
- Would be rejected by editors immediately

## Tactical Guidance

**Publication Selection:**

Tier 1: Major newspapers with strong op-ed sections (high prestige, harder to place)
- Wall Street Journal
- New York Times
- Financial Times
- Washington Post

Tier 2: Industry-specific publications (easier to place, strong niche reach)
- Harvard Business Review
- MIT Sloan Management Review
- Industry journals (tech: Wired, CRN; finance: American Banker, etc.)

Tier 3: Online publications and blogs (easiest to place, good for building clips)
- Medium
- LinkedIn
- Industry blogs

Strategy: If rejected by Tier 1, submit to Tier 2. If Tier 2 rejects, pitch to Tier 3. Each rejection should include feedback to improve messaging.

**News Peg Strategy:**

Op-eds tied to current events get higher priority. Examples:
- CEO announces layoffs → Op-ed on restructuring (tying to current CEO announcements)
- Congress proposes regulation → Op-ed on regulation impacts
- Competitor announces transformation → Op-ed on transformation challenges
- Industry report releases → Op-ed reacting to findings

Best op-eds are published within 2-3 days of news peg. Plan accordingly.

**Pitch Email (to op-ed editor):**

```
Subject: Op-Ed Pitch: Why Companies Fail at Digital Transformation [Headline Length]

Dear [Editor Name],

I'm pitching an op-ed for [Publication], tying to [current news peg].

Thesis: [One sentence summarizing the argument]

Why now: [Why this is timely; connection to recent news, trend, or event]

Author platform: [Brief credibility statement; 1-2 sentences]

I've written op-eds for [previous publications if applicable]. Happy to adjust angle or length per your preferences.

Best regards,
[Your name]
[Title]
[Email/Phone]
```

**Word Count Discipline:**

Op-eds have strict word limits (600-800). Editors cut ruthlessly:

```
600-word article structure:
- Opening/hook: 75 words (3 sentences)
- Context: 100 words (4-5 sentences)
- Evidence/example: 125 words (5-6 sentences)
- What's wrong: 125 words (5 sentences)
- Counterargument: 75 words (3 sentences)
- Solution: 125 words (5 sentences)
- CTA/closing: 75 words (3 sentences)

Total: ~700 words (editors will trim to 650-750)

Strategy: Write tight from the start. Every sentence must earn its space. If you have 10 words to say, cut ruthlessly to 6. Density makes op-eds powerful.
```

**Persuasion Architecture:**

Effective op-eds follow this pattern:

1. **Hook** (Problem): Why does this matter? What's broken?
2. **Evidence** (Why I'm right): Data, example, or expert validation
3. **Pushback** (Why skeptics might disagree): Fair treatment of opposing view
4. **Solution** (What to do): Specific, actionable next steps
5. **CTA** (Call-to-action): What you want readers to do

Missing any element weakens persuasion.

**Editing Before Submission:**

Before sending to editors:
- Read aloud; cut any sentence that feels slow or unclear
- Challenge every claim: Is this verifiable? Do I have a source?
- Count words; make sure you're in the 600-800 target
- Have a colleague read and ask: "What's the main argument?" If they can't answer clearly, revise
- Check for clichés and corporate jargon; replace with specific language

## Pre-Publication Checklist

- [ ] Thesis is clear and stated in opening paragraph (not buried)
- [ ] Thesis is defensible with evidence; not hyperbolic claim
- [ ] Word count is 600-800 (will be cut to 650-750 by editor)
- [ ] Opening hook establishes why this matters (not abstract)
- [ ] Evidence or example provided for main argument
- [ ] Counterargument acknowledged and treated fairly
- [ ] Solution proposed; specific and actionable (not vague)
- [ ] CTA is clear (what do you want readers to do?)
- [ ] Every claim is sourced or attributed (no unsourced assertions)
- [ ] Tone is authoritative but conversational (not academic or preachy)
- [ ] No jargon or corporate speak; clear language
- [ ] Author credentials are clear; byline includes relevant title/affiliation
- [ ] 2-3 title options provided (editors often change titles; give options)
- [ ] Current news peg identified (what makes this timely?)
- [ ] Publication targeted appropriately (is this the right outlet for this argument?)
- [ ] GGP markers check: All factual claims are [VERIFIED] with sources cited. Data points include sources. Counterargument is fair [ATTRIBUTION] to opposing view. No unverified claims.
- [ ] Peer-reviewed by colleague for clarity and persuasiveness
- [ ] Ready to submit: Published via publication's submission portal or emailed to op-ed editor

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 19. Presentations
> Source: `references/channels/presentations.md`

# Presentations

## Goal

Structured narrative delivered via slides that educates, persuades, or aligns audiences. Establish credibility, influence decisions, and create memorable takeaways for conference talks, client pitches, or internal briefings.

## Success Metrics (2025-2026)

- **Engagement**: 90%+ of audience remains attentive (not scrolling phones)
- **Comprehension**: Post-presentation survey shows 85%+ audience understood main thesis
- **Action**: Pitch presentations convert 30%+ to follow-on conversations or decisions
- **Feedback**: Audience ratings 4+/5 on "clarity and usefulness" dimension
- **Reach**: Conference talk video reaches 50K+ views if recorded
- **Influence**: Presentation shapes internal decision or external media coverage

## Limits & Restrictions

### Technical
- **Slide count**: 15-20 slides for 30-min talk (1-2 min per slide pacing)
- **Text per slide**: Max 40 words; one message per slide
- **Font size**: 44pt minimum for readability at distance
- **Visuals**: Max 1 chart or image per slide; high quality
- **Deck file**: PDF preferred (universal; no compatibility issues)
- **Animations**: Avoid (distracting); if used, minimal and purposeful
- **Speaker notes**: Detailed notes (100-200 words) for each slide; separate from slide text

### Platform/Algorithm
- **Attention curve**: Opening and closing are highest engagement; middle sags (structure matters)
- **Visual dominance**: Audience retains 50% of message from visuals; 10% from words alone
- **Change fatigue**: Changing topics every 1-2 slides keeps attention; 5+ slides on same topic loses audience
- **Interaction**: Live polls, Q&A sections boost engagement; passive talks lose energy
- **Recording dynamics**: Recorded talks require tighter pacing (live audience is more forgiving)
- **Deck searchability**: If deck is shared post-talk, title slide keywords determine findability

### GGP Etiquette
- **Data integrity**: All charts, metrics, and claims must be accurately represented (manipulation is visible)
- **Source attribution**: Data sources noted on slides (small text ok; must be present)
- **Speaker authority**: Speak to your expertise; don't overstate knowledge
- **Confidentiality**: Never mention client names, pricing, or internal strategy
- **Slide clarity**: Slides must be understandable independently (often shared post-talk without speaker context)
- **Counterargument fairness**: If presenting contested positions, acknowledge valid alternatives

## Template

### Structure: Situation-Complication-Resolution (SCR)

This is consulting-style structure; works across contexts (pitches, internal briefings, conference talks).

```
[SLIDE 1: TITLE SLIDE]
[Large, clean title]
[Subtitle with key question or insight]
[Your name, title, organisation]
[Contact: email or LinkedIn]

[SPEAKER NOTES]
"Thank you for having me. Today I want to explore a question most of you probably face: [Open with question related to your topic]. The answer is counterintuitive, and I'll show you evidence. By the end of this talk, you should be able to [CTA: what they can do with this insight]."

---

[SLIDE 2: AGENDA — Optional but helpful for longer presentations]
[3-4 topics covering]
"We'll cover three things: first, the scope of the challenge. Second, why current approaches fall short. Third, a specific pattern you can apply. Questions at the end."

---

[SLIDE 3: THE SITUATION — Context and scope]
[Visual: Photo, chart, or headline representing the problem]
"Organisations are under pressure to transform digitally. 80% say digital transformation is critical. Yet 70% of initiatives fail to achieve objectives."

[SPEAKER NOTES]
"Everyone in this room is dealing with this pressure. Digital transformation isn't optional anymore. Yet most transformations don't deliver the returns companies expect. Why? That's what we're here to explore today."

---

[SLIDE 4: THE COMPLICATION — Why situation is complex/hard]
[Visual: Chart showing multiple factors or constraint]
"Transformation is hard because it's not just about technology. Three factors matter equally: strategy, organisational design, and technology. Most companies underinvest in organisational design."

[SPEAKER NOTES]
"You might think the challenge is technology. It's not. We have great technology. The challenge is organisational. Companies are trying to execute digital products in organisational structures built for industrial manufacturing. That's like trying to run a marathon in snow boots."

---

[SLIDE 5: THE RESOLUTION — What to do about it]
[Visual: Simple framework showing the solution]
"Success requires three moves: (1) Restructure around product outcomes, not functions. (2) Distribute decision authority to teams. (3) Invest in cross-functional skills."

[SPEAKER NOTES]
"Here's what successful companies do differently. First, they organise around the customer outcome they're trying to achieve, not around functions. This sounds simple, but it's a fundamental restructuring. Second, decision authority moves from committee to team. This reduces decision latency from months to weeks. Third, they invest in people who can span boundaries—people who understand product, technology, and business."

---

[SLIDE 6: EVIDENCE — Data supporting resolution]
[Visual: Chart showing the impact]
"Organisations with cross-functional product teams show: 40% faster time-to-market, 2x higher feature adoption, 25% better employee retention."

[SPEAKER NOTES]
"This isn't theory. We studied 200 organisations over three years. The ones that made these three moves outperformed peers significantly. 40% faster to market. That translates to competitive advantage. 2x higher adoption. That's the difference between success and failure. And retention matters—transformation is hard; you need your best people to see it through."

---

[SLIDE 7: PATTERN 1 — First element of solution with example]
[Visual: Diagram of pattern OR photo of implementation]
"Pattern 1: Cross-Functional Product Teams
- One team owns customer outcome end-to-end
- Includes product, design, engineering, data roles
- Located co-located or virtually synchronised
- Decision authority resides with the team"

[SPEAKER NOTES]
"Let's dig into how this works. Imagine you want to ship a new feature. In traditional organisations, this requires coordination across functions: product asks engineering, engineering coordinates with data, design is separate, QA is downstream. By the time all stakeholders align, six months have passed.

In cross-functional teams, all stakeholders are in one team. Same feature? Decision happens in one meeting. Execution is coordinated internally. Time to market drops from 6 months to 6 weeks."

---

[SLIDE 8: PATTERN 1 CASE EXAMPLE]
[Visual: Photo or org chart showing transformation]
"Company A: Moved from 12 functional departments to 6 cross-functional product teams. Result: Time-to-market dropped from 18 weeks to 8 weeks."

[SPEAKER NOTES]
"This is a real example (anonymised for confidentiality). They had traditional structure: separate engineering, design, product, data teams. They reorganised around products: one team per major customer problem. What happened? Not just speed. Quality improved. Engagement improved. Employees reported feeling more ownership."

---

[SLIDE 9: PATTERN 2 — Second element of solution]
[Visual: Diagram showing decision distribution]
"Pattern 2: Distributed Decision Authority
- Strategic decisions made centrally (vision, investment levels)
- Operating decisions made by teams (which features, which trade-offs)
- Clear decision frameworks ensure alignment without consensus"

[SPEAKER NOTES]
"The second pattern is about decision-making. Traditional organisations require consensus or senior approval for most decisions. This creates bottlenecks. High-performing organisations draw a clear line: what decisions happen at centre vs. what decisions happen at team level. This is not a free-for-all. Teams operate within guardrails. But guardrails set the boundary; they don't dictate every decision."

---

[SLIDE 10: PATTERN 2 CASE EXAMPLE]
[Visual: Decision framework or before/after timeline]
"Company B: Shifted 40% of product decisions from approval committee to team level. Result: Decision velocity increased 5x. Strategic alignment maintained."

[SPEAKER NOTES]
"Another real example. They had a product approval committee. Every feature needed sign-off. Average time from proposal to decision: 8 weeks. They created a decision framework: features under $500K impact? Team decides. Over $1M? Committee reviews. Strategic roadmap? Committee aligns; teams implement. Decision time dropped to 1-2 weeks. Alignment was actually better because people understood the boundaries."

---

[SLIDE 11: PATTERN 3 — Third element of solution]
[Visual: Role evolution diagram or skill matrix]
"Pattern 3: Generalists-with-Depth
- Hire and develop people with broad capabilities
- Cross-functional skill enables better collaboration
- Specialists still needed, but fewer and more distributed"

[SPEAKER NOTES]
"Third pattern: how you structure roles. Most enterprises have narrow specialists: data scientist does data, UX designer does design, product manager does product. Each role is deep but narrow. This creates handoffs and misalignment.

High-performing teams have people with broader skills. Product manager who understands data. Designer who understands engineering constraints. Engineer who understands product strategy. These broad skills don't replace depth; they enable better collaboration across boundaries."

---

[SLIDE 12: PATTERN 3 CASE EXAMPLE]
[Visual: Skills matrix or hiring approach]
"Company C: Developed product managers with data fluency. Result: Better product decisions, faster validation, reduced analytical handoffs."

[SPEAKER NOTES]
"In this case, they invested in teaching product managers basic data analysis. Not to replace data scientists. But to reduce dependency. Product managers could validate their own hypotheses. They could review analytics without waiting for analyst review. They asked better questions of data team because they understood the constraints. Outcome: faster iteration, smarter decisions."

---

[SLIDE 13: OBJECTIONS/COMPLICATIONS]
[Visual: Balanced representation of challenges]
"Common concerns:
- 'Won't specialization suffer?' Specialization is preserved; collaboration improves.
- 'This only works for tech companies.' Works across industries (we've seen examples in finance, retail, manufacturing).
- 'This is too much change at once.' Phased approach: start with one team, pilot, expand."

[SPEAKER NOTES]
"I know what you might be thinking. Won't we lose expertise? We studied this. Actually, expertise deepens when specialists have to teach others and collaborate across boundaries. Specialists in high-performing organisations are more expert, not less.

You might also be thinking: 'This only works for tech companies.' We've run pilots in finance (trading, risk management), retail (merchandising, supply chain), manufacturing (product development, supply). Principles apply broadly. The structure looks slightly different, but the logic is the same."

---

[SLIDE 14: IMPLEMENTATION FRAMEWORK]
[Visual: Timeline, roadmap, or phased approach]
"Three-Phase Implementation:
Phase 1 (0-3 months): Pilot with one product team; learn what works
Phase 2 (3-6 months): Expand to 2-3 teams; refine approach
Phase 3 (6-12 months): Full rollout; address late-stage obstacles"

[SPEAKER NOTES]
"If you want to try this, here's how to approach it. Don't transform everything at once. Pick one team as pilot. This team has executive sponsorship, access to resources, and permission to do things differently. Run for three months. What worked? What needs adjustment? Then expand. Three teams get the new approach. Refine based on learnings. Then full rollout. This approach has higher success rates than big-bang transformations."

---

[SLIDE 15: CALL TO ACTION]
[Visual: Simple, clear CTA]
"Three questions for your organisation:
1. Are you organised around products or functions?
2. Where are decision bottlenecks?
3. What's the cost of slow execution?"

[SPEAKER NOTES]
"Before you leave, I want to leave you with three questions. First: How is your organisation structured? If it's functional, that's likely a constraint. Second: Where do decisions take forever? Name the bottleneck specifically. Third: What does slow execution cost you? Competitive advantage? Revenue? Market share? That number often justifies the cost of transformation.

If any of these questions resonate, I'd like to continue the conversation. I'm available for follow-ups; I have cards. Thank you."

---

[SLIDE 16: CONTACT / NEXT STEPS]
[Visual: Clean layout with contact info]
"Questions? Let's talk.
[Your name]
[Email]
[LinkedIn]

[Optional: offer]
- 30-minute consultation (free)
- Diagnostic assessment (1-day workshop)
- Speaking at your conference"

[SPEAKER NOTES]
"Thank you for the time. I'm happy to answer questions now, or contact me afterward. If you want to explore whether these patterns apply to your organisation, I offer a free 30-minute consultation. If you want to dig deeper, we run diagnostic workshops. And if your conference or company event is interested in having me speak, I'm available."
```

## Examples

### Good Presentation (Key Slides)

**Slide: The Situation**
- Headline: "70% of Digital Transformations Fail"
- Visual: Chart showing failure rate (with source noted small at bottom)
- Implied question: Why? That draws audience forward

**Slide: The Complication**
- Headline: "But Failure Isn't What You Think"
- Visual: Venn diagram showing Strategy, Technology, Organisation—with Organisation highlighted
- Message: Everyone focuses on strategy and technology; organisation is neglected

**Slide: The Resolution**
- Headline: "Three Moves Separate Success from Failure"
- Visual: Three boxes with move titles
- Message: Clear, specific, actionable

**Slide: Evidence**
- Visual: Chart showing outcomes (speed, adoption, retention) with numbers
- Annotation: source cited
- Message: This works; here's proof

---

### Bad Presentation (Key Issues)

**Slide: Dense text**
- 200 words on one slide
- Audience reads instead of listens
- Speaker reads bullet points
- Audience disengages

**Slide: Vague headline**
- "Digital Transformation"
- Doesn't communicate a specific insight
- Audience doesn't know why this slide matters

**Slide: Unsourced data**
- "Most companies think digital is important"
- No source; audience distrusts
- Distracts from main message

## Tactical Guidance

**Slide Design Principles:**

```
✓ One idea per slide
✓ Headline states the idea, not the category
✓ Max 40 words of text per slide
✓ Large, readable font (44pt minimum)
✓ High-quality visuals (don't use blurry photos or generic stock images)
✓ Data visualizations should be self-explanatory (not require speaker explanation)
✓ Consistent formatting (same font, colour scheme across deck)

✗ Dense text (slides are not documents)
✗ Bullet points with complete thoughts (break thoughts into phrases)
✗ Tiny font (audience can't read)
✗ Clip art or generic imagery (looks unprofessional)
✗ Animation effects (distracting)
✗ Multiple topics per slide (forces rushed pacing)
```

**Speaker Notes (What You Say vs. What's on Slide):**

The slide should have minimal text. Your speaker notes have detailed explanation:

```
Slide text: "70% of transformations fail. Why?"
Speaker notes: "I said earlier that most companies want to transform. Yet evidence shows that 70% of major transformation initiatives don't achieve their intended objectives. That's based on multiple studies—McKinsey data, Gartner research, our own observation of client engagements. I want to explore why this happens. It's not because companies lack resources. It's not because they lack executive commitment. Something else is at play, and that's what I want to uncover for you today."

Notice: Speaker notes provide context, evidence, transitions. Slide is clean and simple.
```

**Pacing and Energy:**

- **30-minute talk**: 15-20 slides (roughly 1.5-2 minutes per slide)
- **60-minute talk**: 30-40 slides (roughly 1.5-2 minutes per slide)
- **Variation**: Don't spend equal time on each slide; slow down on complex ideas, move quickly on setup
- **Interaction**: Every 5-7 slides, pause for Q&A or interactive element (poll, activity) to reset attention

**Data Visualization Standards:**

```
Good chart for presentation:
- Title explains finding (not just "Revenue Growth")
- Legend is clear
- Axes are labelled with units
- Source noted (small text at bottom)
- Caption explains implication (not just what it shows)
- Colour is high-contrast (readable from distance)

Bad chart:
- Tiny text (illegible from back of room)
- No title or legend
- Source missing
- Complicated (requires 2 minutes to understand)
- Passive colours (hard to see from distance)
```

**Consulting Pyramid Principle:**

Structure arguments like a pyramid:

```
Situation (broadest, most general)
        ↓
Complication (why it's hard, what's at stake)
        ↓
Resolution (specific solution)
        ↓
Evidence (proof this works)
        ↓
Implication (what you should do)
            ↑
        (narrowest, most specific)

This structure makes arguments both compelling (draws audience in) and memorable (specific insights stick).
```

## Pre-Publication Checklist

- [ ] Deck follows SCR structure (Situation, Complication, Resolution)
- [ ] Each slide has one clear idea; headline states the idea
- [ ] Slide text is max 40 words; rest goes in speaker notes
- [ ] Font is 44pt minimum; readable from back of room
- [ ] All data visualizations have titles, sources, and clear legends
- [ ] All factual claims are sourced (source noted on slide)
- [ ] Visuals are high-quality (no blurry photos or generic stock imagery)
- [ ] Animations are minimal and purposeful (not distracting)
- [ ] Pace is logical (related slides grouped; doesn't jump topics)
- [ ] Tone is conversational (not academic or script-like)
- [ ] Objections/complications are addressed (not dismissive)
- [ ] CTA is clear (what do you want audience to do?)
- [ ] Speaker notes are detailed (100-200 words per slide)
- [ ] Speaker notes include transitions and context (not just elaboration of slide)
- [ ] Opening hook is compelling (not generic welcome)
- [ ] Closing reinforces key takeaway and offers next step
- [ ] Contact info is on final slide
- [ ] GGP markers check: All data claims [VERIFIED] with sources noted. Counterarguments acknowledged [ATTRIBUTION]. Estimates transparent [ESTIMATED] about methodology. Inferences labelled [INFERENCE] as such.
- [ ] Peer-reviewed by colleague for clarity and flow
- [ ] Presentation has been timed; pacing is appropriate for allocated time

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 20. Press Release
> Source: `references/channels/press-release.md`

# Press Release

## Goal

Official, legally-reviewed announcement of significant business developments (launches, funding, partnerships, leadership changes) for external distribution to media, investors, and public stakeholders.

## Success Metrics (2025-2026)

- **Pickup Rate**: 40%+ of targeted outlets mention announcement within 5 days
- **Reach**: 2M+ impressions across online coverage within first week
- **Positive Sentiment**: 90%+ of mentions are neutral or positive (no negative angles)
- **Quote Attribution**: 100% of quotes are attributed to named executives with titles
- **Accuracy**: Zero fact corrections or clarifications required post-publication
- **SEO Impact**: Announcement ranks in top 3 search results for key terms within 30 days

## Limits & Restrictions

### Technical
- **Format**: Plain text or PDF; follows AP style or Reuters format
- **Length**: 400-600 words (tight; every sentence earns space)
- **Headline**: 10-15 words; no jargon; includes key fact
- **Dateline**: City and date (e.g., "SAN FRANCISCO, Feb. 6, 2025")
- **Quotes**: Max 2-3 per release; 1-2 sentences each
- **Contact info**: Media contact with direct phone and email required
- **Boilerplate**: 2-3 sentences about company; goes after main body

### Platform/Algorithm
- **Wire distribution**: Distributed via PR Newswire, Business Wire, etc.; pricing $500-2K+ per release
- **Media targeting**: Journalists scan 50+ releases daily; subject line is make-or-break
- **Embargoes**: "For Immediate Release" or "EMBARGOED UNTIL [DATE/TIME]" in header
- **SEO timing**: Distribution at 9am ET Tuesday-Thursday for maximum pickup
- **Keyword density**: Include searchable terms 2-3x naturally (not spammy)
- **Link strategy**: Maximum 2-3 links to landing pages or resources

### GGP Etiquette
- **Legal review mandatory**: Every factual claim must be verified before distribution
- **Quote accuracy**: Executives must approve exact wording of attributed quotes
- **No false claims**: "Best-in-class," "industry-leading," etc. must be defensible
- **Competitive framing**: Describe own value, not competitor weakness
- **Attribution transparency**: Attribute claims to data sources or third-party research
- **No embellishment**: "Unprecedented" must be accurate; avoid superlatives without backup

## Template

```
[HEADER]
FOR IMMEDIATE RELEASE
[or]
EMBARGOED UNTIL [DATE, TIME ET]

[DATELINE]
[CITY], [STATE] — [Date] —

[HEADLINE]
[Company] Announces [Specific Achievement/Product/Partnership] [Expected Business Impact]

[LEAD PARAGRAPH — Inverted pyramid: what happened, why it matters]
[Company Name], [industry descriptor], announced today [the specific news]. The [initiative/product/partnership] [key benefit: solves X problem, enables Y outcome, or generates Z value]. [Impact metric if available: "Expected to serve X customers" or "Projected X% increase in Y."]

[SUPPORTING DETAILS — 2-3 paragraphs explaining context, features, timing]
"[Quote from executive with title]. [Quote should explain why this matters, not repeat the headline.]"

"[Quote from partner/customer if applicable, adding external validation.]"

[BODY PARAGRAPH]
[Technical details, timeline, or additional context. Address potential journalist questions.]

[COMPANY BOILERPLATE]
About [Company Name]: [2-3 sentences. Founded when, where, what you do, key differentiator. Include website.]

[MEDIA CONTACT]
[Name]
[Title]
[Company Name]
[Email]
[Phone]

###
```

## Examples

### Good Example

**HEADER:**
FOR IMMEDIATE RELEASE

**DATELINE:**
SAN FRANCISCO, Calif. — Feb. 6, 2025 —

**HEADLINE:**
Acme Consulting Acquires DesignCo, Expanding UX Services to Fortune 500 Clients

**LEAD:**
Acme Consulting, a management consulting firm specializing in digital transformation, announced today the acquisition of DesignCo, a product design agency. The acquisition expands Acme's UX capabilities and enables the combined firm to serve Fortune 500 clients across strategy and execution. The deal is expected to close by Q2 2025.

**BODY 1:**
DesignCo has worked with 40+ enterprise clients and maintains a 95% customer retention rate. The firm's 85-person team will join Acme's San Francisco office, bringing together 150+ designers and strategists. Financial terms were not disclosed.

**QUOTE 1:**
"Design and strategy are inseparable," said Jane Smith, CEO of Acme Consulting. "This acquisition lets us deliver integrated solutions from day one, rather than partnering externally. Clients get better outcomes and faster delivery."

**QUOTE 2:**
"Joining Acme means we can tackle enterprise-scale challenges," said Marcus Lee, Founder of DesignCo. "Our team was excited by the opportunity to work on larger transformation initiatives with Acme's expertise in change management."

**BODY 2:**
The combined firm will maintain offices in San Francisco, New York, and Los Angeles. Acme expects the acquisition to be accretive to earnings by the end of 2025. Integration planning is underway and will be led by Chief Operating Officer [Name].

**BOILERPLATE:**
About Acme Consulting: Founded in 2010, Acme Consulting advises Fortune 500 companies on digital transformation strategy and implementation. The firm has 600+ employees across six offices and has led transformation efforts for 200+ clients. Learn more at acmeconsulting.com.

**CONTACT:**
Sarah Johnson
VP of Marketing
Acme Consulting
sjohnson@acmeconsulting.com
415-555-0123

###

---

**Why this works:**
- Headline has specific fact (acquisition) and business impact (expands services)
- Lead answers: what (acquisition), why (expands capabilities), when (Q2 2025)
- Quotes explain strategic value, not repeat headline
- All factual claims are specific: "95% retention," "85-person team," "40+ clients"
- CEO quote addresses competitive positioning (design + strategy advantage)
- Timeline and financial transparency (deal terms not disclosed, but integration timeline stated)
- Contact info is complete and current

### Bad Example

**HEADER:**
FOR IMMEDIATE RELEASE

**HEADLINE:**
Revolutionary Game-Changing Acquisition Announced!

**LEAD:**
Big news! Acme Consulting is super excited to announce a mega-acquisition of an amazing design company. This is literally the best partnership ever and will totally change the industry. Everyone thinks design is important and we're really good at it.

**QUOTE:**
"We're thrilled and honoured to be joining the Acme family," said the other guy. "This is such an incredible opportunity."

**BOILERPLATE:**
Acme Consulting is an industry leader and does consulting and stuff. We're really great.

---

**Why this fails:**
- Headline uses hype words without specific facts ("Revolutionary," "game-changing")
- Lead doesn't answer key questions: company name, deal value, timing, specific impact
- No data to back claims ("mega-acquisition," "best partnership ever")
- Quotes are vague and non-attributable ("the other guy")
- No name, title, or business rationale in second quote
- Boilerplate is unprofessional and lacks specifics
- No contact information
- No financial or timeline details
- Tone is unprofessional and will be rejected by media outlets

## Tactical Guidance

**Timing Strategy:**

- **Morning release**: 9am ET Tuesday-Thursday maximises pickup before end-of-day media cycles
- **Avoid Friday**: Friday releases miss Monday media coverage; save for less competitive news
- **Embargo timing**: If using embargo, set for 7am ET on announced release day (gives press 2 hours before trading opens)
- **Public trading**: Coordinate with investor relations; file 8-K or equivalent before release if material

**Quote Strategy:**

- **Executive quotes**: CEO or business unit lead; should address why this matters strategically
- **Partner/customer quotes**: Third-party validation; should address customer impact, not echo your marketing
- **Multiple quotes**: 2-3 max; don't make release quote-heavy
- **Quote approval**: Executive and any external quoted party must approve exact wording in advance

**Claim Verification:**

Every factual statement must have a source:

```
Claim: "85-person team"
Source: DesignCo headcount verification or contract agreement

Claim: "95% customer retention"
Source: DesignCo client audit or third-party review data

Claim: "40+ enterprise clients"
Source: Customer list or case study inventory

Claim: "Expected to close by Q2 2025"
Source: Deal legal documents or board approval minutes
```

**Distribution Strategy:**

- **Wire services**: PR Newswire or Business Wire for broad distribution ($500-2K)
- **Direct media**: Email to relevant journalists 15 min after wire distribution (embargo-honoured list)
- **Internal channels**: Internal announcement same day or day before (prevent leaks)
- **Social media**: Tweet/LinkedIn post day of release with link to full release

**Headline Formula:**

```
[Company Name] Announces [Specific Action: Acquires, Launches, Raises, Partners with]
[Specific Entity/Product], [Expected Impact: Expands, Enables, Brings]
[Business Outcome: New Market, Customer Segment, Capability]

Examples:
✓ Acme Consulting Acquires DesignCo, Expanding UX Services to Fortune 500
✓ Acme Raises $50M Series B to Expand European Operations
✓ Acme Launches AI-Powered Transformation Platform for Manufacturing
```

## Pre-Publication Checklist

- [ ] All factual claims verified against source documents (contracts, board minutes, customer lists)
- [ ] Legal review completed and approved
- [ ] CEO/executive quoted has approved exact quote wording
- [ ] All external quotes (partners, customers) have approved exact text
- [ ] Headline is specific, includes key fact, and avoids hype words
- [ ] Lead paragraph answers: What? Why? When?
- [ ] No unsourced claims ("industry-leading," "best-in-class" without defensible data)
- [ ] Contact info is complete: name, title, email, direct phone
- [ ] Boilerplate is current and includes website
- [ ] Dateline is correct city and date
- [ ] Distribution timing confirmed: 9am ET Tues-Thurs for maximum pickup
- [ ] Embargo status clear: "FOR IMMEDIATE RELEASE" or "EMBARGOED UNTIL [DATE, TIME ET]"
- [ ] Investor relations coordinated (if material announcement affecting stock price)
- [ ] PR wire service selected and account ready for distribution
- [ ] GGP markers check: Every factual claim sourced [VERIFIED]. Competitive claims framed as company value, not competitor criticism. All metrics transparent about source.
- [ ] Final tone check: Professional, news-appropriate, no marketing jargon or hype

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 21. Skills Docs
> Source: `references/channels/skills-docs.md`

# Skills & Plugin Documentation

## Goal
Enable developers to integrate, extend, and trust skills/plugins through complete specification, parameter documentation, and tested examples so they can use tools without reverse-engineering behaviour.

## Success Metrics (2025-2026)
- All skill parameters tested and documented (0% "I assume this works")
- 100% of tools execute as documented (no hidden parameter requirements)
- Integration time reduced to <30 minutes from documentation alone
- Breaking changes announced 2+ weeks in advance
- Plugin version compatibility matrix maintained and accurate

## Limits & Restrictions

### Technical
- All tool parameters must be tested against actual implementation
- Return values must match documented schema (tested with real invocations)
- Error handling documented with specific error codes and recovery steps
- Version compatibility matrix maintained and tested (which versions work together?)
- All examples must be executable; tested before publication

### Platform/Context
- Lives in plugin registries, API documentation, GitHub wikis, internal skill catalogs
- Primary audience: developers integrating plugins, platform engineers, community contributors
- Lifecycle: Updated with each release; breaking changes versioned prominently
- Discoverability: Indexed by tool name, capability, framework compatibility

### GGP Etiquette
- Never fabricate tool capabilities — only document what actually exists
- All parameter descriptions tested; don't guess behaviour based on code inspection
- Mark version-specific features as [INFERENCE] pending verification
- Flag breaking changes prominently; document migration path
- Deprecated tools marked with removal timeline (minimum 2 versions ahead)

## Template

```markdown
# Skill: [Name]

## Overview
[What does this skill do? What problems does it solve?]

## Quick Start
\`\`\`javascript
// Complete, executable example
const skill = require('@org/skill-name');
const result = await skill.doSomething({ param: 'value' });
console.log(result); // Expected output
\`\`\`

## Specification

**Version:** [Semantic version]
**Status:** [Active, Deprecated (removal: vX.Y.Z), Beta]
**Framework Compatibility:** [Framework version range]
**Dependencies:** [Required dependencies with versions]

## Tools

### \`toolName(options)\`
**Description:** [What does this tool do?]

**Parameters:**
| Name | Type | Required | Default | Description | Example |
|---|---|---|---|---|---|
| param1 | string | Yes | — | Description | "value" |
| param2 | number | No | 100 | Description | 50 |

**Returns:**
\`\`\`javascript
{
  success: boolean,
  data: {
    field1: string,
    field2: number
  },
  error?: string
}
\`\`\`

**Throws:**
- \`ErrorCode\` (HTTP 400): [Condition]. Recovery: [Step]
- \`ErrorCode\` (HTTP 500): [Condition]. Recovery: [Step]

**Example:**
\`\`\`javascript
const result = await skill.toolName({ param1: 'test' });
// Returns: { success: true, data: { field1: 'result' } }
\`\`\`

## Version Compatibility

| Skill Version | Node Version | Framework | Status |
|---|---|---|---|
| v2.x.x | 18.12.0+ | Framework 3.x | ✓ Active |
| v1.x.x | 16.13.0+ | Framework 2.x | [RISK] Deprecated (removal: v3.0.0) |

## Integration Guide

### Prerequisites
- [Framework version]
- [Dependency version]

### Installation
\`\`\`bash
npm install @org/skill-name@2.0.0
\`\`\`

### Setup
\`\`\`javascript
// Initialization code
\`\`\`

## Changelog
- v2.0.0: [Breaking changes]
- v1.0.0: Initial release
```

## Examples

### Good Example
```markdown
# Skill: Stripe Payment Processor

## Overview
Process credit card payments through Stripe. Handles authorization, capture, refunds, and error recovery. Used in checkout, subscriptions, and billing systems.

## Quick Start
\`\`\`javascript
const stripe = require('@company/stripe-processor');
const payment = await stripe.charge({
  amount: 1050, // USD cents
  currency: 'USD',
  cardToken: 'tok_visa'
});
console.log(payment);
// { success: true, transactionId: 'txn_1A2B3C', status: 'succeeded' }
\`\`\`

## Specification

**Version:** 2.3.1
**Status:** Active (Production)
**Framework Compatibility:** Node 18.12.0+, Express 4.18+
**Dependencies:**
- stripe@13.10.0
- dotenv@16.3.1

## Tools

### \`charge(options)\`
**Description:** Process a one-time credit card charge. Validates card, handles Stripe communication, retries on transient failures.

**Parameters:**
| Name | Type | Required | Default | Description | Example |
|---|---|---|---|---|---|
| amount | number | Yes | — | USD cents (e.g., 1050 = $10.50). Min: 50 | 1050 |
| currency | string | No | "USD" | ISO 4217 code | "USD" |
| cardToken | string | Yes | — | Stripe token from Stripe.js | "tok_visa" |
| metadata | object | No | {} | Custom data (max 50 keys) | { orderId: "123" } |
| idempotencyKey | string | No | — | Unique key for retry safety (max 255 chars) | "order_123_attempt_1" |

**Returns:**
\`\`\`javascript
{
  success: boolean,
  transactionId: string,      // Stripe transaction ID
  status: 'succeeded' | 'requires_action' | 'declined',
  error?: {
    code: string,             // Stripe error code
    message: string,          // Human-readable message
    recoveryAction?: string   // Suggested next step
  }
}
\`\`\`

**Throws:**
- \`InvalidAmountError\` (HTTP 400): amount < 50 cents or > $99,999. Recovery: Validate amount before retry.
- \`InvalidTokenError\` (HTTP 400): card token expired or revoked. Recovery: Request new token from user.
- \`AuthenticationError\` (HTTP 401): Stripe API key invalid/missing. Recovery: Check env var STRIPE_API_KEY.
- \`RateLimitError\` (HTTP 429): Too many requests. Recovery: Exponential backoff (wait 1s, then 2s, then 4s).
- \`StripeServiceError\` (HTTP 500): Stripe API unreachable. Recovery: Retry up to 3 times; if still failing, queue for async processing.

**Examples:**

Success case:
\`\`\`javascript
const result = await stripe.charge({
  amount: 1050,
  currency: 'USD',
  cardToken: 'tok_visa',
  idempotencyKey: 'order_12345_v1'
});
// Returns:
// {
//   success: true,
//   transactionId: 'txn_1A2B3C4D5E6F7G8H',
//   status: 'succeeded'
// }
\`\`\`

Failure case (expired card):
\`\`\`javascript
const result = await stripe.charge({
  amount: 1050,
  cardToken: 'tok_chargeDeclined'
});
// Returns:
// {
//   success: false,
//   transactionId: null,
//   status: 'declined',
//   error: {
//     code: 'card_declined',
//     message: 'Your card was declined',
//     recoveryAction: 'Ask user for different payment method'
//   }
// }
\`\`\`

---

### \`refund(transactionId, options)\`
**Description:** Refund a previous charge (full or partial).

**Parameters:**
| Name | Type | Required | Default | Description | Example |
|---|---|---|---|---|---|
| transactionId | string | Yes | — | Stripe transaction ID from charge() | "txn_1A2B3C" |
| amount | number | No | — | USD cents to refund; if omitted, full refund | 500 |
| reason | string | No | — | Reason: 'duplicate', 'fraudulent', 'requested_by_customer' | "requested_by_customer" |

**Returns:**
\`\`\`javascript
{
  success: boolean,
  refundId: string,
  originalTransactionId: string,
  amountRefunded: number,  // USD cents
  status: 'succeeded' | 'pending',
  error?: string
}
\`\`\`

**Example:**
\`\`\`javascript
const refund = await stripe.refund('txn_1A2B3C', {
  amount: 500,  // Partial refund: $5.00
  reason: 'requested_by_customer'
});
// Returns:
// {
//   success: true,
//   refundId: 're_1X2Y3Z',
//   originalTransactionId: 'txn_1A2B3C',
//   amountRefunded: 500,
//   status: 'succeeded'
// }
\`\`\`

## Version Compatibility

| Skill Version | Node | Express | Stripe API | Status | Notes |
|---|---|---|---|---|---|
| v2.3.x | 18.12.0+ | 4.18+ | 2024-01 | ✓ Active | Latest; recommended |
| v2.2.x | 18.0.0+ | 4.18+ | 2023-11 | ✓ Supported | Security patches only |
| v2.0.x | 16.13.0+ | 4.16+ | 2023-08 | [RISK] Deprecated | Removal: v3.0.0 (2025-06-01) |
| v1.x.x | 14.0.0+ | 4.0+ | 2022-11 | [RISK] End of Life | No longer supported |

## Integration Guide

### Prerequisites
- Node 18.12.0 or later
- Express 4.18+ (if using middleware mode)
- Valid Stripe account with API key
- Environment variable: STRIPE_API_KEY

### Installation
\`\`\`bash
npm install @company/stripe-processor@2.3.1
\`\`\`

### Configuration
\`\`\`javascript
// .env
STRIPE_API_KEY=sk_live_51A2B3C4D5E6F7G8H9I0J

// index.js
const stripe = require('@company/stripe-processor');

stripe.init({
  apiKey: process.env.STRIPE_API_KEY,
  retryAttempts: 3,
  retryDelayMs: 1000,
  timeout: 30000
});
\`\`\`

### Testing
\`\`\`bash
# Run integration tests against Stripe sandbox
npm test -- --env=sandbox

# Test results should show:
# ✓ charge() succeeds with valid token
# ✓ charge() returns declined with invalid token
# ✓ refund() succeeds with valid transaction
# ✓ refund() fails with nonexistent transaction
\`\`\`

## Breaking Changes & Deprecations

### Deprecated in v2.2.0 (Removal: v3.0.0, ETA: 2025-06-01)
- \`chargeWithEmail()\` — Use \`charge({}, { customerEmail: 'x@y.com' })\` instead
- Direct Stripe API access via \`stripe.api\` — Removed; use documented methods only

**Migration Path:**
\`\`\`javascript
// Old (v2.1 and earlier)
const result = await stripe.chargeWithEmail({
  amount: 1050,
  email: 'customer@example.com'
});

// New (v2.2+)
const result = await stripe.charge({
  amount: 1050,
  metadata: { customerEmail: 'customer@example.com' }
});
\`\`\`

## Changelog

**v2.3.1 (2025-01-10)**
- Fix: Exponential backoff now uses correct jitter (was causing rate limit retries to pile up)
- Improvement: Idempotency key validation more permissive (allows underscores, hyphens)
- Security: Stripe API key no longer logged in debug mode

**v2.3.0 (2024-12-01)**
- Feature: Added \`refund()\` tool
- Improvement: Timeout reduced from 40s to 30s (most Stripe requests < 2s)
- Change: Error responses now include \`recoveryAction\` field

**v2.2.0 (2024-09-01)**
- Deprecation: \`chargeWithEmail()\` (removal: v3.0.0)
- Improvement: Retry logic with exponential backoff for transient failures
- Fix: Metadata handling now supports all Stripe-compatible types

**v2.0.0 (2024-01-15)** — BREAKING CHANGE
- Change: Return format restructured (was flat, now nested \`{success, data, error}\`)
- Change: Parameter name \`cardNumber\` → \`cardToken\` (requires Stripe.js)
- Migration: See Breaking Changes section above

**v1.0.0 (2023-06-01)**
- Initial release; basic charge functionality

## Troubleshooting

| Issue | Cause | Solution |
|---|---|---|
| "STRIPE_API_KEY not found" | Missing env var | Set STRIPE_API_KEY in .env |
| "rate_limit_error" after 1 request | Incorrect backoff logic | Upgrade to v2.3.1+ |
| "card_declined" on valid card | Test key used in production | Switch to live Stripe keys |
| "Invalid idempotency key" | Key contains invalid characters | Use alphanumeric, underscore, hyphen only |

## Support
- Issues: GitHub Issues or Slack #stripe-support
- API Reference: https://stripe.com/docs/api (Stripe official docs)
- Skill Questions: @stripe-integration-team
```

**Why this works:** Parameters tested and documented. Error codes specific and recoverable. Version compatibility matrix clear. Breaking changes announced and migration path provided. Examples are executable (tested). Deprecated methods flagged with removal date.

---

### Bad Example
```markdown
# Stripe Skill

Processes payments using Stripe.

\`\`\`javascript
const charge = stripe.charge(amount);
\`\`\`

Parameters: amount (the amount)

Returns: something successful or an error

Errors: might fail sometimes

Versions: Works with most Node versions

Deprecated: Some old methods might not work
```

**Why this fails:** Parameters ambiguous (amount in cents? dollars?). No error codes. No recovery steps. Return schema unclear. Version compatibility vague. No migration path. Examples not executable.

## Tactical Guidance

**Parameter Documentation:**
- Test every parameter combination before publishing
- Specify units (USD cents, not dollars; milliseconds, not seconds)
- Document constraints (min/max, valid values, length limits)
- Include data type explicitly (string, number, boolean, object)

**Error Handling:**
- List all possible error codes (don't say "might fail")
- Document trigger condition (when does this error occur?)
- Include recovery action (what should the caller do?)
- Test error path as thoroughly as success path

**Version Compatibility:**
- Maintain compatibility matrix (which versions work together?)
- Test upgrade path (can users safely upgrade from v1 → v2?)
- Document minimum versions (don't say "should work with")
- Pin versions in examples; don't use ^ or ~

**Breaking Changes:**
- Announce 2+ weeks before deployment
- Provide migration path (old code → new code)
- Support dual-write period (both APIs work simultaneously)
- Document removal timeline (minimum 2 major versions ahead)

**Examples:**
- Extract from test suite; must be copy-paste executable
- Include both success and failure paths
- Show error handling (try/catch, null checks)
- Verify output matches actual returns

## Pre-Publication Checklist
- [ ] All tool parameters tested against actual implementation
- [ ] Parameter descriptions complete: type, units, constraints, defaults
- [ ] Return values match documented schema (tested with real invocations)
- [ ] All error codes documented with trigger conditions and recovery steps
- [ ] Examples are executable and match actual behaviour
- [ ] Version compatibility matrix accurate and tested
- [ ] Breaking changes announced with migration path and removal date
- [ ] Deprecated features marked with removal timeline
- [ ] Integration guide tested by new developer (should take <30 min)
- [ ] Dependency versions pinned to exact patch (18.12.0, not ^18)
- [ ] Framework compatibility tested in actual environment
- [ ] Changelog updated for every version
- [ ] GGP markers applied: [CONFIRMED] for verified behaviour, [INFERENCE] for version-specific features, [GAP] for gaps

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 22. Twitter
> Source: `references/channels/twitter.md`

# Twitter/X

## Goal

Real-time, concise commentary on industry trends and research announcements that establishes subject-matter expertise and invites substantive engagement from niche specialist audiences.

## Success Metrics (2025-2026)

- **Engagement Rate**: 1-3% (retweets, likes, replies) within 24 hours
- **Specialist Interaction**: 30%+ of replies from tier-1 experts in your field
- **Retweet Ratio**: 3:1 retweets-to-likes (signal of credibility, not ego metrics)
- **Thread Performance**: Threads average 2x engagement of single tweets
- **DM Mentions**: 1-2 substantive DM inquiries per month from tweets

## Limits & Restrictions

### Technical
- **Character limit**: 280 characters
- **Threads**: Unlimited length when chained; use 3-5 tweets maximum per thread
- **Links**: Shorten with t.co (automatic); count as 23 characters
- **Images**: 4 images max per tweet; 16:9 or 1:1 aspect ratio
- **Video**: Up to 2 minutes 20 seconds; auto-plays with sound off (add captions)
- **Hashtags**: 1-3 per tweet (Twitter rewards hashtag restraint vs. LinkedIn)

### Platform/Algorithm
- **Recency**: Twitter is live; old tweets die fast (peak engagement 0-4 hours)
- **Conversation metric**: Replies > retweets > likes in algorithm priority
- **Thread engagement**: Reads decay by tweet in thread; put key insight in first tweet
- **Quote tweets**: Retweets with commentary outperform silent retweets
- **Timing**: Peak US activity 8-10am ET and 5-6pm ET, Monday-Thursday
- **Search visibility**: Hashtags and @mentions improve discoverability
- **Trending hijack**: Avoid riding trending topics if unrelated (appears opportunistic)

### GGP Etiquette
- **No hot takes about clients**: Ever. Use general examples only
- **No competitor attacks**: Criticism about alternatives is fine; attacks on founders is not
- **No unverified claims**: Twitter rewards speed, but GGP requires source before publishing
- **Attribution required**: If quoting research, link the paper or mention the researcher
- **Disagreement protocol**: Challenges to your view should be replied to, not blocked
- **No astroturfing**: Don't create fake accounts to amplify your tweets
- **Correction norm**: If corrected, acknowledge and fix; pin correction to thread if needed

## Template

```
[PRIMARY OBSERVATION or INSIGHT — one clear point]
The biggest productivity killer isn't distraction. It's unclear decision rights.

[SUPPORTING EVIDENCE or PATTERN]
Teams spend 40% of meeting time asking "who decides this?" not debating the actual decision.

[IMPLICATION or REFRAME]
Fix decision rights before you fix your process.

#OrganizationDesign
```

### Thread Example

```
Tweet 1:
[HOOK — something counterintuitive]
Most "digital transformation" projects fail because they start with technology instead of capability. Here's what I've learned from 30+ transformations.

Tweet 2:
[PATTERN]
The highest-success companies start with: What new customer capabilities do we want? Then: What organisation design supports that? Finally: What tech enables it?

Tweet 3:
[EVIDENCE or EXAMPLE]
Company A prioritised tech first: spent $40M, adoption 12%, ROI near zero.
Company B prioritised capability: spent $8M, adoption 78%, 3x revenue lift.

Same industry. Different starting point.

Tweet 4:
[ACTIONABLE TAKEAWAY]
If you're planning a transformation, ask first: What new work will people do? Then design around that.

#DigitalTransformation #OrganizationDesign
```

## Examples

### Good Example

**Tweet:**

"We reviewed 50 post-mortem reports from failed implementations. One variable predicted success better than budget or timeline: the sponsor spent 20+ hours in the work, not just the approval.

Sponsorship isn't a title. It's presence."

---

**Why this works:**
- Specific data point (50 reports, one variable)
- Counterintuitive insight (sponsor presence > budget)
- Actionable (not abstract)
- Invitation to reflection, not engagement bait
- Relevant to operations/leadership professionals
- Verifiable (author could cite which reports, with permission)

### Bad Example

**Tweet:**

"SHOCKING: Nobody knows how to run projects anymore!! 🚨🚨 Companies are losing MILLIONS because leaders don't care. This is why everyone's digital transformation fails. Like and retweet if you agree. DM me for my FREE guide 😱"

---

**Why this fails:**
- Hyperbolic claim without evidence ("SHOCKING," "MILLIONS")
- Engagement bait ("like and retweet if you agree")
- Cynical tone ("leaders don't care") without grounding
- Obvious selling (free guide that's probably a lead-gen trap)
- No substance; outrage instead of insight
- Multiple emojis signal low credibility
- Sweeping generalisation without nuance

## Tactical Guidance

**Tweet Frequency:**
- 3-5 tweets per week maximum (quality matters far more than frequency)
- Batch tweeting reduces visibility; space out across days
- Avoid tweeting outside your expertise area (dilutes authority)

**Thread Strategy:**
- Threads should have ONE thesis; break into 3-5 logical parts
- Each tweet should stand alone if read separately
- Put action item or key insight in final tweet (maximize value for readers)
- Use Twitter's thread feature (don't manually number "1/5, 2/5")

**Claim Verification:**
- If citing data, link to source immediately (first tweet for threads)
- If paraphrasing research, mention researcher/institution name
- If generalising from experience, frame as "In my work..." or "I've observed..."
- Use [VERIFIED], [ESTIMATED], [INFERENCE] tags if helpful for clarity

**Character Economy:**
```
≈ 240 usable characters after accounting for links and metrics

Tight examples:
"Process doesn't fix culture. Culture fixes process." [4 words, 46 chars]

Bad example:
"Have you ever thought about how amazing culture is and how it impacts literally everything in your organisation because it determines behaviour?" [138 chars of waffle]
```

**Engagement Protocol:**
- Reply to substantive replies within 24 hours (even brief acknowledgment)
- Quote-tweet if disagreement is substantive and worth educating followers
- Ignore hot-take opportunism and pile-ons (don't feed drama)
- Thank people who share your threads (builds community goodwill)

## Pre-Publication Checklist

- [ ] Single clear idea per tweet (or per first tweet if thread)
- [ ] No character filler; every word earns its space
- [ ] All numerical claims have source linked or attributed
- [ ] Claim distinguishes between [VERIFIED] observation and [INFERENCE] or [ESTIMATED]
- [ ] No client names or confidential client data (use general examples)
- [ ] No attacks on competitors, vendors, or individuals (criticism of ideas is fine)
- [ ] No engagement bait ("like if you agree," "retweet this")
- [ ] Tone is confident but not arrogant; opens dialogue, doesn't close it
- [ ] Links are verified and working (especially research links)
- [ ] Ready to engage: Willing to reply to substantive challenges within 24 hours
- [ ] GGP markers check: Is every factual claim sourced? Are estimates transparent?
- [ ] Thread (if applicable) has clear thesis and logical flow across tweets

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---

# 23. Version Control
> Source: `references/channels/version-control.md`

# Version Control & Release Documentation

## Goal
Enable users to understand changes, upgrade safely, and troubleshoot regressions through transparent changelogs, clear release notes, and tested migration paths so they can make informed decisions about adopting updates.

## Success Metrics (2025-2026)
- 100% of releases documented within 24 hours of deployment
- All breaking changes announced 2+ weeks in advance with migration paths
- Deprecation timeline always 2+ major versions ahead (e.g., deprecated in v1, removed in v3)
- Migration steps tested and reproducible (not theoretical)
- Zero "unexpected breaking change" support tickets due to clear release notes

## Limits & Restrictions

### Technical
- Changelog versions must match actual git tags (no fabricated versions)
- All migration steps tested in staging environment before publication
- Deprecation timelines must be realistic and honoured (don't move deadlines without notice)
- Semantic versioning applied consistently (MAJOR.MINOR.PATCH)
- Breaking changes explicitly flagged and linked to migration guide

### Platform/Context
- Lives in CHANGELOG.md, GitHub Releases, release notes wiki, upgrade guides
- Primary audience: users considering upgrades, operations teams, developers troubleshooting
- Lifecycle: Updated with each release; deprecations honoured as published
- Discoverability: Linked from docs homepage, displayed in CLI upgrade checks

### GGP Etiquette
- Never hide breaking changes in "improvements" section
- Never move deprecation deadlines without notice (breaks trust)
- Mark all assumed compatibility as [INFERENCE] (e.g., "should upgrade safely")
- Provide exact before-after code for migrations
- Document security fixes prominently; flag CVE numbers

## Template

```markdown
# Changelog

All notable changes to this project documented here.

## [Unreleased]

### Added
- [New feature]

### Changed
- [Changed feature]

### Deprecated
- [Deprecated feature] — Removal: v[X].[Y].[Z]

### Removed
- [Removed feature]

### Fixed
- [Bug fix]

### Security
- [Security patch]

---

## [Version] — YYYY-MM-DD

### Added
- [New feature] ([#PR](link))

### Changed
- [Changed feature] ([#PR](link))

### Deprecated
- [Deprecated feature] — Removal: v[X].[Y].[Z]
- **Migration Path:** [Link to upgrade guide]

### Removed
- [RISK] **BREAKING:** [Removed feature] ([#PR](link))
- **Migration:** [Before code] → [After code]
- **Impact:** [What breaks, why, severity]

### Fixed
- [Bug fix] ([#Issue](link), [#PR](link))

### Security
- CVE-YYYY-XXXXX: [Vulnerability description, severity, patch]
```

## Examples

### Good Example
```markdown
# Changelog

All notable changes documented here. Follows Semantic Versioning.

## [Unreleased]

### Added
- Support for OAuth2 PKCE flow (improved security for SPAs)
- New \`--dry-run\` flag for safer deployments

### Deprecated
- Direct environment variable authentication — Use \`--auth-file\` instead. Removal: v3.0.0

---

## [2.3.1] — 2025-01-15

### Fixed
- Database connection timeout on slow networks (fixes #2847)
- Incorrect error message for invalid config files

### Security
- CVE-2025-1234: Insufficient input validation in config parser (CVSS 6.5, Medium)
  - Impact: Possible code injection in config files from untrusted sources
  - Fix: All config values now escaped; no code execution allowed
  - Upgrade: Recommended for all users handling untrusted configs

---

## [2.3.0] — 2024-12-01

### Added
- \`--format json\` flag for CLI output (machine-readable)
- Streaming API support (new \`stream()\` method)
- Health check endpoint at \`/health\`

### Changed
- Default retry policy increased from 2 → 3 attempts (better reliability on flaky networks)
- Response timeout increased from 5s → 30s (accommodates slower backends)
- Error messages now include suggestion for recovery

### Deprecated
- Environment variable \`CONFIG_OLD_STYLE\` — Use \`--config-file\` instead. Removal: v3.0.0
  - **Why:** Environment variables hard to audit in production; config files more secure
  - **Migration:** \`export CONFIG_OLD_STYLE=value\` → \`echo "key: value" > config.yaml && app --config-file config.yaml\`
  - **Timeline:** Feb 1, 2025 (v2.4), May 1, 2025 (v2.5), Aug 1, 2025 (v3.0 removal)

### Removed
- [RISK] **BREAKING: Removed support for Python 2.7** ([#2650](https://github.com/org/repo/pull/2650))
  - **Impact:** v2.3.0 requires Python 3.8+; existing Python 2.7 environments will not run
  - **Why:** Python 2.7 end-of-life (Jan 2020); security support ended; codebase maintenance burden
  - **Migration Steps:**
    1. Check current Python version: \`python --version\`
    2. If Python 2.7: Install Python 3.8+: \`brew install python@3.8\` (macOS) or apt install python3.8 (Linux)
    3. Verify new version: \`python3.8 --version\`
    4. Upgrade package: \`pip3.8 install app==2.3.0\`
    5. Test: \`python3.8 -m app --version\` should show v2.3.0
  - **Fallback:** Stay on v2.2.x if Python 2.7 support required (security patches available until 2025-06-01)

- [RISK] **BREAKING: Removed legacy API endpoint \`/api/v1/data\`** ([#2651](https://github.com/org/repo/pull/2651))
  - **Impact:** Clients calling \`GET /api/v1/data\` will receive HTTP 404; use \`/api/v2/query\` instead
  - **Why:** v1 API performance poor, hard to maintain; v2 API available since v1.5.0 (12+ months)
  - **Migration Steps:**
    1. Update client code: \`GET /api/v1/data?id=123\` → \`GET /api/v2/query?id=123\`
    2. Update response parsing: Response format unchanged (\`{ data: [...] }\`)
    3. Test in staging: Verify response parsing still works
    4. Deploy: \`npm install app@2.3.0\` and restart
  - **Verification:** \`curl https://api.example.com/api/v2/query?id=123\` should return data

### Fixed
- Memory leak in connection pooling (resolves out-of-memory crashes after 48h uptime)
- Incorrect timestamp formatting in logs (was showing UTC+0 only; now respects TZ env var)
- Config file parsing failed on Windows paths (backslashes now handled correctly)

### Performance
- Query latency reduced 35% through caching optimisation ([#2640](https://github.com/org/repo/pull/2640))
- Startup time reduced from 4.2s → 2.1s (faster cold starts)

---

## [2.2.0] — 2024-09-01

### Added
- Streaming output mode (\`--stream\` flag)
- Configuration file support (\`--config-file\` flag)
- Health check endpoint

### Changed
- Improved error messages with recovery suggestions
- Retry logic now uses exponential backoff (was linear)

### Deprecated
- \`--verbose\` flag — Use \`--log-level debug\` instead. Removal: v3.0.0
  - Migration: \`app --verbose\` → \`app --log-level debug\`

### Fixed
- Crash when input file missing (now returns helpful error message)

---

## [2.0.0] — 2024-01-15 — MAJOR VERSION RELEASE

### [RISK] BREAKING CHANGES

All breaking changes require code updates. This is a major version; extensive testing recommended.

1. **API Response Format Changed**
   - Old: \`{ status: 'ok', result: {...} }\`
   - New: \`{ success: true, data: {...}, error: null }\`
   - Reason: Consistency with REST conventions; easier error handling
   - Migration: [Update response parsing in 3 steps](https://docs.example.com/upgrade-v1-to-v2)

2. **Database Schema Updated**
   - Old table \`users\` → New schema: \`users_v2\`
   - Migration required: [Automatic migration script available](https://docs.example.com/migrate-schema)
   - Downtime: ~30 minutes (expect service unavailable during migration)
   - Rollback: Script provided to revert schema if needed

3. **Authentication Changed from Basic Auth → OAuth2**
   - Old: \`Authorization: Basic base64(user:pass)\`
   - New: \`Authorization: Bearer <access_token>\`
   - Migration: [OAuth2 setup guide](https://docs.example.com/oauth2-setup)

### Added
- OAuth2 authentication support
- Streaming API endpoints
- GraphQL support (experimental)

### Removed
- Basic authentication (HTTP 401 if attempted)
- \`/api/v1\` endpoints (use \`/api/v2\` instead)
- Python 2 support

### Fixed
- Critical memory leak in long-running processes
- Database connection handling under high load

### Migration Guide
[Detailed step-by-step upgrade guide available →](https://docs.example.com/upgrade-v1-to-v2)

Estimated time: 1-2 hours for typical deployment (includes testing, backups, rollback verification)

---

## [1.5.0] — 2023-09-01

### Added
- New endpoint: \`GET /api/v2/query\` (faster, recommended for new integrations)

### Deprecated
- \`GET /api/v1/data\` — Use \`/api/v2/query\` instead. Removal: v2.0.0

---

## [1.0.0] — 2023-01-15

Initial release.
```

**Why this works:** Breaking changes flagged with [RISK]. Migration steps are exact (code before/after). Deprecation timeline clear (2+ versions ahead). Security issues have CVE numbers and severity. Removal reasons explained. Estimated migration time provided.

---

### Bad Example
```markdown
# Changes

v2.3 - Improvements and fixes
v2.2 - Updates
v2.1 - Some breaking changes
v2.0 - Major update, many changes

See GitHub for details.
```

**Why this fails:** No dates. No details of breaking changes. No migration paths. No security information. Can't tell what changed or how to upgrade.

## Tactical Guidance

**Semantic Versioning:**
- MAJOR: Breaking changes (increment on incompatible API changes)
- MINOR: New features, backward compatible
- PATCH: Bug fixes, backward compatible
- Pre-release: Use tags like v1.0.0-beta.1, v2.0.0-rc.1

**Changelog Sections:**
- **Added:** New features (backward compatible)
- **Changed:** Existing feature improvements (backward compatible)
- **Deprecated:** Features marked for removal (keep working, timeline announced)
- **Removed:** Features deleted ([RISK] breaking change, migration required)
- **Fixed:** Bug fixes
- **Security:** CVEs, security patches

**Breaking Changes:**
- Flag with [RISK] **BREAKING**
- Include: what changed, why, impact, migration steps
- Provide code examples: before code → after code
- Announce 2+ major versions in advance via deprecation warning
- Honour announced deadlines (breaking trust is costly)

**Deprecation Warnings:**
- Announce in deprecation note (Removal: v[X.Y.Z])
- Provide migration path (old code → new code)
- Print runtime warning when deprecated feature used
- Keep working for ≥2 versions before removal

**Release Notes:**
- Date and version number at top
- Highlights section for major changes
- Full changelog grouped by type (Added, Changed, etc.)
- Known issues section
- Upgrade instructions (if non-trivial)

**Migration Guides:**
- Step-by-step numbered instructions
- Code examples showing before/after
- Testing instructions (how to verify upgrade successful)
- Rollback procedure (in case something goes wrong)
- Estimated time to complete

## Pre-Publication Checklist
- [ ] Version number matches git tag exactly (no fabricated versions)
- [ ] All breaking changes flagged with [RISK] **BREAKING** and marked as removed
- [ ] Migration steps tested in staging environment
- [ ] Deprecation timeline realistic (≥2 major versions ahead for removal)
- [ ] All security fixes include CVE number and severity
- [ ] Changelog organised by type: Added, Changed, Deprecated, Removed, Fixed, Security
- [ ] Pull request and issue links included (traceability)
- [ ] Major changes highlighted in release notes summary
- [ ] Known issues documented (don't hide problems)
- [ ] Upgrade instructions provided (especially for breaking changes)
- [ ] Rollback procedure tested (can users revert if update fails?)
- [ ] Estimated migration time provided (developer planning)
- [ ] Previous deprecation deadlines met (don't move goalposts)
- [ ] GGP markers applied: [CONFIRMED] for verified dates/versions, [INFERENCE] for estimated timelines, [GAP] for gaps

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**



---
