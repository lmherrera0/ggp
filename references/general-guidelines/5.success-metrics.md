# GGP Success Metrics
## Measuring Grounded Gate Protocol Effectiveness

---

## Purpose

Transform GGP from "a process" into "a measurable system." These KPIs enable tracking, benchmarking, and evidence-based improvement of AI output quality.

---

## Core KPIs

| # | Metric | Target | How to Measure | Frequency |
|---|--------|--------|----------------|-----------|
| 1 | **Hallucination Rate** | < 5% of claims without CONFIRMED | Audit 20 random outputs; count claims lacking Tier 1-3 sources | Monthly |
| 2 | **Gap Declaration Rate** | > 95% of gaps marked | Post-hoc review: compare final output to available data; identify unmarked gaps | Monthly |
| 3 | **Loop Iterations** | ≤ 2 rounds average | Count conversation turns from request to first draft with markers | Per output |
| 4 | **User Override Rate** | < 15% | Track how often user requests changes to clean output after GGP process completes | Monthly |
| 5 | **Time to Clean Output** | ≤ 3 turns | Count turns from initial request to clean output delivery (Phase 3h) | Per output |

---

## Extended Metrics (For Evidence Portfolio and Enterprise Adoption)

| # | Metric | Target | Purpose |
|---|--------|--------|---------|
| 6 | **Source Tier Distribution** | > 60% Tier 1-2 sources per output | Measures quality of evidence base |
| 7 | **Devil's Advocate Catch Rate** | > 0 risks flagged per 5 outputs | Validates reputation protection is active |
| 8 | **Inference Acceptance Rate** | Track percentage of inferences accepted vs. rejected by user | Calibrates AI confidence accuracy |
| 9 | **Channel Compliance Rate** | 100% channel checklist pass on first validation | Measures skill reference loading discipline |
| 10 | **Cross-Platform Consistency** | Same claim receives same marker across Claude, ChatGPT, Copilot | Validates platform template parity |

---

## Measurement Protocol

### Monthly Audit Process

1. **Sample**: Select 20 GGP-processed outputs from the past month
2. **Review**: For each output, check:
   - Are all factual claims tagged with CONFIRMED + source?
   - Are all assumptions tagged with INFERENCE?
   - Are all missing data points tagged with GAP?
   - Was Devil's Advocate run (check for RISK flags or explicit "no risks found")?
   - Did the output reach clean version in ≤ 3 turns?
3. **Score**: Calculate each KPI
4. **Report**: Document results and trends

### Scoring Template

```
GGP MONTHLY SCORECARD

Period: [Month Year]
Outputs Audited: [N]

| KPI | Target | Actual | Status |
|-----|--------|--------|--------|
| Hallucination Rate | < 5% | [X]% | PASS/FAIL |
| Gap Declaration Rate | > 95% | [X]% | PASS/FAIL |
| Loop Iterations (avg) | ≤ 2 | [X] | PASS/FAIL |
| User Override Rate | < 15% | [X]% | PASS/FAIL |
| Time to Clean Output (avg) | ≤ 3 turns | [X] | PASS/FAIL |

Overall Score: [X]/5 targets met
Trend vs. Prior Month: [Improving/Stable/Declining]

Notes: [Observations, patterns, improvement actions]
```

---

## Maturity Levels

Use these to assess GGP adoption maturity within a team or organisation:

| Level | Name | Criteria |
|-------|------|----------|
| 1 | **Awareness** | Team knows GGP exists; no formal adoption |
| 2 | **Adoption** | GGP markers used in some outputs; no measurement |
| 3 | **Compliance** | All professional outputs use GGP; monthly audits started |
| 4 | **Optimisation** | KPIs tracked monthly; targets met consistently; platform templates deployed |
| 5 | **Excellence** | Cross-platform consistency verified; community contributions; framework evolution |

---

## Version
GGP Success Metrics v1.0 | Last Updated: 2026-02-20

---

## GGP Return to Main Flow

This file is OPERATIONAL — not part of the content creation flow. After reading:

- **If auditing GGP performance**: Use the scorecard template above and return to the task at hand.
- **If referenced during content creation**: Return to SKILL.md Phase 3 (Create + Validate + Deliver).

**NEVER stay in this file. Return to the task at hand.**

