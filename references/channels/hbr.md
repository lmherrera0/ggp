# Harvard Business Review (HBR)

## Goal

Narrative-driven business insights that combine academic rigor with accessibility for executive leaders. Establish thought leadership among C-suite audiences and drive speaking/consulting opportunities.

## Success Metrics (2025-2026)

- **Publication**: Accepted by HBR editorial team; published in print and online
- **Reach**: 5M+ impressions across HBR platforms within 30 days
- **Engagement**: 2K+ shares, 500+ comments on published article
- **Citation**: Article referenced in 10+ external articles or research within 12 months
- **Opportunity**: 3+ speaking invitations or consulting leads from article visibility
- **Downstream**: Article drives 500+ newsletter subscribers or DM inquiries from target audience

## Limits & Restrictions

### Technical
- **Word count**: 2000-4000 words (HBR average 2500)
- **Format**: Submitted as Google Doc or Word with single spacing, margins, citations
- **Structure**: Title, subtitle, deck (2-3 sentences), body, takeaways
- **Citations**: Chicago Manual of Style (footnotes) or endnotes; APA acceptable
- **Visuals**: Max 1-2 charts/graphics; all original or properly licensed
- **Images**: High-res (300dpi); rights secured for any photos or diagrams

### Platform/Algorithm
- **HBR editorial cycle**: 3-4 months from submission to publication (plan ahead)
- **Editor preference**: Editors favour contrarian but defensible perspectives
- **Searchability**: Keywords in title and deck determine visibility in HBR search
- **Curator priority**: HBR curators share pieces that challenge conventional wisdom
- **Peer review**: Light peer review by HBR editors; fact-checking is editorial responsibility
- **SEO impact**: Published articles rank in top 5 for keywords within 2 weeks

### GGP Etiquette
- **Rigorous sourcing**: HBR readers are executives; unsourced claims damage credibility
- **Counterargument acknowledgement**: Fair treatment of opposing views strengthens argument
- **No client names**: Never use real client examples; disguise or use composite examples
- **Attribution mandatory**: Name researchers, cite studies, link sources
- **Academic honesty**: Avoid plagiarism; all frameworks must be original or properly credited
- **No sponsored-content masquerade**: If sponsored by organisation, disclose clearly

## Template

```
[TITLE — Specific, provocative, searchable]
Why Traditional Performance Reviews Are Obsolete: Evidence from Fortune 500 Transformations

[SUBTITLE — Expands on title, adds context]
A study of 200+ organisations reveals that real-time feedback systems outperform annual reviews by 3x on employee retention and engagement.

[DECK — 2-3 sentences, hypothesis summary]
Annual performance reviews have survived 50 years of management science because they're familiar, not because they work. This article examines data from 200+ organisations that have replaced reviews with continuous feedback systems, identifying what works, what requires cultural change, and why the shift is happening now.

[INTRODUCTION — Hook + problem statement]
[Hook: Start with surprising stat or observation]
Example: "Seventy-eight percent of executives say their performance review system is broken. Yet 92% of organisations haven't changed their approach in five years."

[Problem: Why current state is inadequate]
Annual performance reviews served industrial-era companies well. Fixed timelines, bureaucratic rigor, paper trails. But today's businesses operate at velocity that annual cycles can't match. Employees need feedback in days, not months.

[Why now: What's changed]
Technology enables continuous feedback (software platforms, real-time data). Generational expectations have shifted (younger workers expect coaching, not judgment). Competitive pressure is intense (companies using continuous feedback report 2x faster adaptation).

[BODY SECTION 1 — Research findings / framework]
[Data section: 2-3 paragraphs with evidence]
We analysed 200+ organisations that transitioned from annual reviews to continuous feedback systems. Key findings:

1. [Finding with data]: Organisations that shifted to quarterly check-ins saw 34% increase in retention (95% CI: 28-40%). Employees reported clearer expectations (81% vs. 54% in annual-review orgs).

2. [Finding with data]: The first year of transition is hardest. Manager training requirements doubled in year 1 but stabilised by year 2. Orgs that front-loaded training had 2x better outcomes.

3. [Finding with data]: Remote organisations transitioned faster than co-located ones, likely because continuous systems don't require annual in-person review logistics.

[Framework section: 2-3 paragraphs]
What separates successful transitions from failed ones? We identified three dimensions:

Dimension 1: [Name and describe]
Organisations that succeeded framed this not as "replacing reviews" but as "adding coaching." Existing infrastructure (1:1s, skip-level meetings) became touchpoints for continuous feedback rather than isolated review events.

Dimension 2: [Name and describe]
Measurement changed. Organisations still track performance against goals, but frequency increased from annual to quarterly. This allows real-time course correction rather than post-hoc evaluation.

Dimension 3: [Name and describe]
Accountability shifted from manager to system. Organisations built feedback into workflows: project retrospectives, peer input windows, customer feedback summaries. Feedback became ambient, not ceremonial.

[BODY SECTION 2 — Complications / counterargument]
[Address the "but what about..." concern]
This isn't easy. Organisations that tried and failed typically stumbled on three points:

Complication 1: [What doesn't work]
Manager training: Frequent feedback requires different skills than annual review. Managers who are good at annual reviews (structured, formal) struggle with coaching (frequent, conversational). Organisations that just "told people to give more feedback" without training had retention increase of only 8% (vs. 34% with training).

Complication 2: [What requires change]
Technology isn't enough: Systems alone don't change behaviour. Organisations need cultural permission for managers to spend more time on coaching. Budget allocation matters: Time for coaching = time away from production. CFOs need to understand this trade-off.

Complication 3: [Valid criticism of the argument]
For roles with clear, measurable output (sales, manufacturing), annual reviews have clarity advantage: you can measure Q1-Q4 objectively. Shifting to quarterly doesn't add value if the output doesn't change quarterly. These roles may not benefit from continuous feedback (though improved communication still has value).

[Acknowledge reasonable objection]
"If continuous feedback is so good, why haven't more organisations adopted it?" Fair question. Adoption is hard; switching costs are real. Many executives who've experienced the transition say the value justifies the cost, but it's not a no-brainer.

[BODY SECTION 3 — Implications / what to do]
[For different audiences]
For CEOs considering this shift:
- Start with pilot: Choose one department (15-25 people) that's high-trust and high-engagement. Run parallel annual reviews for one year. Measure retention, engagement, time investment. If results are positive, expand.
- Budget for training: 80% of transition failure is due to inadequate manager coaching training. Invest 4-6 hours per manager in year 1.
- Communicate the why: Employees interpret feedback-frequency changes as surveillance unless leadership explains the intent. Frame as "we want to help you succeed" not "we're watching you more."

For HR leaders implementing this:
- Select technology that integrates with existing workflows (not another standalone system)
- Build feedback into existing cadences (1:1s, sprint retrospectives, project close-outs)
- Create templates for consistency; this reduces manager burden and improves quality

For managers in organisations making this shift:
- View feedback as coaching, not evaluation. Your job is to help people succeed, not judge them.
- Ask before giving: "Would feedback be helpful here?" respects autonomy.
- Balance positive and developmental feedback 3:1 (research shows this ratio is optimal for motivation)

[CONCLUSION — Reinforce thesis, look forward]
[Restate thesis in light of evidence]
Performance reviews aren't broken because companies are lazy. They're broken because business velocity has changed and annual cycles can't keep up. Evidence from 200+ organisations shows that switching to continuous feedback improves retention, engagement, and agility.

[Future angle]
The next frontier isn't frequency; it's quality. Organisations that master continuous feedback will next tackle feedback specificity and skill-building. Feedback that identifies gaps + provides learning resources = accelerated development.

[Final thought]
The companies that moved fastest on this transition are 3 years ahead in employee capability and retention. The cost of waiting is growing.

[KEY TAKEAWAYS — 3-5 bullet points, pulled out in sidebar]
• Annual performance reviews are ineffective; continuous feedback systems improve retention by 34% based on data from 200+ organisations
• Successful transitions require manager training (80% of failures are training-related), not just technology
• Continuous feedback works best when built into existing workflows (1:1s, retrospectives) rather than as a standalone system
• Organisations that transitioned report 2x faster adaptation and clearer employee expectations
• Success requires framing as coaching opportunity, not surveillance; communication of intent is critical
```

## Examples

### Good Example (Article Title/Deck)

**TITLE:**
How Data Democratization Transforms Organisational Agility

**DECK:**
Most organisations hoard data in analytics teams. A study of 150 companies reveals that organisations that distribute data access and literacy across functions adapt 3x faster, make better decisions, and retain more talent. Here's how they did it and what stumbled them.

---

**Why this works:**
- Title makes a specific claim (democratization → agility) and is searchable
- Deck includes data point (3x faster) and previews structure (what worked, what failed)
- Implies contrarian angle without being inflammatory
- Sets up for body to explain mechanism and evidence

### Bad Example (Article Title/Deck)

**TITLE:**
Everyone's Doing Analytics Wrong!!!

**DECK:**
Most companies don't understand data and they should. This article explains why analytics is important and what people should be doing instead. Lots of companies fail and some succeed.

---

**Why this fails:**
- Title is hyperbolic and not searchable ("Everyone" is too vague)
- Deck has no data, no specificity, no contrarian insight
- Doesn't preview what the article will teach
- Tone suggests clickbait rather than rigorous argument
- HBR editors would reject immediately based on deck

## Tactical Guidance

**Research Requirements:**

- Primary research ideal (survey, interviews, data analysis)
- But secondary research acceptable if novel analysis or synthesis
- Minimum: 5-10 peer-reviewed or high-credibility sources
- All claims with numbers need citations (not optional at HBR)

**Writing Style:**

```
✓ HBR voice: "Our research suggests..." [with data]
✓ HBR voice: "Three mechanisms explain..." [framework]
✓ HBR voice: "Some companies succeeded by..." [evidence]

✗ Not HBR voice: "Everyone should..." [prescriptive without nuance]
✗ Not HBR voice: "The future is..." [prediction without evidence]
✗ Not HBR voice: "It's obvious that..." [dismissive of complexity]
```

**Editorial Strategy:**

- Write for HBR editors (not for your peers). Ask: "Does an executive leading this function learn something actionable?"
- Lead with data or surprising observation, not with theory
- Address the "so what?" explicitly. Why should a busy executive care?
- Structure as: Problem (why this matters) → Evidence (here's what we found) → Implication (here's what to do)

**Submission Process:**

1. Query first: Email brief pitch (150 words) to appropriate HBR section editor
2. Wait for green light: Don't write full article without editorial encouragement
3. Draft and submit: Include title, subtitle, deck, article, key takeaways, author bio
4. Expect revision: HBR editors will request changes; be collaborative
5. Timeline: 3-4 months from submission to publication typical

## Pre-Publication Checklist

- [ ] Title is specific and searchable (includes searchable keyword like "democratization," not just "trends")
- [ ] Deck previews thesis and key finding with data point
- [ ] Article is 2000-4000 words; on target for word count
- [ ] BLUF applied: Problem and evidence come before lengthy background
- [ ] All numerical claims are cited (no unsourced data)
- [ ] Research is original or novel synthesis of existing research
- [ ] Counterargument section acknowledges valid criticisms of main argument
- [ ] Implications section is specific to audience (CEOs, HR, managers, etc.)
- [ ] Tone is authoritative but not prescriptive; acknowledges complexity
- [ ] No client names used; examples are disguised or composite
- [ ] All attribution complete: named researchers, linked studies, proper citations
- [ ] Key takeaways box extracted and summarised (3-5 points)
- [ ] Author bio is current and relevant to topic
- [ ] GGP markers check: Every factual claim is [VERIFIED] with source citation. Estimates are transparent about sample size and confidence intervals. Inferences are labelled as interpretations. No unverified claims.
- [ ] Submitted to appropriate HBR section editor with brief query first

---

**Last Updated**: February 2026

---

## GGP Mandatory Validation — Return to SKILL.md

After completing this channel's checklist, you MUST return to SKILL.md and execute:

- **3f. Devil's Advocate** (8 dimensions scored 1-3: Hostile Reader, Screenshot, CEO)
- **3g. Validation Gate** (8-point checklist — must score 8/8)
- **3h. Refinement + Clean Output** (present gaps/inferences for user decision)

**Flow**: Load channel → Create content → Channel checklist → **Return to SKILL.md 3f-3h** → Validate → Deliver.

**NEVER deliver content without completing the full GGP Validation Gate.**

